---
title: "Rock Paper Scissors Hackathon"
format:
  html:
    math: true
---
**R**ock **P**aper **S**cissors Hackathon by [Poker Camp](https://poker.camp)

## Event Details
**What:** All participant bots play 1000 games against each other bot and a selection of our own that have varying levels of complexity

**When:** Sunday Sep 8, 2024 11am-5pm (can arrive starting at 10:30am)

**Where:** [Fractal Tech Hub](https://fractalbootcamp.com/office): [111 Conselyea St, Brooklyn NY](https://maps.app.goo.gl/fuZpTaLtaxy9W4v98)

<br><br><iframe src="https://www.google.com/maps/embed?pb=!1m18!1m12!1m3!1d6048.25343208826!2d-73.9467208!3d40.715226900000005!2m3!1f0!2f0!3f0!3m2!1i1024!2i768!4f13.1!3m3!1m2!1s0x89c259574c8d3d9f%3A0x50a89962ece47d61!2s111%20Conselyea%20St%2C%20Brooklyn%2C%20NY%2011211!5e0!3m2!1sen!2sus!4v1723515244564!5m2!1sen!2sus" width="400" height="300" style="border:0;" allowfullscreen="" loading="lazy" referrerpolicy="no-referrer-when-downgrade"></iframe>

## How it Works
Rock defeats scissors, scissors defeats paper, and paper defeats rock. You get 1 point for winning, -1 point for losing, and 0 points for ties.

Starter code, 1000 game matches, see history 

Schedule

Prizes

Submissions

 In sequential decision-making, agent evaluation has largely been restricted to few
interactions against experts, with the aim to reach some desired level of performance (e.g.
beating a human professional player). We propose a benchmark for multiagent learning
based on repeated play of the simple game Rock, Paper, Scissors

## Strategy
<iframe width="560" height="315" src="https://www.youtube.com/embed/b0SoKWLkmLU?si=0AQ3_NgHOe4tc4OU" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>

### Game Theory
![](m.png)

RPS is a zero-sum game and the payouts are symmetrical as follows: 

| Player 1/2 | Rock    | Paper   | Scissors |
|--------|---------|---------|----------|
| Rock   | (0, 0)  | (-1, 1) | (1, -1)  |
| Paper  | (1, -1) | (0, 0)  | (-1, 1)  |
|Scissors| (-1, 1) | (1, -1) | (0, 0)   |

The Nash Equilibrium strategy is to play each action $r = p = s = 1/3$ of the time. 

:::{.callout-tip collapse="true" appearance="minimal"}
## Nash Equilibrium Strategy for RPS
If Player 1 plays Rock with probability $r$, Paper with probability $p$, and Scissors with probability $s$, we have the following expected value equations for Player 2: 

$\mathbb{E}(\text{R}) = -1*p + 1*s$

$\mathbb{E}(\text{P}) = 1*r - 1*s$

$\mathbb{E}(\text{S}) = -1*r + 1*p$

Since no action dominates, we know that the EV of every strategic action should be equal. 

$\mathbb{E}(\text{R}) = \mathbb{E}(\text{P})$

$-1*p + 1*s = 1*r - 1*s$

$2*s = p + r$

$\mathbb{E}(\text{R}) = \mathbb{E}(\text{S})$

$-1*p + 1*s = -1*r + 1*p$

$s + r = 2*p$

$\mathbb{E}(\text{P}) = \mathbb{E}(\text{S})$

$1*r - 1*s = -1*r + 1*p$

$2*r = s + p$

Solving the equations: 

$r = 2*s - p$

$s + (2*s - p) = 2*p$

$3*s = 3*p$

$s = p$


$s + r = 2*p$

$s + r = 2*s$

$r = s$

We know that all are equal: 

$s = p = r$

We also know that they must all sum to $1$: 

$r + p + s = 1$

Since they're all equal and sum to $1$, we can substitute $p$ and $s$ with $r$: 

$3*r = 1$

$r = 1/3$

So all actions are taken with probability $1/3$: 

$r = p = s = 1/3$
:::

Playing this strategy means that whatever your opponent does, you will breakeven! You can see this interactively below: 

<iframe src="rps_strategy.html" width="100%" height="600" style="border:none;"></iframe>

### Adapting to Opponents
The goal of this challenge is to focus on tracking opponent distributions and how to respond to them.

How would you maximize in RPS knowing the opponent plays a fixed non-Nash strategy that you know? You can try this by playing with the above interactive with the 40% Rock option. 