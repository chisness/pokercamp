---
title: "Other Games (2): Reading"
sidebar: aipcs24
format:
  html:
    math: true
---
## UNDER CONSTRUCTION! 

## Types of Games

| Game/Opponent     | Fixed/Probabilistic Opponent | Adversarial Opponent | 
|------------|-----------|--------|
| Imperfect Info, No Player Agency/Decisions      |           |
| Perfect Info, Player Actions Always Perfect Info     |  |
| Imperfect Info, Imperfect from Randomness     |   |
| Imperfect Info, Imperfect from Randomness AND Game States    |   |

:::{.callout-tip collapse="true"  appearance="minimal"}
## Pre-Filled Table

| Game/Opponent     | Fixed/Probabilistic Opponent | Adversarial Opponent | 
|------------|-----------|--------|
| Imperfect Info, No Player Agency/Decisions      | Candy Land, War, Dreidel, Bingo, Chutes and Ladders, Slot machine          |
| Perfect Info, Player Actions Always Perfect Info     | Puzzles  | Tictactoe, Checkers, Chess, Arimaa, Go |
| Imperfect Info, Imperfect from Randomness     | Blackjack   | Backgammon |
| Imperfect Info, Imperfect from Randomness AND Game States    | Partial Info Multi-armed Bandit   | Poker, Rock Paper Scissors, Liar’s Dice, Figgie |

:::

What about solitaire? With Blackjack?
What about a lottery? 
Mahjong?

(Ross note: The difference between “Chance” and “Imperfect Info” is that in Chance, the unknown [thing] doesn’t affect anything about the world until it becomes known, and then it’s not unknown any more. In Imperfect Info, the information has some effect on the world at time T1, then you need to make a decision at time T2, then the information will matter at some later point T3.)

What makes the bottom right of the table interesting?


## Intro to Reinforcement Learning (RL)
The main idea of reinforcement learning is that an agent learns by interacting with its environment. The main elements of a learning system are: a policy, a reward, a value function, and sometimes a model of the environment.

Policy, reward signal, value function, model of environment
Agent, environment interface RL book chapter 3 stuff
Tabular environment stuff
TD/SARSA/Q-LEARNING, relation to CFR
https://arena3-chapter2-rl.streamlit.app/[2.1]_Intro_to_RL


## War
If you thought that Kuhn Poker was a simple card game, meet War. 

There are two players, each gets half the deck, 26 cards. Each player turns over their top card and faces it against the opponent's top card. The better card wins, with Aces high. This repeats until one player has all the cards. 

When the cards match, the players go to “War”. When this happens, put the next card face down, and then the card after that face up, and then these up-cards face off against each other. The winner takes all six cards. If there’s another tie, then repeat and the winner takes 10 cards, etc. 

:::{.callout-note  appearance="minimal"}
## Optional Coding Exercise
Code and run 10,000 simulations of War and determine how many turns the average game takes and how many War situations occur on average in each simulation.

:::

You can see a Dreidel game simulator written by Ben Blatt in Slate from 2014 at [this link](https://www.slate.com/articles/life/holidays/2014/12/rules_of_dreidel_the_hannukah_game_is_way_too_slow_let_s_speed_it_up.html). 

## Tictactoe 
![](assets/backprop.png)

Evolutionary algorithms for poker and tic-tac-toe

Exercise: Look at this game tree with payouts at the bottom written in terms of Player 1. Start from the bottom of the tree and figure out the actions of each player at each node. Then figure out the value of the game for Player 1 and for Player 2. This procedure is called backpropagation. 

Question: Why can’t we use this procedure in Kuhn Poker and imperfect info games? 
Tree imperfect info vs perfect info issues, show trees
Compare this to the Wabbits game and problem sfrom paper 

### Minimax
https://www.neverstopbuilding.com/blog/minimax

### Value Function
Minimax assumes opponent playing best plays too
temporal-difference

### RL: Planning
MCTS decision-time planning

RL pages 8-12
MCTS 8.11
https://starai.cs.ucla.edu/papers/VdBBNAIC09.pdf 
What about just using imperfect info version of MCTS? 

## Multi-Armed Bandits
![](assets/10bandits.png)
RL book 26-44, explore exploit 
Bandits are a set of problems with repeated decisions and a fixed number of actions possible. This is related to reinforcement learning because the agent player updates its strategy based on what it learns from the feedback from the environment. Consider a 10-armed bandit setup like in the image below: 

Each time the player pulls an arm, they get some reward, which could be positive or negative. 

A basic setting initializes each of 10 arms with q*(arm) = N(0,1) so each is initialized with a center point around a Gaussian distribution. Each pull of an arm returns a reward of R = N(q*(arm), 1). 

In other words, each arm is initialized with a value centered around 0 but with some variance, so each will be a bit different. Then from that point, the actual pull of an arm is centered around that new point with some variance as seen in this figure with a 10-armed bandit from the book Intro to Reinforcement Learning by Sutton and Barto:

![](assets/banditdist.png)

The player can sample actions and estimate their values based on experience and then use some algorithm for deciding which action to use next to maximize rewards. 

:::{.callout-note  appearance="minimal"}
## Exercise

For example, a naive approach is that a player could sample each action once and then always use the action that gave the best reward going forward.  

What is a problem with this strategy? Can you suggest a better one? 

:::



### Regret
How would you define regret in this bandit setting? 
How is minimizing regret related to maximizing reward? 



### RL: Action-Value Methods
Hidden imperfect info, understand in distribution over possible states of the world
Depending on state will want to make decisions differently 
Bayesian explore exploit optimizer 
What is my opponent range exercise in Bayesian update
Explore vs. exploit
Bayesian updating
https://aipokertutorial.com/game-theory-foundation/#regret
https://www.reddit.com/r/statistics/comments/1949met/how_are_multi_armed_bandits_related_to_bayesian/ 
https://tor-lattimore.com/downloads/book/book.pdf 
https://lcalem.github.io/blog/2018/09/22/sutton-chap02-bandits#26-optimistic-initial-values 

But you could use the MCTS that does work by changing the game setup


## Blackjack
Setup
Solving


### RL: Dynamic Programming
My blog post


### RL: Monte Carlo Methods

## Poker Games
Why are these the most interesting games? We will mostly focus on these going forward, sometimes GTO and sometimes exploitative. 

## Optional: Agent Submissions
Tictactoe

Blackjack

How to submit?