---
title: "#2 Other Games: Reading"
sidebar: aipcs24
format:
  html:
    math: true
---
## Types of Games
This course is primarily about poker and poker-adjacent games, but let's take a step back and look at different classes of games and where poker fits into this table. 

The goal of this reading is to introduce various concepts that will be useful when building agents for future challenges. 

:::{.callout-note  appearance="minimal"}
## Exercise
Fill in the table below with games you know about: 

:::

| Game/Opponent     | Fixed/Probabilistic Opponent | Adversarial Opponent | 
|------------|-----------|--------|
| Imperfect Info, No Player Agency/Decisions      |           |
| Perfect Info, Player Actions Always Perfect Info     |  |
| Imperfect Info, Imperfect from Randomness     |   |
| Imperfect Info, Imperfect from Randomness AND Game States    |   |

:::{.callout-tip collapse="true"  appearance="minimal"}
## Pre-Filled Table

| Game/Opponent     | Fixed/Probabilistic Opponent | Adversarial Opponent | 
|------------|-----------|--------|
| Imperfect Info, No Player Agency/Decisions      | Candy Land, War, Dreidel, Bingo, Chutes and Ladders, Slot machine          |
| Perfect Info, Player Actions Always Perfect Info     | Puzzles  | Tictactoe, Checkers, Chess, Arimaa, Go |
| Imperfect Info, Imperfect from Randomness     | Blackjack   | Backgammon |
| Imperfect Info, Imperfect from Randomness AND Game States    | Partial Info Multi-armed Bandit   | Poker, Rock Paper Scissors, Liar’s Dice, Figgie |

:::

:::{.callout-note  appearance="minimal"}
## Exercise
What makes poker and other games in the bottom right of the table interesting? 

:::


What about solitaire? With Blackjack?
What about a lottery? 
Mahjong?

(Ross note: The difference between “Chance” and “Imperfect Info” is that in Chance, the unknown [thing] doesn’t affect anything about the world until it becomes known, and then it’s not unknown any more. In Imperfect Info, the information has some effect on the world at time T1, then you need to make a decision at time T2, then the information will matter at some later point T3.)

## Reinforcement Learning (RL)
*Many ideas in this section on RL come from Sutton and Barto's Reinforcement Learning book.* 

The main idea of reinforcement learning is that an *agent* learns by taking *actions* in its *environment*. The environment then gives feedback in the form of *rewards* and a new *state*. 

![Diagram from Sutton Barton Reinforcement Learning](assets/agentactionenv.png)

We have for each time $t$: 
- State: $S_t$
- Action: $A_t$ 
- Reward: $R_t$ 

The goal of an agent is generally to maximize the expected value of the rewards received. 

Reward values can be designed in a way to optimally train the agent. In chess for example, we could give a reward of $+1$ for winning a game, $-1$ for losing, and $0$ for all other states including draws. Defining intermediate rewards for actions like taking an opponent piece risks the agent prioritizing that goal over winning the game. 

Reward hacking is when agents find a way to obtain rewards in a  way that isn't aligned with the intended goal. (The agents will take the reward structure very literally and this is all they have to go on!) 

A Markov Decision Process (MDP) is a simplified model of the reinforcement learning problem in that the probability of future states and rewards depends only on the previous state and actions. 

This can be written as: 

$$
p(s', r \mid s, a) = \Pr\{S_t=s', R_t=r \mid S_{t-1}=s, A_{t-1}=a\}
$$

Value functions estimate the value of being in a certain state in terms of expected future rewards (which can be discounted). 

- If you're about to flip a coin and want it to be heads ($+1$) and not tails ($-1$), then the value of that state is $0$. 

- If you're in a chess game where winning is $+1$ and losing is $-1$ and everything else is $0$ and you're in a state where it's your turn and you can checkmate, then that state has a value of $+1$. 

A policy is a mapping from states to probabilties of selecting each action and is written as $\pi(a\mid s)$. 



Simple MDP example,  like buy carrots not anything else 

Policy, value function, optimal policy, optimal value function

## War
If you thought that Kuhn Poker was a simple card game, meet War. 

There are two players, each gets half the deck, 26 cards. Each player turns over their top card and faces it against the opponent's top card. The better card wins, with Aces high. This repeats until one player has all the cards. 

When the cards match, the players go to “War”. When this happens, put the next card face down, and then the card after that face up, and then these up-cards face off against each other. The winner takes all six cards. If there’s another tie, then repeat and the winner takes 10 cards, etc. 

:::{.callout-note  appearance="minimal"}
## Optional Coding Exercise
Code and run 10,000 simulations of War and determine how many turns the average game takes and how many War situations occur on average in each simulation.

:::

You can see a Dreidel game simulator written by Ben Blatt in Slate from 2014 at [this link](https://www.slate.com/articles/life/holidays/2014/12/rules_of_dreidel_the_hannukah_game_is_way_too_slow_let_s_speed_it_up.html). 

## Tictactoe 
![](assets/backprop.png)

Evolutionary algorithms for poker and tic-tac-toe

Exercise: Look at this game tree with payouts at the bottom written in terms of Player 1. Start from the bottom of the tree and figure out the actions of each player at each node. Then figure out the value of the game for Player 1 and for Player 2. This procedure is called backpropagation. 

Question: Why can’t we use this procedure in Kuhn Poker and imperfect info games? 
Tree imperfect info vs perfect info issues, show trees
Compare this to the Wabbits game and problem from paper 

### Minimax
https://www.neverstopbuilding.com/blog/minimax

### Value Function
Minimax assumes opponent playing best plays too
temporal-difference

### RL: Planning and Learning

### RL: Monte Carlo Tree Search

https://starai.cs.ucla.edu/papers/VdBBNAIC09.pdf 
What about just using imperfect info version of MCTS? 

## Multi-Armed Bandits
![](assets/10bandits.png)

Bandits are a set of problems with repeated decisions and a fixed number of actions possible coming from a single state. 

The agent updates its strategy based on what it learns from the feedback from the environment. You could think of this in real-world settings like picking which dish to eat at a restaurant. 

Consider a 10-armed bandit setup like in the image below: 

![Sutton Barto Bandits](assets/banditdist.png)

Each time the player pulls an arm, they get some reward, which could be positive or negative. 

A basic setting initializes each of 10 arms with $q*(\text{arm}) = \mathcal{N}(0,1)$ so each is initialized with a center point around a Gaussian distribution. Each pull of an arm returns a reward of $R = \mathcal{N}(q*(\text{arm}_i), 1)$. 

In other words, each arm is initialized with a value centered around $0$ but with some variance, so each will be a bit different. Then from that point, the actual pull of an arm is centered around that new point with some variance as seen in this figure above. 

The agent can sample actions and estimate their values based on experience and then use some algorithm for deciding which action to use next to maximize rewards. The estimated value of action $a$ at timestep $t$ is defined as $Q_t(a)$. 

The agent is then faced with two competing goals: 

1. Get as accurate an estimate $Q_t(a)$ as possible
2. Select actions with the highest rewards as much as possible

Exploring refers to figuring out the values of the arms, or in the case of a restaurant, figuring out how good each dish is. 

Exploiting refers to using current knowledge to choose the highest value estimated action. 

:::{.callout-note  appearance="minimal"}
## Exercise

A naive approach is that a player could sample each action once and then always use the action that gave the best reward going forward.  

What is a problem with this strategy? Can you suggest a better one? 

:::

$Q_t(a)$ can simply be estimated by averaging the rewards received each time a specific arm has been tried. The so-called *greedy* action rule is to then take the largest $Q_t(a)$ action, $A_t = \argmax_{a} Q_t(a)$

### Regret
How would you define regret in this bandit setting? 
How is minimizing regret related to maximizing reward? 
explore exploit 

### RL: Action-Value Methods
Hidden imperfect info, understand in distribution over possible states of the world
Depending on state will want to make decisions differently 
Bayesian explore exploit optimizer 
What is my opponent range exercise in Bayesian update
Explore vs. exploit
Bayesian updating
https://aipokertutorial.com/game-theory-foundation/#regret
https://www.reddit.com/r/statistics/comments/1949met/how_are_multi_armed_bandits_related_to_bayesian/ 
https://tor-lattimore.com/downloads/book/book.pdf 
https://lcalem.github.io/blog/2018/09/22/sutton-chap02-bandits#26-optimistic-initial-values 

But you could use the MCTS that does work by changing the game setup


## Blackjack
Setup
Solving


### RL: Dynamic Programming
My blog post


### RL: Monte Carlo Methods

## Gridworld
pg 60

### RL: Q-Learning 

## Poker Games
Why are we mostly interested in poker games? We think that 

imperfect
adversarial
widely played
well researched, but only nash equilibrium