---
title: "#2 Other Games: Reading"
sidebar: aipcs24
format:
  html:
    math: true
---
## Types of Games
This course is primarily about poker and poker-adjacent games, but let's take a step back and look at different classes of games and where poker fits into this table. 

The goal of this reading is to introduce various concepts that will be useful when building agents for future challenges. 

Tennis value of states in RL 

:::{.callout-note  appearance="minimal"}
## Exercise
Fill in the table below with games you know about: 

:::

| Game/Opponent     | Fixed/Probabilistic Opponent | Adversarial Opponent | 
|------------|-----------|--------|
| Imperfect Info, No Player Agency/Decisions      |           |
| Perfect Info, Player Actions Always Perfect Info     |  |
| Imperfect Info, Imperfect from Randomness     |   |
| Imperfect Info, Imperfect from Randomness AND Game States    |   |

:::{.callout-tip collapse="true"  appearance="minimal"}
## Pre-Filled Table

| Game/Opponent     | Fixed/Probabilistic Opponent | Adversarial Opponent | 
|------------|-----------|--------|
| Imperfect Info, No Player Agency/Decisions      | Candy Land, War, Dreidel, Bingo, Chutes and Ladders, Slot machine          |
| Perfect Info, Player Actions Always Perfect Info     | Puzzles  | Tictactoe, Checkers, Chess, Arimaa, Go |
| Imperfect Info, Imperfect from Randomness     | Blackjack   | Backgammon |
| Imperfect Info, Imperfect from Randomness AND Game States    | Partial Info Multi-armed Bandit   | Poker, Rock Paper Scissors, Liar’s Dice, Figgie |

:::

:::{.callout-note  appearance="minimal"}
## Exercise
What makes poker and other games in the bottom right of the table interesting? 

:::


What about solitaire? With Blackjack?
What about a lottery? 
Mahjong?

(Ross note: The difference between “Chance” and “Imperfect Info” is that in Chance, the unknown [thing] doesn’t affect anything about the world until it becomes known, and then it’s not unknown any more. In Imperfect Info, the information has some effect on the world at time T1, then you need to make a decision at time T2, then the information will matter at some later point T3.)

## Concept: Nash Equilibrium
A Nash equililibrium is a set of strategies for both players such that neither player ever plays an action with regret (when they take those strategies as given).

Recall that regret only makes sense in the context of a particular strategy and assumed opponent’s strategy. When you submitted to Challenge 1, you submitted a strategy for being P1, and a strategy for being P2, but you won’t ever play against yourself – so why is it helpful to find a pair that plays against itself without regret?

The other paradigm for game strategies is opponent exploitation, which we will address in future sections. 

### Indifference
Consider the **Soccer Penalty Kick** game where a Kicker is trying to score a goal and the Goalie is trying to block it. 

| Kicker/Goalie | Lean Left | Lean Right | 
|------------|-----------|--------|
| Kick Left  | 0, 0  | +2, -2 |
| Kick Right | +1, -1 | 0, 0|

The game setup is zero-sum. If Kicker and Goalie both go in one direction, then it’s assumed that the goal will miss and both get $0$ payoffs. If the Kicker plays Kick Right when the Goalie plays Lean Left, then the Kicker is favored and gets a payoff of $+1$. If the Kicker plays Kick Left when the Goalie plays Lean Right, then the kicker is even more favored, because it’s easier to kick left than right, and gets $+2$.

:::{.callout-note  appearance="minimal"}
## Nash Equilibrium Exercise

Which of these, if any, is a Nash Equilibrium? You can check by seeing if either player would benefit by changing their action. 

| Kicker | Goalie | Equilibrium or Change? | 
|------------|-----------|--------|
| Left  | Left  |  |
| Left | Right | |
| Right | Left | |
| Right | Right | |

:::

:::{.callout-warning collapse="true" appearance="minimal"}
## Solution

There are no pure Nash equilibrium solutions because when the actions match, the Kicker will always want to change, and when they don’t match, the Goalie will always want to change. 

| Kicker | Goalie | Equilibrium or Change? | 
|------------|-----------|--------|
| Left  | Left  | Kicker changes to right  |
| Left | Right | Goalie changes to left |
| Right | Left | Goalie changes to right |
| Right | Right | Kicker changes to left |
:::


:::{.callout-note  appearance="minimal"}
## Expected Value Exercise
Assume that they both play Left 50% and Right 50% -- what is the expected value of the game? 

:::

:::{.callout-warning collapse="true" appearance="minimal"}
## Solution

| Kicker/Goalie | Lean Left (0.5) | Lean Right (0.5) | 
|------------|-----------|--------|
| Kick Left (0.5) | 0, 0  | +2, -2 |
| Kick Right (0.5) | +1, -1 | 0, 0|

We apply these probabilities to each of the 4 outcomes: 

| Kicker/Goalie | Lean Left (0.5) | Lean Right (0.5) | 
|------------|-----------|--------|
| Kick Left (0.5) | 0, 0 (0.25) | +2, -2 (0.25) |
| Kick Right (0.5) | +1, -1 (0.25) | 0, 0 (0.25) |

Now for the Kicker, we have $\mathbb{E} = 0.25*0 + 0.25*2 + 0.25*1 + 0.25*0 = 0.75$. 

Since it's zero-sum, we have $\mathbb{E} = -0.75$ for the Goalie.

Note that, for example, the Kicker playing 50% Left and 50% Right could be interpreted as a single player having these probabilities or a field of players averaging to these probabilities. So out of 100 players, this could mean: 

- 100 players playing 50% Left and 50% Right
- 50 players playing 100% Left and 50 players playing 100% Right
- 50 players playing 75% Left/25% Right and 50 players playing 25% Left/75% right

:::

When the Goalie plays left with probability $p$ and right with probability $1-p$, we can find the expected value of the Kicker actions.

| Kicker/Goalie | Lean Left (p) | Lean Right (1-p) | 
|------------|-----------|--------|
| Kick Left | 0, 0  | +2, -2 |
| Kick Right | +1, -1 | 0, 0|

$\mathbb{E}(\text{Kick Left}) = 0*p + 2*(1-p) = 2 - 2*p$

$\mathbb{E}(\text{Kick Right}) = 1*p + 0*(1-p) = 1*p$

The Kicker is going to play the best response to the Goalie’s strategy. The Goalie wants to make the Kicker **indifferent** to Kick Left and Kick Right because if the Kicker was not going to be indifferent, then he would prefer one of the actions, meaning that action would be superior to the other. Therefore the Kicker will play a mixed strategy in response that will result in a Nash equilibrium where neither player benefits from unilaterally changing strategies. (Note that indifferent does not mean 50% each, but means the expected value is the same for each.)

![](assets/kickergoalieplot.png)

By setting the values equal, we get $2 - 2*p = 1*p \Rightarrow p = \frac{2}{3}$ as shown in the plot. This means that $1-p = 1 - \frac{2}{3} = \frac{1}{3}$. Therefore the Goalie should play Lean Left $\frac{2}{3}$ and Lean Right $\frac{1}{3}$. The value for the Kicker is $\frac{2}{3}$, or $(0.67)$, for both actions, regardless of the Kicker's mixing strategy. 

Note that the Kicker is worse off now ($0.67$ now compared to $0.75$) than when both players played 50% each action. Why?

If the Kicker plays Left with probability $q$ and Right with probability $1-q$, then the Goalie’s values are: 

$\mathbb{E}(\text{Lean Left}) = 0*q - 1*(1-q) = -1 + q$

$\mathbb{E}(\text{Lean Right}) = -2*q + 0 = -2*q$

Setting equal, 

$$
\begin{equation}
\begin{split}
-1 + q &= -2*q \\
-1 &= -3*q  \\
\frac{1}{3} &= q
\end{split}
\end{equation}
$$

Therefore the Kicker should play Left $\frac{1}{3}$ and Right $\frac{2}{3}$, giving a value of $-\frac{2}{3}$ to the Goalie. 

We can see this from the game table: 

| Kicker/Goalie | Lean Left ($\frac{2}{3}$) | Lean Right ($\frac{1}{3}$) | 
|------------|-----------|--------|
| Kick Left ($\frac{1}{3}$) | 0, 0 ($\frac{2}{9}$) | +2, -2 ($\frac{1}{9}$) |
| Kick Right ($\frac{2}{3}$) | +1, -1 ($\frac{4}{9}$) | 0, 0 ($\frac{2}{9}$)|

Therefore the expected payoffs in this game are $\frac{2}{9}*0 + \frac{1}{9}*2 + \frac{4}{9}*1 + \frac{2}{9}*0 = \frac{6}{9} = 0.67$ for the Kicker and $-0.67$ for the Goalie. 

In an equilibrium, no player should be able to unilaterally improve by changing their strategy. What if the Kicker switches to always Kick Left?

| Kicker/Goalie | Lean Left ($\frac{2}{3}$) | Lean Right ($\frac{1}{3}$) | 
|------------|-----------|--------|
| Kick Left ($1$) | 0, 0 ($\frac{2}{3}$) | +2, -2 ($\frac{1}{3}$) |
| Kick Right ($0$) | +1, -1 ($0$) | 0, 0 ($0$)|

Now the Kicker's payoff is still $\frac{1}{3}*2 = 0.67$. 

When a player makes their opponent indifferent, this means that any action the opponent takes (within the set of equilibrium actions) will result in the same payoff! 

So if you know your opponent is playing the equilibrium strategy, then you can actually do whatever you want with no penalty with the mixing actions. Sort of. 

The risk is that the opponent can now deviate from equilibrium and take advantage of your new strategy. For example, if the Goalie caught on and moved to always Lean Left, then expected value is reduced to $0$ for both players. 

To summarize, you can only be penalized for not playing the equilibrium mixing strategy if your opponent plays a non-equilibrium strategy that exploits your strategy. 

:::{.callout-note  appearance="minimal"}
## Indifference
Why do players make their opponent indifferent?
:::

## Reinforcement Learning (RL)
*Many ideas in this section on RL come from Sutton and Barto's Reinforcement Learning book.* 

The main idea of reinforcement learning is that an *agent* learns by taking *actions* in its *environment*. The environment then gives feedback in the form of *rewards* and a new *state*. 

![Diagram from Sutton Barton Reinforcement Learning](assets/agentactionenv.png)

We have for each time $t$: 
- State: $S_t$
- Action: $A_t$ 
- Reward: $R_t$ 

The goal of an agent is generally to maximize the expected value of the rewards received. 

Reward values can be designed in a way to optimally train the agent. In chess for example, we could give a reward of $+1$ for winning a game, $-1$ for losing, and $0$ for all other states including draws. Defining intermediate rewards for actions like taking an opponent piece risks the agent prioritizing that goal over winning the game. 

Reward hacking is when agents find a way to obtain rewards in a  way that isn't aligned with the intended goal. (The agents will take the reward structure very literally and this is all they have to go on!) 

A Markov Decision Process (MDP) is a simplified model of the reinforcement learning problem in that the probability of future states and rewards depends only on the previous state and actions. 

This can be written as: 

$$
p(s', r \mid s, a) = \Pr\{S_t=s', R_t=r \mid S_{t-1}=s, A_{t-1}=a\}
$$

Value functions estimate the value of being in a certain state in terms of expected future rewards (which can be discounted). 

- If you're about to flip a coin and want it to be heads ($+1$) and not tails ($-1$), then the value of that state is $0$. 

- If you're in a chess game where winning is $+1$ and losing is $-1$ and everything else is $0$ and you're in a state where it's your turn and you can checkmate, then that state has a value of $+1$. 

A policy is a mapping from states to probabilties of selecting each action and is written as $\pi(a\mid s)$. 



Simple MDP example,  like buy carrots not anything else 

Policy, value function, optimal policy, optimal value function

## War
If you thought that Kuhn Poker was a simple card game, meet War. 

There are two players, each gets half the deck, 26 cards. Each player turns over their top card and faces it against the opponent's top card. The better card wins, with Aces high. This repeats until one player has all the cards. 

When the cards match, the players go to “War”. When this happens, put the next card face down, and then the card after that face up, and then these up-cards face off against each other. The winner takes all six cards. If there’s another tie, then repeat and the winner takes 10 cards, etc. 

:::{.callout-note  appearance="minimal"}
## Optional Coding Exercise
Code and run 10,000 simulations of War and determine how many turns the average game takes and how many War situations occur on average in each simulation.

:::

You can see a Dreidel game simulator written by Ben Blatt in Slate from 2014 at [this link](https://www.slate.com/articles/life/holidays/2014/12/rules_of_dreidel_the_hannukah_game_is_way_too_slow_let_s_speed_it_up.html). 

## Tictactoe 
![](assets/backprop.png)

Evolutionary algorithms for poker and tic-tac-toe

Exercise: Look at this game tree with payouts at the bottom written in terms of Player 1. Start from the bottom of the tree and figure out the actions of each player at each node. Then figure out the value of the game for Player 1 and for Player 2. This procedure is called backpropagation. 

Question: Why can’t we use this procedure in Kuhn Poker and imperfect info games? 
Tree imperfect info vs perfect info issues, show trees
Compare this to the Wabbits game and problem from paper 

### Minimax
https://www.neverstopbuilding.com/blog/minimax

### Value Function
Minimax assumes opponent playing best plays too
temporal-difference

### RL: Planning and Learning

### RL: Monte Carlo Tree Search

https://starai.cs.ucla.edu/papers/VdBBNAIC09.pdf 
What about just using imperfect info version of MCTS? 

## Multi-Armed Bandits
![](assets/10bandits.png)

Bandits are a set of problems with repeated decisions and a fixed number of actions possible coming from a single state. 

The agent updates its strategy based on what it learns from the feedback from the environment. You could think of this in real-world settings like picking which dish to eat at a restaurant. 

Consider a 10-armed bandit setup like in the image below: 

![Sutton Barto Bandits](assets/banditdist.png)

Each time the player pulls an arm, they get some reward, which could be positive or negative. 

A basic setting initializes each of 10 arms with $q*(\text{arm}) = \mathcal{N}(0,1)$ so each is initialized with a center point around a Gaussian distribution. Each pull of an arm returns a reward of $R = \mathcal{N}(q*(\text{arm}_i), 1)$. 

In other words, each arm is initialized with a value centered around $0$ but with some variance, so each will be a bit different. Then from that point, the actual pull of an arm is centered around that new point with some variance as seen in this figure above. 

The agent can sample actions and estimate their values based on experience and then use some algorithm for deciding which action to use next to maximize rewards. The estimated value of action $a$ at timestep $t$ is defined as $Q_t(a)$. 

The agent is then faced with two competing goals: 

1. Get as accurate an estimate $Q_t(a)$ as possible
2. Select actions with the highest rewards as much as possible

Exploring refers to figuring out the values of the arms, or in the case of a restaurant, figuring out how good each dish is. 

Exploiting refers to using current knowledge to choose the highest value estimated action. 

:::{.callout-note  appearance="minimal"}
## Exercise

A naive approach is that a player could sample each action once and then always use the action that gave the best reward going forward.  

What is a problem with this strategy? Can you suggest a better one? 

:::

$Q_t(a)$ can simply be estimated by averaging the rewards received each time a specific arm has been tried. The so-called *greedy* action rule is to then take the largest $Q_t(a)$ action, $A_t = \argmax_{a} Q_t(a)$

### Regret
How would you define regret in this bandit setting? 
How is minimizing regret related to maximizing reward? 
explore exploit 

### RL: Action-Value Methods
Hidden imperfect info, understand in distribution over possible states of the world
Depending on state will want to make decisions differently 
Bayesian explore exploit optimizer 
What is my opponent range exercise in Bayesian update
Explore vs. exploit
Bayesian updating
https://aipokertutorial.com/game-theory-foundation/#regret
https://www.reddit.com/r/statistics/comments/1949met/how_are_multi_armed_bandits_related_to_bayesian/ 
https://tor-lattimore.com/downloads/book/book.pdf 
https://lcalem.github.io/blog/2018/09/22/sutton-chap02-bandits#26-optimistic-initial-values 

But you could use the MCTS that does work by changing the game setup


## Blackjack
Setup
Solving


### RL: Dynamic Programming
My blog post


### RL: Monte Carlo Methods

## Gridworld
pg 60

### RL: Q-Learning 

## Poker Games
Why are we mostly interested in poker games? We think that 

imperfect
adversarial
widely played
well researched, but only nash equilibrium