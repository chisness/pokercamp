---
title: "Challenge 1: Kuhn Poker CFR Tutorial"
sidebar: aipcs24
format:
  html:
    math: true
---

# Hunting Wabbits
(This game is modified from *[Lisy et al. 2015](https://mlanctot.info/files/papers/aamas15-iioos.pdf))*.

![Bugs and Fudd](assets/bugsfudd.jpeg)

This game has one chance player (the Author), one maximizing player (Fudd) and one minimizing player (Bugs). Fudd and Bugs are in a long-term, all-out war, and so any energy that Fudd wastes or saves is a gain or loss for Bugs; our game is zero-sum.

First, the Author will choose whether to write an episode where Bugs is at the Opera (50%) or in the Forest (50%). Fudd cannot tell what the Author has chosen.

Next, Fudd will decide whether to Hunt_Near or Hunt_Far. If he chooses Hunt_Far, he takes -1 value for the extra effort.

Bugs knows whether the Author wrote him into the Opera or Forest, but he does not know Fudd's hunting location. If the Author gives him the Opera, Bugs has no choices to make and the game ends. If Forest, Bugs will decide whether to Play_Near or Play_Far. If he plays in the same location as Fudd is hunting, Fudd gets +3 value for a successful hunt. If they are in different locations (or if Bugs is at the Opera), Fudd will get 0 value for an unsuccessful hunt.

Putting it all together, the payoff structure of this game is:

| Author     | Bugs/Fudd | Hunt_Near | Hunt_Far |
|------------|-----------|-----------|----------|
| Opera      |           | 0, 0      | +1, -1   |
| Forest     | Play_Near | -3, +3    | 0, 0     |
| Forest     | Play_Far  | +1, -1    | -2, +2   |

If it were a complete-information game, the tree structure would be:

![](assets/bugsfuddtree.png)

Note that Bugs’s optimal actions depend on Fudd’s Opera action even though that outcome cannot be reached once Bugs is playing! 

If Fudd knows he’s at the Opera, then he prefers to Hunt Near, but since he doesn’t know his location, he must take both scenarios into account. 

# Concept: Information Set
In a complete-information game, we can draw a tree of the possible states the game can be in. Every time a player is called to take an action, they will know what node they are at, and what node they will go to with each legal action.

In an incomplete-information game, we can still draw that tree, but now a player might be called to take an action without knowing what node they are actually at. Instead, there will be a set of one or more nodes that are indistinguishable to that player based on what they have seen so far, and they will have to take an action knowing only that they are in that set. Such a set of nodes is an **information set** or an **infoset**.

A **player strategy** is a rule that says, for every information set that player will face, what action or (random choice of actions) that player will take. For a game like Hunting Wabbits or Kuhn Poker, we can list every information set and its probabilities. For a more complicated game, we might write our strategy as a computation that will output probabilities based on inputs and an algorithm.

# Concept: Expected Value
Once we’ve assigned definite values to the ultimate outcomes, the expected value of a situation is the value of the average outcome, weighted by the probability that you get to that outcome, respectively.

:::{.callout-note  appearance="minimal"}
## Exercise

If Bugs plays uniform 0.5 Play_Near and 0.5 Play_Far, what is the value of each Bugs node and what should Fudd do if he knew Bugs’s actions? What is Fudd’s expected value in this case?
:::

:::{.callout-warning collapse="true" appearance="minimal"}
## Solution

Solution here
Left node: +1.5 for Fudd
Right node: +0.5 for Fudd
Fudd chooses Left
:::

In imperfect-information games, we consider probabilities over two different sources of uncertainty, after assuming a particular P1 strategy and P2 strategy:

1. Uncertainty about which node we are actually in, given that we know that we’re in one of multiple nodes that we can’t tell apart.
  - The probabilities of being in node 1, node 2, … of an information set can be calculated by the probabilities of decisions upwards in the game tree (and the probabilites of chance events that have already happened).
2. Uncertainty about what will happen after we go to a node downwards in the game tree, coming from chance events or strategy probabilities in the players’ following actions.

By taking the expected value over the second type of uncertainty, we calculate the expected values of being at each node we might be going to (e.g., EV at the node [ Forest, Hunt_Near ]). Then by taking the expected value over the first type of uncertainty, we calculate the expected value of taking a given action at that information set (e.g., EV of playing Play_Near at infoset [ Forest, _ ]).

![](assets/bugsfuddtree.png)

We will focus on zero-sum two-player games, so the value to one player is simply the negative of the value to the other. Therefore, we can represent value in the game as a single number that the *maximizing player* wishes to make positive and the *minimizing player* wishes to make negative.

We will focus on maximizing (or minimizing) expected value as our goal for all of Course 1. One thing that makes it natural to care about expected value is that it’s usually the best way to predict what your score will be after a very large number of games, whether they are the same game or different from each other.

# Concept: Regret
For a given P1 strategy and P2 strategy, a player has regret when they take an action at an infoset that was not the highest-EV action at that infoset. The amount of regret is the difference between the highest-EV action and the selected action. 

:::{.callout-note  appearance="minimal"}
## Exercise

![](assets/basictree.png)

What is the regret for each action? 

| Action     | Regret |
|------------|-----------|
| A      |           |
| B     |  |
| C     |   |

:::

:::{.callout-warning collapse="true"  appearance="minimal"}
## Solution

| Action     | Regret |
|------------|-----------|
| A      |  4         |
| B     | 2  |
| C     | 0  |

:::

There are other things that “regret” can mean in English, that are separate from this technical concept:
- Based on what chance events later happened, I wish I had taken a different action instead.
- I was wrong about what strategy my opponent was playing, and I wish I had taken a different action instead.

However, we will use “regret” as a technical concept to mean only actions that are not-highest-EV given a particular P1 strategy and P2 strategy.

Some students report that they misunderstood what we mean by “regret” the first time that they read this. Go back and check the two things that regret isn’t.

Max: Explain differences here between the regret that wasn’t converging on the site and the one that is

# Exercises
:::{.callout-note  appearance="minimal"}
## Information Set

What are Fudd’s information set(s)? What are Bugs’s information set(s)?
:::

:::{.callout-note  appearance="minimal"}
## Expected Value

Say that Fudd chooses to Hunt_Near (at the appropriate information set) with probability $p$. 

![](assets/bugsfuddtreep.png)

1. At the Bugs infoset where Bugs knows he’s in the Forest, what is the expected value of choosing to Play_Near? 

2. What is the expected value of choosing to Play_Far? 

(Reminder: Bugs wants to avoid a “positive” score.)
:::

:::{.callout-note  appearance="minimal"}
## Expected Value 2

Say that Bugs chooses to Play_Near with probability $q$. 

![](assets/bugsfuddtreepq.png)


1. What is Fudd’s expected value of choosing Hunt_Near at his infoset? 

2. What is Fudd’s EV of choosing Hunt_Far at his infoset?
:::

:::{.callout-note  appearance="minimal"}
## Regret

Find a $p$ and a $q$ such that both:

1. Bugs never chooses an action with regret

2. Fudd never chooses an action with regret
:::

# Kuhn Poker
Game rules
Up down


# Challenge 1

## Submitting Agents
Running agents/submitting agents

Using the Kuhn Interactive site, find a strategy that you think is best. Go to the “competition” tab, and press “submit”. This will generate and submit a bot that plays with your chosen probabilities. (It has no memory of other rounds, and plays each round based on the probabilities.) Your submission will play 100,000 times against all other submissions (including the 10 bots that we added to start the challenge). It will play as Fudd half the time and Bugs half the time.

After a short delay, the leaderboard should update; your score will be compared to other players and selected bots. You will be listed as “tied” with another entry if your total score against all opponents is within one standard error of theirs (a statistician might say, you have a p>31%).

Mention that later we’ll talk about opponent modeling, but equilibrium is easiest concept for now


Advice: We believe you will get the most out of the course if you approach each challenge with the goal of making your bot achieve the highest score possible in each matchup. Because there is no memory between rounds, this is equivalent to maximizing your expected score in each round.

No regret for p1 vs p2 and no regret for p2 vs p1 more analogous to Bugs Fudd

Interactive questions? 

Video for exercises? Microvideos 

Ross: Make sequential so only see first round then expand?


Why can’t we just do tree search?
Because of incomplete information.

If we had complete information, then the regret-minimizing strategy downstream of a node would depend only on stuff downtree of that node, and we could – in theory – compute the strategy recursively from the bottom up. [Is this how Arimaa was solved?]

I claim that if you attempt to follow this strategy in Kuhn poker, it will fail. You can try this for yourself – if it was valid to “solve from the bottom up”, then the following steps should get to a regret-minimal strategy.
Reset
Update the strategy weights for the X_↓↑ nodes until they converge.
Update the strategy weights for the _X↓ and _X↑ nodes until they converge.
Update the strategy weights for the X_ nodes until they converge.

Follow these steps. You do not have a regret-minimal strategy. Why not? What went wrong?
Do we really need mixed strategies?
You can test this for yourself. If you set the learning rate to 1.0, updating a cell will move it all the way to playing the locally better action 100% of the time.

Follow these steps:
Reset
Set the learning rate to 1.0.
In any order you like, pick cells and update them.
Repeat 3 while there are still suboptimal moves in your strategy.

A bad thing is happening to you. Why?
Actually solving Kuhn Poker
Pick a reasonable learning rate, update individual cells until they converge.
Next, enter your strategy into the daily leaderboard.