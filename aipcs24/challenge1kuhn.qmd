---
title: "Challenge 1: Kuhn Poker CFR Tutorial"
sidebar: aipcs24
format:
  html:
    math: true
---

## Hunting Wabbits
(This game is modified from *[Lisy et al. 2015](https://mlanctot.info/files/papers/aamas15-iioos.pdf)*.)

![Bugs and Fudd](assets/bugsfudd.jpeg)

This game has one chance player (the Author), one maximizing player (Fudd) and one minimizing player (Bugs). Fudd and Bugs are in a long-term, all-out war, and so any energy that Fudd wastes or saves is a gain or loss for Bugs; our game is zero-sum.

First, the Author will choose whether to write an episode where Bugs is at the Opera (50%) or in the Forest (50%). Fudd cannot tell what the Author has chosen.

Next, Fudd will decide whether to Hunt_Near or Hunt_Far. If he chooses Hunt_Far, he takes -1 value for the extra effort.

Bugs knows whether the Author wrote him into the Opera or Forest, but he does not know Fudd's hunting location. If the Author gives him the Opera, Bugs has no choices to make and the game ends after Fudd's action. If Forest, Bugs will decide whether to Play_Near or Play_Far. If he plays in the same location as Fudd is hunting, Fudd gets +3 value for a successful hunt (for a total of +2 in the Hunt_Far action due to the -1 value for the extra effort). If they are in different locations (or if Bugs is at the Opera), Fudd will get 0 value for an unsuccessful hunt (-1 for the Hunt_Far misses).

Putting it all together, the payoff structure of this game is:

**This should probably be Fudd/Bugs to have 1st player and maximizing player first**

| Author     | Bugs/Fudd | Hunt_Near | Hunt_Far |
|------------|-----------|-----------|----------|
| Opera      |           | 0, 0      | +1, -1   |
| Forest     | Play_Near | -3, +3    | +1, -1   |
| Forest     | Play_Far  | 0, 0      | -2, +2   |

If it were a complete-information game, the tree structure would be:

![](assets/bugsfuddtree.png)

If Fudd knows he’s at the Opera, then he prefers to Hunt Near, but since he doesn’t know his location, he must take both scenarios into account. Note that Bugs’s optimal actions depend on Fudd’s Opera action even though that outcome cannot be reached once Bugs is playing! 

## Concept: Information Set
In a complete-information game, we can draw a tree of the possible states the game can be in. Every time a player is called to take an action, they will know what node they are at, and what node they will go to with each legal action.

In an incomplete-information game, we can still draw that tree, but now a player might be called to take an action without knowing what node they are actually at. Instead, there will be a set of one or more nodes that are indistinguishable to that player based on what they have seen so far, and they will have to take an action knowing only that they are in that set. Such a set of nodes is an **information set** or an **infoset**.

A **player strategy** is a rule that says, for every information set that player will face, what action or (random choice of actions) that player will take. For a game like Hunting Wabbits or Kuhn Poker, we can list every information set and its probabilities. For a more complicated game, we might write our strategy as a computation that will output probabilities based on inputs and an algorithm.

## Concept: Expected Value
Once we’ve assigned definite values to the ultimate outcomes, the expected value of a situation is the value of the average outcome, weighted by the probability that you get to that outcome, respectively.

:::{.callout-note  appearance="minimal"}
## Exercise

Suppose that Bugs plays uniform $0.5$ Play_Near and $0.5$ Play_Far.

1. What is the value of each Bugs node and what should Fudd do if he knew Bugs’s actions? 

2. What is Fudd’s expected value in this case?
:::

:::{.callout-warning collapse="true" appearance="minimal"}
## Solution

1. Bugs's node values are: 

$$
\begin{align*}
\mathbb{E}(\text{Left Node}) &= 0.5*-3 + 0.5*0 = -1.5 \\
\mathbb{E}(\text{Right Node}) &= 0.5*1 + 0.5*-2 = -0.5
\end{align*}
$$

<div style="margin-left: 2.5em;">
The values for Fudd are inverse, so Fudd prefers the Left Node with a value of $1.5$. This means that if Fudd is in the Forest, he prefers to Hunt_Near. Lucky for him, he also prefers to Hunt_Near in the Opera. 
</div>

2. Therefore Fudd will always play Hunt_Near and will have the following expected value:

$$
\begin{equation}
\begin{split}
\mathbb{E}(\text{Hunt\_Near}) &= \Pr(\text{Opera})*u(\text{Hunt\_Near}) + \Pr(\text{Forest})*u(\text{Hunt\_Near}) \\
  &= 0.5*0 + 0.5*1.5 \\
  &= 0.75
\end{split}
\end{equation}
$$

:::

In imperfect-information games, we consider probabilities over two different sources of uncertainty, after assuming a particular P1 strategy and P2 strategy:

1. Uncertainty about which node we are actually in, given that we know that we’re in one of multiple nodes that we can’t tell apart.
<div style="margin-left: 2.5em;">
  - The probabilities of being in node 1, node 2, … of an information set can be calculated by the probabilities of decisions upwards in the game tree (and the probabilites of chance events that have already happened).
</div>
2. Uncertainty about what will happen after we go to a node downwards in the game tree, coming from chance events or strategy probabilities in the players’ following actions.

**Is below paragraph clear?** 

By taking the expected value over the second type of uncertainty, we calculate the expected values of being at each node we might be going to (e.g., EV at the node [ Forest, Hunt_Near ]). Then by taking the expected value over the first type of uncertainty, we calculate the expected value of taking a given action at that information set (e.g., EV of playing Play_Near at infoset [ Forest, _ ]).

![](assets/bugsfuddtree.png)

We will focus on zero-sum two-player games, so the value to one player is simply the negative of the value to the other. Therefore, we can represent value in the game as a single number that the *maximizing player* wishes to make positive and the *minimizing player* wishes to make negative.

We will focus on maximizing (or minimizing) expected value as our goal for all of Course 1. One thing that makes it natural to care about expected value is that it’s usually the best way to predict what your score will be after a very large number of games, whether they are the same game or different from each other.

## Concept: Regret
For a given P1 strategy and P2 strategy, a player has regret when they take an action at an infoset that was not the highest-EV action at that infoset. The amount of regret is the difference between the highest-EV action and the selected action. 

:::{.callout-note  appearance="minimal"}
## Exercise

![](assets/basictree.png)

What is the regret for each action? 

| Action     | Regret |
|------------|-----------|
| A      |           |
| B     |  |
| C     |   |

:::

:::{.callout-warning collapse="true"  appearance="minimal"}
## Solution

| Action     | Regret |
|------------|-----------|
| A      |  4         |
| B     | 2  |
| C     | 0  |

:::

There are other things that “regret” can mean in English, that are separate from this technical concept:

- Based on what chance events later happened, I wish I had taken a different action instead.

- I was wrong about what strategy my opponent was playing, and I wish I had taken a different action instead.

However, we will use “regret” as a technical concept to mean how much worse actions that are not-highest-EV perform compared to highest-EV actions given a particular P1 strategy and P2 strategy.

## Exercises
:::{.callout-note  appearance="minimal"}
## Information Set

1. What are Fudd’s information set(s)? 

2. What are Bugs’s information set(s)?

![](assets/bugsfuddtree.png)
:::

:::{.callout-note  appearance="minimal"}
## Expected Value

Say that Fudd chooses to Hunt_Near (at the appropriate information set) with probability $p$. 

![](assets/bugsfuddtreep.png)

1. At the Bugs infoset where Bugs knows he’s in the Forest, what is the expected value of choosing to Play_Near? 

2. What is the expected value of choosing to Play_Far? 

(Reminder: Bugs wants to avoid a “positive” score.)
:::

:::{.callout-note  appearance="minimal"}
## Expected Value 2

Say that Bugs chooses to Play_Near with probability $q$. 

![](assets/bugsfuddtreepq.png)

1. What is Fudd’s expected value of choosing Hunt_Near at his infoset? 

2. What is Fudd’s EV of choosing Hunt_Far at his infoset?
:::

:::{.callout-note  appearance="minimal"}
## Regret

Find a $p$ and a $q$ such that both:

1. Bugs never chooses an action with regret

2. Fudd never chooses an action with regret

:::{.callout-tip collapse="true"  appearance="minimal"}
## Hint

In order to have no regret, the expected value of both actions should be equal
:::
:::

## Kuhn Poker
[Kuhn Poker](https://en.wikipedia.org/wiki/Kuhn_poker) is the most simple version of poker.

![](assets/kuhnbee.png)

- 3 card deck: Queen, King, Ace (Ace is highest)

- 2 players

- Each player starts with 2 chips and antes 1

- Deal 1 card to each player (discard the 3rd)

- Players can take Up or Down actions

  - Up actions indicate a Bet or Call (putting a chip into the pot)

  - Down actions indicate a Check or Fold (not putting a chip into the pot)

- Rotate who acts first each hand

- There is one betting round that can take the following sequences: 

| Sequence   | Player 1  | Player 2  | Player 1 | Winner       |
|------------|-----------|-----------|----------|--------------|
| Sequence 1 | Down      | Up        | Down     |Player 2 (+1) |
| Sequence 2 | Down      | Up        | Up       |High Card (+2)|
| Sequence 3 | Down      | Down      |          |High Card (+1)|
| Sequence 4 | Up        | Down      |          |Player 1 (+1) |
| Sequence 5 | Up        | Up        |          |High Card (+2)|

## Challenge 1
Play with Kuhn Poker strategies on our [interactive site](https://static.rossry.net/sketches/poker/kuhn/) and submit once you find the strategy that you think is best. 

After a short delay, the leaderboard should update; your score will be compared to other players and selected bots. You will be listed as “tied” with another entry if your total score against all opponents is within one standard error of theirs (a statistician might say, you have a p>31%).

Advice: We believe you will get the most out of the course if you approach each challenge with the goal of making your bot achieve the highest score possible in each matchup. Because there is no memory between rounds, this is equivalent to maximizing your expected score in each round.

### How Kuhn Interactive works
TO DO

### Why can’t we just do tree search?

Because of incomplete information.

If we had complete information, then the regret-minimizing strategy downstream of a node would depend only on stuff downtree of that node, and we could – in theory – compute the strategy recursively from the bottom up.

If you attempt to follow this strategy in Kuhn poker, it will fail. You can try this for yourself – if it was valid to “solve from the bottom up”, then the following steps should get to a regret-minimal strategy: 

1. Reset
2. Update the strategy weights for the X_↓↑ nodes until they converge.
3. Update the strategy weights for the _X↓ and _X↑ nodes until they converge.
4. Update the strategy weights for the X_ nodes until they converge.

Follow these steps. You do not have a regret-minimal strategy. Why not? What went wrong? 

### Do we really need mixed strategies?

You can test this for yourself. If you set the learning rate to 1.0, updating a cell will move it all the way to playing the locally better action 100% of the time.

Follow these steps:

1. Reset
2. Set the learning rate to 1.0.
3. In any order you like, pick cells and update them.
4. Repeat 3 while there are still suboptimal moves in your strategy.

A bad thing is happening to you. Why?

### Actually solving Kuhn Poker
Pick a reasonable learning rate, update individual cells until they converge.

Next, enter your strategy into the daily leaderboard.

### How results are generated 
This will generate and submit a bot that plays with your chosen probabilities. (It has no memory of other rounds, and plays each round based on the probabilities.) Your submission will play 100,000 times against all other submissions (including the 10 bots that we added to start the challenge). It will play as P1 half the time and P2 half the time. Each 1v1 will be played in duplicate. 