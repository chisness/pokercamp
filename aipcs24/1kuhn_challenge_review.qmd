---
title: "#1: Kuhn Poker | Challenge Review"
sidebar: aipcs24
format:
  html:
    math: true
---

Explain differences here between the regret that wasnâ€™t converging on the site and the one that is

Average strategy thing 
Regret min thing from cfr.pdf 
Failure mode spinning around the right solution, want to spin inwards/converge

## Review of Challenge 1
Goal: Equilibrium agent, later opponent modeling

What does best bot vs you look like? What is your exploitability? How do we evaluate agents?

### Tournament Results
Yep, just a slight variation here where the dealer burns 26/52 cards and then you play against the house; make it something other than just off-the-shelf and make your MC solver have to do some real work.


### How the Interactive Works
Odds vs. probs
Regret
Full CFR details next session
Show equations and graphs of how things correct when a single probability is thrown off

### Optimal Strategies
Surprise! There are multiple Nash equilibria in Kuhn!

Value of the game

Position thing

Principle of 3 general types of hands and how it applies to regular poker

### Pure vs. Mixed Strategies

Gradient Descent: Involves iteratively adjusting parameters to minimize a cost function. Each step moves the parameters in the direction of the negative gradient of the cost function, gradually converging to a local or global minimum. Aims to converge to the optimal parameters that minimize the cost function. With an appropriate learning rate and sufficient iterations, it can find the minimum. Utilizes feedback from the gradient of the cost function at each iteration to update the parameters. Focuses on a static objective function (cost function) and aims to find its minimum.


Regret Minimization: In online learning and decision-making contexts, it involves iteratively updating strategies to minimize regret, which is the difference between the actual cumulative loss and the best possible cumulative loss in hindsight. Each step adjusts the strategy based on past performance to improve future decisions. Aims to minimize regret over time, which means the strategy becomes nearly as good as the best fixed strategy in hindsight. With enough iterations, the average regret per iteration tends to zero. Utilizes feedback from past performance (losses) to update the strategy, aiming to reduce future regret. Focuses on a dynamic objective (minimizing regret over time) in potentially changing environments, where the best action may vary over time.

Regret Matching: Involves iteratively updating the probability distribution over actions based on past regrets. Actions with higher regrets (indicating they would have performed better in the past) are chosen more frequently in the future. Uses past regrets to update the probability distribution over actions. The probability of selecting each action increases proportionally to the regret of not having taken that action.

Pruning and compare to CFR 