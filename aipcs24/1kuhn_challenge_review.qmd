---
title: "#1 Kuhn Poker: Challenge Review"
sidebar: aipcs24
format:
  html:
    math: true
---

Explain differences here between the regret that wasn’t converging on the site and the one that is

Average strategy thing 
Regret min thing from cfr.pdf 
Failure mode spinning around the right solution, want to spin inwards/converge

### Indifference in Poker 
Back to poker. We can apply this indifference principle in computing equilibrium strategies in poker. When you make your opponent indifferent, then you don’t give them any best play. 

If you play an equilibrium strategy, opponents will only get penalized for playing hands outside of the set of hands in the mixed strategy equilibrium (also known as the support, or the set of pure strategies that are played with non-zero probability under the mixed strategy). If opponents are not playing equilibrium, though, then they open themselves up to exploitation. 

Let’s look at one particular situation in Kuhn Poker and work it out by hand. Suppose that you are Player 2 with card Q after a Check from Player 1.

![](assets/kuhnindiff.png)

:::{.callout-note  appearance="minimal"}
## Expected Value Exercise
What indifference is Player 2 trying to induce? 

:::

:::{.callout-warning collapse="true" appearance="minimal"}
## Solution
Making P1 indifferent between calling and folding with a K 
:::

We can work out Player 2's betting strategy by calculating the indifference. Let $b$ be the probability that P2 bets with a Q after P1 checks. 


$$
\begin{equation}
\begin{split}
\mathbb{E}(\text{P1 Check K then Fold to Bet}) &= 0 \\
\\

\mathbb{E}(\text{P1 Check K then Call Bet}) &= -1*\Pr(\text{P2 has A and Bets}) + 3*\Pr(\text{P2 has Q and Bets}) \\
  &= -1*\frac{1}{2} + 3*\frac{1}{2}*b \\
  &= -0.5 + 1.5*b
\end{split}
\end{equation}
$$

Setting these equal: 

$0 = -0.5 + 1.5*b$ 

$b = \frac{1}{3}$

Therefore in equilibrium, P2 should bet $\frac{1}{3}$ with Q after P1 checks.  

:::{.callout-note  appearance="minimal"}
## Equilibrium Mixed Strategy Change Exercise
If P2 bet $\frac{2}{3}$ instead of $\frac{1}{3}$ with Q after P1 checks and P1 is playing an equilibrium strategy, how would P2’s expected value change? 

:::

:::{.callout-warning collapse="true" appearance="minimal"}
## Solution
It wouldn't! As long as P1 doesn't modify their strategy, then P2 can mix his strategy however he wants and have the same EV. 

:::

:::{.callout-note  appearance="minimal"}
## Bluff:Value Ratio Exercise
Given that P2 has bet after P1 checks and is playing the equilibrium strategy, what is the probability that they are bluffing?

(Note: Including cases where you have an A, so Q bets are bluffs and A bets are value bets.)

:::

:::{.callout-warning collapse="true" appearance="minimal"}
## Solution
P2 has Q and A each $\frac{1}{2}$ of the time. 

P2 is betting Q $\frac{1}{3}$ of the time (bluffing). 

P2 is betting A always (value betting). 

Therefore for every 3 times you have Q you will bet once and for every 3 times you have A you will bet 3 times. Out of the 4 bets, 1 of them is a bluff. 

$\Pr(\text{P2 Bluff after P1 Check}) = \frac{1}{4}$

:::

## TO DO BELOW 

TO DO: 
- What if the bet size was 2 instead of 1?

- Can you come up with a general formula for how often to bluff given a pot and bet size? 

- Range of hands vs. range of hands vs. your hand vs. their strategy. Different if their strategy never changes.  

- What is a creative strategy to try against a player who studies poker and is trying to play an equilibrium strategy, but might be open to changing their strategy? Bet sizes they haven’t studied, do something crazy and then counterexploit?

- Node locking question 


In what case could we deduce our opponent’s strategy with a large sample size? Note: We can assume that they are playing the pure strategy decisions correctly. 

For example, if we as P1 Check with K, we can know how often they’re betting Q since we know they’ll always bet A. 

Over a sample of 1000 hands that we Check with K, we expect that 500 times they would have Q and 500 times they would have A. We expect that they would always bet with A and bet some percentage $p$ of their Q hands. 

Formula for how often they’re betting Q. 

What’s another case we could determine? Bet Q, how often call/fold K. How to use this?

## Review of Challenge 1
Goal: Equilibrium agent, later opponent modeling

What does best bot vs you look like? What is your exploitability? How do we evaluate agents?

### Tournament Results
Yep, just a slight variation here where the dealer burns 26/52 cards and then you play against the house; make it something other than just off-the-shelf and make your MC solver have to do some real work.


### How the Interactive Works
Odds vs. probs
Regret
Full CFR details next session
Show equations and graphs of how things correct when a single probability is thrown off

### Optimal Strategies
Surprise! There are multiple Nash equilibria in Kuhn!

Value of the game

Position thing

Principle of 3 general types of hands and how it applies to regular poker

### Pure vs. Mixed Strategies

Gradient Descent: Involves iteratively adjusting parameters to minimize a cost function. Each step moves the parameters in the direction of the negative gradient of the cost function, gradually converging to a local or global minimum. Aims to converge to the optimal parameters that minimize the cost function. With an appropriate learning rate and sufficient iterations, it can find the minimum. Utilizes feedback from the gradient of the cost function at each iteration to update the parameters. Focuses on a static objective function (cost function) and aims to find its minimum.


Regret Minimization: In online learning and decision-making contexts, it involves iteratively updating strategies to minimize regret, which is the difference between the actual cumulative loss and the best possible cumulative loss in hindsight. Each step adjusts the strategy based on past performance to improve future decisions. Aims to minimize regret over time, which means the strategy becomes nearly as good as the best fixed strategy in hindsight. With enough iterations, the average regret per iteration tends to zero. Utilizes feedback from past performance (losses) to update the strategy, aiming to reduce future regret. Focuses on a dynamic objective (minimizing regret over time) in potentially changing environments, where the best action may vary over time.

Regret Matching: Involves iteratively updating the probability distribution over actions based on past regrets. Actions with higher regrets (indicating they would have performed better in the past) are chosen more frequently in the future. Uses past regrets to update the probability distribution over actions. The probability of selecting each action increases proportionally to the regret of not having taken that action.

Pruning and compare to CFR 