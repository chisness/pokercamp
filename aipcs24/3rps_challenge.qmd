---
title: "#3 Rock Paper Scissors: Challenge"
sidebar: aipcs24
format:
  html:
    math: true
---

## Rock Paper Scissors

<iframe width="560" height="315" src="https://www.youtube.com/embed/b0SoKWLkmLU?si=0AQ3_NgHOe4tc4OU" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>

Rock defeats scissors, scissors defeats paper, and paper defeats rock. You get 1 point for winning, -1 point for losing, and 0 points for ties. 

The goal of this challenge is to focus on tracking opponent distributions and how to respond to them. 

### Game Theory
RPS is a zero-sum game and the payouts are symmetrical as follows: 

| Player 1/2 | Rock    | Paper   | Scissors |
|--------|---------|---------|----------|
| Rock   | (0, 0)  | (-1, 1) | (1, -1)  |
| Paper  | (1, -1) | (0, 0)  | (-1, 1)  |
|Scissors| (-1, 1) | (1, -1) | (0, 0)   |

The Nash Equilibrium strategy is to play each action $r = p = s = 1/3$ of the time. 

:::{.callout-tip collapse="true" appearance="minimal"}
## Nash Equilibrium Strategy for RPS
If Player 1 plays Rock with probability $r$, Paper with probability $p$, and Scissors with probability $s$, we have the following expected value equations for Player 2: 

$\mathbb{E}(\text{R}) = -1*p + 1*s$

$\mathbb{E}(\text{P}) = 1*r - 1*s$

$\mathbb{E}(\text{S}) = -1*r + 1*p$

Since no action dominates, we know that the EV of every strategic action should be equal. 

$\mathbb{E}(\text{R}) = \mathbb{E}(\text{P})$
$-1*p + 1*s = 1*r - 1*s$
$2*s = p + r$

$\mathbb{E}(\text{R}) = \mathbb{E}(\text{S})$

$-1*p + 1*s = -1*r + 1*p$
$s + r = 2*p$

$\mathbb{E}(\text{P}) = \mathbb{E}(\text{S})$
$1*r - 1*s = -1*r + 1*p$
$2*r = s + p$

Solving the equations: 

$r = 2*s - p$
$s + (2*s - p) = 2*p$
$3*s = 3*p$
$s = p$

$s + r = 2*p$
$s + r = 2*s$
$r = s$

We know that all are equal: 

$s = p = r$

We also know that they must all sum to $1$: 

$r + p + s = 1$
$3*r = 1$
$r = 1/3$

So all actions are taken with probability $1/3$: 

$r = p = s = 1/3$
:::

Playing this strategy means that whatever your opponent does, you will breakeven! 

### Types of Opponent Adaptation
1. Offline: In Kuhn Poker, we discussed building an agent based on weights that play optimally against a specific fixed opponent or group of opponents. 

:::{.callout-note  appearance="minimal"}
## Best Strategy in RPS Against Fixed Opponent
How would you maximize in RPS knowing the opponent plays a fixed non-Nash strategy that you know?

:::

:::{.callout-warning collapse="true" appearance="minimal"}
## Solution
If we knew that our opponent played Rock 100% of the time, we should play Paper 100% of the time. 

In general, if our opponent plays a mixed strategy, there is always a best pure strategy that is the move that beats their most-played action. 

:::

2. Online: Now in this challenge we want to think about *online* opponent adaptation. Suppose now that we don't know how our opponent is going to play, but want to build an agent that adapts to them in real time as the match is going on. 

## Regret Matching
In general, we define regret as: 

$\text{Regret} = u(\text{Alternative Strategy}) − u(\text{Current Strategy})$

We prefer alternative actions with high regret. 

:::{.callout-tip collapse="true" appearance="minimal"}
## RPS Regret Details
We play Rock and opponent plays Paper $\implies \text{u(rock,paper)} = -1$

$\text{Regret(scissors)} = \text{u(scissors,paper)} - \text{u(rock,paper)} = 1-(-1) = 2$

$\text{Regret(paper)} = \text{u(paper,paper)} - \text{u(rock,paper)} = 0-(-1) = 1$

$\text{Regret(rock)} = \text{u(rock,paper)} - \text{u(rock,paper)} = -1-(-1) = 0$

We play Scissors and opponent plays Paper $\implies \text{u(scissors,paper)} = 1$

$\text{Regret(scissors)} = \text{u(scissors,paper)} - \text{u(scissors,paper)} = 1-1 = 0$

$\text{Regret(paper)} = \text{u(paper,paper)} - \text{u(scissors,paper)} = 0-1 = -1$

$\text{Regret(rock)} = \text{u(rock,paper)} - \text{u(scissors,paper)} = -1-1 = -2$

We play Paper and opponent plays Paper $\implies \text{u(paper,paper)} = 0$

$\text{Regret(scissors)} = \text{u(scissors,paper)} - \text{u(paper,paper)} = 1-0 = 1$

$\text{Regret(paper)} = \text{u(paper,paper)} - \text{u(paper,paper)} = 0-0 = 0$$

$\text{Regret(rock)} = \text{u(rock,paper)} - \text{u(paper,paper)} = -1-0 = -1$

To generalize:

- The action played always gets a regret of 0 since the "alternative" is really just that same action
- When we play a tying action, the alternative losing action gets a regret of -1 and the alternative winning action gets a regret of +1
- When we play a winning action, the alternative tying action gets a regret of -1 and the alternative losing action gets a regret of -2
- When we play a losing action, the alternative winning action gets a regret of +2 and the alternative tying action gets a regret of +1
:::

After each play, we accumulate regrets for each of the 3 actions. 

Then going forward, we play the strategy using regret matching, which means playing a strategy that normalizes over the *positive* accumulated regrets, i.e. playing in proportion to the positive regrets.

Let: 

- $a$ represent actions
- $\sigma$ represent the strategy
- $t$ is the time
- $i$ is the player
- $R$ is the cumulative regret

$$
\sigma_i^t(a) = \begin{cases}
\frac{\max(R_i^t(a), 0)}{\sum_{a' \in A} \max(R_i^t(a'), 0)} & \text{if } \sum_{a' \in A} \max(R_i^t(a'), 0) > 0 \\
\frac{1}{|A|} & \text{otherwise}
\end{cases}
$$

This is showing that we take the cumulative regret for an action divided by the cumulative regrets for all actions (normalizing) and then play that strategy for this action. 

If cumulative regrets for an action are $<0$ then we use $0$. 

If all cumulative regrets are $\leq 0$ then we use the uniform distribution.  

In code: 

```python
	def get_strategy(self):
  #First find the normalizing sum
		normalizing_sum = 0
		for a in range(NUM_ACTIONS):
			if self.regret_sum[a] > 0:
				self.strategy[a] = self.regret_sum[a]
			else:
				self.strategy[a] = 0
			normalizing_sum += self.strategy[a]

    #Then normalize each action
		for a in range(NUM_ACTIONS):
			if normalizing_sum > 0:
				self.strategy[a] /= normalizing_sum
			else:
				self.strategy[a] = 1.0/NUM_ACTIONS
			self.strategy_sum[a] += self.strategy[a]

		return self.strategy
```

If two players were training using regret matching, they would converge to the Nash Equilibrium of $1/3$ for each action. 

### RPS Regret Matching Experiment

Suppose that your opponent Player 2 is playing 40% Rock, 30% Paper, and 30% Scissors. Here is a regret matching 10,000 game experiment. It shows that it takes around 1600 games before Player 1 plays only Paper (this will vary). 

![](assets/rps_rm.png)

We see that if there is a fixed player, regret matching converges to the best strategy. 

But what if your opponent is not using a fixed strategy? We'll talk about that soon. 

### Data: Talk Paper Scissors
[eieio games](https://eieio.games/nonsense/game-13-talk-paper-scissors/) made a Rock Paper Scissors over voice game in which players call a phone number and get matched up with another player for a 3 game RPS match. 

They published their 40,000 round data on X: 

![Overall: R 37.2%, P 35.4%, S 27.4%](assets/tps.png)

![Round 1: R 39.7%, P 37.6%, S 22.7%](assets/tpsr1.png)

![Round 2: R 34.0%, 33.4%, 32.6%](assets/tpsr2.png)

![Round 3: R 37.2%, 34.7%, 28.1%](assets/tpsr3.png)

:::{.callout-note  appearance="minimal"}
## Expected Value Against TPS Player
What is the best strategy per round against the average TPS player? What is your expected value per round and overall?
:::

:::{.callout-warning collapse="true" appearance="minimal"}
## Solution
The best strategy is to always play Paper. 

$EV(Round 1) = 0.397*1 + 0.376*0 + 0.227*-1 = 0.17$

$EV(Round 2) = 0.34*1 + 0.334*0 + 0.326*-1 = 0.014$

$EV(Round 3) = 0.372*1 + 0.347*0 + 0.281*-1 = 0.091$

$EV(Overall) = 0.17 + 0.014 + 0.091 = 0.275$

:::

## Exercises

:::{.callout-note  appearance="minimal"}
## Maximize Against non-Nash Fixed Opponent
How would you maximize in RPS knowing the opponent plays a fixed non-Nash strategy that you don't know?
:::

:::{.callout-warning collapse="true" appearance="minimal"}
## Solution
One option is to play the equilibrium strategy until you get a significant sample on your opponent and then to exploit their strategy going forward. 
:::

:::{.callout-note  appearance="minimal"}
## Maximize Against Adapting Rock Opponent
What if the opponent is adapting to you, but 10% of the time they are forced to play Rock?
:::

<!-- :::{.callout-warning collapse="true" appearance="minimal"}
## Solution
::: -->

:::{.callout-note  appearance="minimal"}
## Strategy Against No-Rock Opponent
What is the optimal play if your opponent can't play rock? 
:::

<!-- :::{.callout-warning collapse="true" appearance="minimal"}
## Solution
::: -->

:::{.callout-note  appearance="minimal"}
## Skewed rock Payoff
What is the equilibrium strategy if the payoff for Rock over Paper is 2 (others stay the same)? 
:::

<!-- :::{.callout-warning collapse="true" appearance="minimal"}
## Solution
::: -->

## RPS Academic Research

1. [*Nonparametric Strategy Test*, Sam Ganzfried](https://arxiv.org/abs/2312.10695)

This paper takes Rock Paper Scissors sample data from 500 human players who each played 50 games. It does an experiment to see if (a) they are playing the uniform 1/3 1/3 1/3 strategy and (b) that the actions chosen are independent between games. The paper finds that 61% of players are doing so. 

2. 

Humans are predictably irrational

What he discovered was that the student’s decisions during the game tended towards the Nash Equilibrium, which would be picking rock 1/3 of the time, scissors 1/3 of the time, and paper 1/3 of the time. However, despite the Nash Equilibrium, he noticed a pattern with how the games were played. Winners tended to stick with their same strategy, and losers would move on to another strategy. This would repeat for every time a player lost, which he calls “persistent cyclic flows”.

1. Basics of non-Nash responses

1. Advice on parametrizing opponent strategies

2. Online CFR Minimization won’t cut it here – why?


## RPS Bot Ideas

1. Look at opponent action 2 moves ago and randomize between that action and the action that beats it 

2. Look at opponent action 2 moves ago and randomize between the other 2 actions

3. [Iocaine Powder](https://web.archive.org/web/20160819141717id_/http://www.ofb.net/~egnor/iocaine.html): 

4. Use online CFR

5. Beat last move

6. Beat last 10 moves

7. If win, do the same thing again. If lose, randomize. 

8. Look at my own history and what opponent will estimate that I'm playing. Beat the thing they will play to beat me. 

9. Use percentages that are fixed/slowly moving

10. Track opponent with +1