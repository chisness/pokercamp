[
  {
    "objectID": "aicamp/index.html",
    "href": "aicamp/index.html",
    "title": "AI Poker Camp",
    "section": "",
    "text": "Coming soon:\n\nJul 15-Aug 15: Beta in SF\nSep TBD: Beta v2 in NYC\nSep 24-Nov 26: Full course virtual"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Poker Camp",
    "section": "",
    "text": "Poker Camp is a program to learn about probability, game theory, AI, and decision making under uncertainty through the lens of poker.\nOur first camp will take place from Jul 15 to Aug 15 in person in San Francisco.\n\n\n\n\n\n\nSign up now!\n\n\n\nSignup Form\n\nAfter signing up, we’ll be in touch with all details\nThe beta program is free!\n\n\n\nClick here for more info\n\n\n\nNote that the programming and related materials/workshops are for educational purposes and will not use any real money."
  },
  {
    "objectID": "aipcs24/2othergames_reading.html",
    "href": "aipcs24/2othergames_reading.html",
    "title": "Other Games (2): Reading",
    "section": "",
    "text": "This course is primarily about poker and poker-adjacent games, but let’s take a step back and look at different classes of games and where poker fits into this table.\n\n\n\n\n\n\nExercise\n\n\n\nFill in the table below with games you know about:\n\n\n\n\n\n\n\n\n\n\nGame/Opponent\nFixed/Probabilistic Opponent\nAdversarial Opponent\n\n\n\n\nImperfect Info, No Player Agency/Decisions\n\n\n\n\nPerfect Info, Player Actions Always Perfect Info\n\n\n\n\nImperfect Info, Imperfect from Randomness\n\n\n\n\nImperfect Info, Imperfect from Randomness AND Game States\n\n\n\n\n\n\n\n\n\n\n\nPre-Filled Table\n\n\n\n\n\n\n\n\n\n\n\n\n\nGame/Opponent\nFixed/Probabilistic Opponent\nAdversarial Opponent\n\n\n\n\nImperfect Info, No Player Agency/Decisions\nCandy Land, War, Dreidel, Bingo, Chutes and Ladders, Slot machine\n\n\n\nPerfect Info, Player Actions Always Perfect Info\nPuzzles\nTictactoe, Checkers, Chess, Arimaa, Go\n\n\nImperfect Info, Imperfect from Randomness\nBlackjack\nBackgammon\n\n\nImperfect Info, Imperfect from Randomness AND Game States\nPartial Info Multi-armed Bandit\nPoker, Rock Paper Scissors, Liar’s Dice, Figgie\n\n\n\n\n\n\n\n\n\n\n\n\nExercise\n\n\n\nWhat makes poker and other games in the bottom right of the table interesting?\n\n\nWhat about solitaire? With Blackjack? What about a lottery? Mahjong?\n(Ross note: The difference between “Chance” and “Imperfect Info” is that in Chance, the unknown [thing] doesn’t affect anything about the world until it becomes known, and then it’s not unknown any more. In Imperfect Info, the information has some effect on the world at time T1, then you need to make a decision at time T2, then the information will matter at some later point T3.)",
    "crumbs": [
      "Signup: AI Poker Camp Summer 2024",
      "Challenges",
      "2: Other Games",
      "Reading"
    ]
  },
  {
    "objectID": "aipcs24/2othergames_reading.html#types-of-games",
    "href": "aipcs24/2othergames_reading.html#types-of-games",
    "title": "Other Games (2): Reading",
    "section": "",
    "text": "This course is primarily about poker and poker-adjacent games, but let’s take a step back and look at different classes of games and where poker fits into this table.\n\n\n\n\n\n\nExercise\n\n\n\nFill in the table below with games you know about:\n\n\n\n\n\n\n\n\n\n\nGame/Opponent\nFixed/Probabilistic Opponent\nAdversarial Opponent\n\n\n\n\nImperfect Info, No Player Agency/Decisions\n\n\n\n\nPerfect Info, Player Actions Always Perfect Info\n\n\n\n\nImperfect Info, Imperfect from Randomness\n\n\n\n\nImperfect Info, Imperfect from Randomness AND Game States\n\n\n\n\n\n\n\n\n\n\n\nPre-Filled Table\n\n\n\n\n\n\n\n\n\n\n\n\n\nGame/Opponent\nFixed/Probabilistic Opponent\nAdversarial Opponent\n\n\n\n\nImperfect Info, No Player Agency/Decisions\nCandy Land, War, Dreidel, Bingo, Chutes and Ladders, Slot machine\n\n\n\nPerfect Info, Player Actions Always Perfect Info\nPuzzles\nTictactoe, Checkers, Chess, Arimaa, Go\n\n\nImperfect Info, Imperfect from Randomness\nBlackjack\nBackgammon\n\n\nImperfect Info, Imperfect from Randomness AND Game States\nPartial Info Multi-armed Bandit\nPoker, Rock Paper Scissors, Liar’s Dice, Figgie\n\n\n\n\n\n\n\n\n\n\n\n\nExercise\n\n\n\nWhat makes poker and other games in the bottom right of the table interesting?\n\n\nWhat about solitaire? With Blackjack? What about a lottery? Mahjong?\n(Ross note: The difference between “Chance” and “Imperfect Info” is that in Chance, the unknown [thing] doesn’t affect anything about the world until it becomes known, and then it’s not unknown any more. In Imperfect Info, the information has some effect on the world at time T1, then you need to make a decision at time T2, then the information will matter at some later point T3.)",
    "crumbs": [
      "Signup: AI Poker Camp Summer 2024",
      "Challenges",
      "2: Other Games",
      "Reading"
    ]
  },
  {
    "objectID": "aipcs24/2othergames_reading.html#reinforcement-learning-rl",
    "href": "aipcs24/2othergames_reading.html#reinforcement-learning-rl",
    "title": "Other Games (2): Reading",
    "section": "Reinforcement Learning (RL)",
    "text": "Reinforcement Learning (RL)\nThe main idea of reinforcement learning is that an agent learns by interacting with its environment. The main elements of a learning system are: a policy, a reward, a value function, and sometimes a model of the environment.\nAgent, environment interface RL book chapter 3 stuff Tabular environment stuff TD/SARSA/Q-LEARNING, relation to CFR https://arena3-chapter2-rl.streamlit.app/[2.1]_Intro_to_RL",
    "crumbs": [
      "Signup: AI Poker Camp Summer 2024",
      "Challenges",
      "2: Other Games",
      "Reading"
    ]
  },
  {
    "objectID": "aipcs24/2othergames_reading.html#war",
    "href": "aipcs24/2othergames_reading.html#war",
    "title": "Other Games (2): Reading",
    "section": "War",
    "text": "War\nIf you thought that Kuhn Poker was a simple card game, meet War.\nThere are two players, each gets half the deck, 26 cards. Each player turns over their top card and faces it against the opponent’s top card. The better card wins, with Aces high. This repeats until one player has all the cards.\nWhen the cards match, the players go to “War”. When this happens, put the next card face down, and then the card after that face up, and then these up-cards face off against each other. The winner takes all six cards. If there’s another tie, then repeat and the winner takes 10 cards, etc.\n\n\n\n\n\n\nOptional Coding Exercise\n\n\n\nCode and run 10,000 simulations of War and determine how many turns the average game takes and how many War situations occur on average in each simulation.\n\n\nYou can see a Dreidel game simulator written by Ben Blatt in Slate from 2014 at this link.",
    "crumbs": [
      "Signup: AI Poker Camp Summer 2024",
      "Challenges",
      "2: Other Games",
      "Reading"
    ]
  },
  {
    "objectID": "aipcs24/2othergames_reading.html#tictactoe",
    "href": "aipcs24/2othergames_reading.html#tictactoe",
    "title": "Other Games (2): Reading",
    "section": "Tictactoe",
    "text": "Tictactoe\n\nEvolutionary algorithms for poker and tic-tac-toe\nExercise: Look at this game tree with payouts at the bottom written in terms of Player 1. Start from the bottom of the tree and figure out the actions of each player at each node. Then figure out the value of the game for Player 1 and for Player 2. This procedure is called backpropagation.\nQuestion: Why can’t we use this procedure in Kuhn Poker and imperfect info games? Tree imperfect info vs perfect info issues, show trees Compare this to the Wabbits game and problem from paper\n\nMinimax\nhttps://www.neverstopbuilding.com/blog/minimax\n\n\nValue Function\nMinimax assumes opponent playing best plays too temporal-difference\n\n\nRL: Planning\nMCTS decision-time planning\nRL pages 8-12 MCTS 8.11 https://starai.cs.ucla.edu/papers/VdBBNAIC09.pdf What about just using imperfect info version of MCTS?",
    "crumbs": [
      "Signup: AI Poker Camp Summer 2024",
      "Challenges",
      "2: Other Games",
      "Reading"
    ]
  },
  {
    "objectID": "aipcs24/2othergames_reading.html#multi-armed-bandits",
    "href": "aipcs24/2othergames_reading.html#multi-armed-bandits",
    "title": "Other Games (2): Reading",
    "section": "Multi-Armed Bandits",
    "text": "Multi-Armed Bandits\n RL book 26-44, explore exploit Bandits are a set of problems with repeated decisions and a fixed number of actions possible. This is related to reinforcement learning because the agent player updates its strategy based on what it learns from the feedback from the environment. Consider a 10-armed bandit setup like in the image below:\nEach time the player pulls an arm, they get some reward, which could be positive or negative.\nA basic setting initializes each of 10 arms with q(arm) = N(0,1) so each is initialized with a center point around a Gaussian distribution. Each pull of an arm returns a reward of R = N(q(arm), 1).\nIn other words, each arm is initialized with a value centered around 0 but with some variance, so each will be a bit different. Then from that point, the actual pull of an arm is centered around that new point with some variance as seen in this figure with a 10-armed bandit from the book Intro to Reinforcement Learning by Sutton and Barto:\n\nThe player can sample actions and estimate their values based on experience and then use some algorithm for deciding which action to use next to maximize rewards.\n\n\n\n\n\n\nExercise\n\n\n\nFor example, a naive approach is that a player could sample each action once and then always use the action that gave the best reward going forward.\nWhat is a problem with this strategy? Can you suggest a better one?\n\n\n\nRegret\nHow would you define regret in this bandit setting? How is minimizing regret related to maximizing reward?\n\n\nRL: Action-Value Methods\nHidden imperfect info, understand in distribution over possible states of the world Depending on state will want to make decisions differently Bayesian explore exploit optimizer What is my opponent range exercise in Bayesian update Explore vs. exploit Bayesian updating https://aipokertutorial.com/game-theory-foundation/#regret https://www.reddit.com/r/statistics/comments/1949met/how_are_multi_armed_bandits_related_to_bayesian/ https://tor-lattimore.com/downloads/book/book.pdf https://lcalem.github.io/blog/2018/09/22/sutton-chap02-bandits#26-optimistic-initial-values\nBut you could use the MCTS that does work by changing the game setup",
    "crumbs": [
      "Signup: AI Poker Camp Summer 2024",
      "Challenges",
      "2: Other Games",
      "Reading"
    ]
  },
  {
    "objectID": "aipcs24/2othergames_reading.html#blackjack",
    "href": "aipcs24/2othergames_reading.html#blackjack",
    "title": "Other Games (2): Reading",
    "section": "Blackjack",
    "text": "Blackjack\nSetup Solving\n\nRL: Dynamic Programming\nMy blog post\n\n\nRL: Monte Carlo Methods",
    "crumbs": [
      "Signup: AI Poker Camp Summer 2024",
      "Challenges",
      "2: Other Games",
      "Reading"
    ]
  },
  {
    "objectID": "aipcs24/2othergames_reading.html#poker-games",
    "href": "aipcs24/2othergames_reading.html#poker-games",
    "title": "Other Games (2): Reading",
    "section": "Poker Games",
    "text": "Poker Games\nWhy are we mostly interested in poker games? We think that\nimperfect adversarial widely played well researched, but only nash equilibrium",
    "crumbs": [
      "Signup: AI Poker Camp Summer 2024",
      "Challenges",
      "2: Other Games",
      "Reading"
    ]
  },
  {
    "objectID": "aipcs24/1kuhn_reading.html",
    "href": "aipcs24/1kuhn_reading.html",
    "title": "Kuhn Poker (1): Reading",
    "section": "",
    "text": "Thanks for signing up. This is the first time we’re running AI Poker Camp and it’s very much in beta. The idea to run this course has existed for a few years, but this curriculum is very new and came about from a discussion at Manifest.\nWe’re glad to have you along for the ride and welcome any feedback about all aspects of the course.\nWe’re running this under the flipped classroom model, where we spend the majority of the time in the sessions focused on thinking about and getting started on the challenge problems, and time outside the sessions doing readings and further work on the challenge problems.\nReadings like this one highlight concepts that are relevant to the accompanying challenge.\nOur goal is to develop your intuitions around decision making and problem solving by engaging with challenges in fun and non-idealized scenarios.",
    "crumbs": [
      "Signup: AI Poker Camp Summer 2024",
      "Challenges",
      "1: Kuhn Poker",
      "Reading"
    ]
  },
  {
    "objectID": "aipcs24/1kuhn_reading.html#welcome-to-ai-poker-camp-summer-2024",
    "href": "aipcs24/1kuhn_reading.html#welcome-to-ai-poker-camp-summer-2024",
    "title": "Kuhn Poker (1): Reading",
    "section": "",
    "text": "Thanks for signing up. This is the first time we’re running AI Poker Camp and it’s very much in beta. The idea to run this course has existed for a few years, but this curriculum is very new and came about from a discussion at Manifest.\nWe’re glad to have you along for the ride and welcome any feedback about all aspects of the course.\nWe’re running this under the flipped classroom model, where we spend the majority of the time in the sessions focused on thinking about and getting started on the challenge problems, and time outside the sessions doing readings and further work on the challenge problems.\nReadings like this one highlight concepts that are relevant to the accompanying challenge.\nOur goal is to develop your intuitions around decision making and problem solving by engaging with challenges in fun and non-idealized scenarios.",
    "crumbs": [
      "Signup: AI Poker Camp Summer 2024",
      "Challenges",
      "1: Kuhn Poker",
      "Reading"
    ]
  },
  {
    "objectID": "aipcs24/1kuhn_reading.html#hunting-wabbits-game",
    "href": "aipcs24/1kuhn_reading.html#hunting-wabbits-game",
    "title": "Kuhn Poker (1): Reading",
    "section": "Hunting Wabbits Game",
    "text": "Hunting Wabbits Game\n(This game is modified from Lisy et al. 2015.)\n\n\n\nBugs and Fudd\n\n\nWe developed this game to show concepts in games with imperfect information that are applicable to our first challenge. Imperfect information games involve hidden information, which are things like opponent cards in poker or enemy location in the Hunting Wabbits game. In perfect information games like chess, all information is available to both players.\nThis game has one chance player (the Author), one maximizing player (Fudd) and one minimizing player (Bugs). Fudd and Bugs are in a long-term, all-out war, and so any energy that Fudd wastes or saves is a gain or loss for Bugs; our game is zero-sum.\nFirst, the Author will choose whether to write an episode where Bugs is at the Opera (50%) or in the Forest (50%). Fudd cannot tell what the Author has chosen.\nNext, Fudd will decide whether to Hunt_Near or Hunt_Far. If he chooses Hunt_Far, he takes -1 value for the extra effort.\nBugs knows whether the Author wrote him into the Opera or Forest, but he does not know Fudd’s hunting location. If the Author gives him the Opera, Bugs has no choices to make and the game ends after Fudd’s action. If Forest, Bugs will decide whether to Play_Near or Play_Far. If he plays in the same location as Fudd is hunting, Fudd gets +3 value for a successful hunt (for a total of +2 in the Hunt_Far action due to the -1 value for the extra effort). If they are in different locations (or if Bugs is at the Opera), Fudd will get 0 value for an unsuccessful hunt (-1 for the Hunt_Far misses).\nPutting it all together, the payoff structure of this game is:\n\n\n\nAuthor\nBugs/Fudd\nHunt_Near\nHunt_Far\n\n\n\n\nOpera\n\n0, 0\n+1, -1\n\n\nForest\nPlay_Near\n-3, +3\n+1, -1\n\n\nForest\nPlay_Far\n0, 0\n-2, +2\n\n\n\nThe tree structure is as follows with the payoffs written from the perspective of Fudd:\n\nIf Fudd knows he’s at the Opera, then he must prefer to Hunt_Near to get a value of \\(0\\) instead of \\(-1\\) for Hunt_Far, but since he doesn’t know his location, he must take both scenarios into account.\nNote that Bugs’s optimal actions depend on Fudd’s Opera strategy even though that outcome cannot be reached once Bugs is playing since Bugs only plays in the Forest! For example if we kept the same game tree except Fudd had a \\(+100\\) Opera Hunt_Far payoff, then he would always Hunt_Far. Bugs would see this and it would affect how Bugs plays in the Forest scenario.\n\n\n\n\n\n\nExercise\n\n\n\nHow would Bugs play in the Forest scenario knowing that Fudd is always playing Hunt_Far?\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nBugs would always Play_Near because then his payoff in the only scenario he can control would be \\(+1\\), whereas Play_Far would get him a payout of \\(-2\\). (The payoffs are all written from the perspective of Fudd, so Bugs’s are opposite.)",
    "crumbs": [
      "Signup: AI Poker Camp Summer 2024",
      "Challenges",
      "1: Kuhn Poker",
      "Reading"
    ]
  },
  {
    "objectID": "aipcs24/1kuhn_reading.html#concept-information-set",
    "href": "aipcs24/1kuhn_reading.html#concept-information-set",
    "title": "Kuhn Poker (1): Reading",
    "section": "Concept: Information Set",
    "text": "Concept: Information Set\nIn a perfect information game, we can draw a tree of the possible states the game can be in. Every time a player is called to take an action, they will know what node they are at, and what node they will go to with each legal action.\nIn an imperfect information game, we can still draw that tree, but now a player might be called to take an action without knowing what node they are actually at. Instead, there will be a set of one or more nodes that are indistinguishable to that player based on what they have seen so far, and they will have to take an action knowing only that they are in that set. Such a set of nodes is an information set or an infoset.\nThe infosets contain information about the player and what actions have been seen so far. For example, [Fudd] is an infoset that contains the nodes [Fudd, Forest] and [Fudd, Opera]. (Just [Fudd] because no other actions/information have been revealed to Fudd at this point.)\nA player strategy is a rule that says, for every information set that player will face, what action or (random choice of actions) that player will take. For a game like Hunting Wabbits or Kuhn Poker (the Challenge 1 game), we can list every information set and its probabilities. For a more complicated game, we might write our strategy as a computation that will output probabilities based on inputs and an algorithm.",
    "crumbs": [
      "Signup: AI Poker Camp Summer 2024",
      "Challenges",
      "1: Kuhn Poker",
      "Reading"
    ]
  },
  {
    "objectID": "aipcs24/1kuhn_reading.html#concept-expected-value",
    "href": "aipcs24/1kuhn_reading.html#concept-expected-value",
    "title": "Kuhn Poker (1): Reading",
    "section": "Concept: Expected Value",
    "text": "Concept: Expected Value\nOnce we’ve assigned definite values to the ultimate outcomes, the expected value, or EV, of a situation is the value of each outcome weighted by the probability of that outcome.\n\n\n\n\n\n\nExercise\n\n\n\nSuppose that Bugs plays uniform \\(0.5\\) Play_Near and \\(0.5\\) Play_Far.\n\nWhat is the value of each Bugs node and what should Fudd do if he knew Bugs’s actions? (Recall that the payoff values are from the perspective of Fudd and Bugs uses opposite values.)\nWhat is Fudd’s expected value in this case?\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nBugs’s node values are:\n\n\\[\n\\begin{align*}\n\\mathbb{E}(\\text{Left Node}) &= 0.5*-3 + 0.5*0 = -1.5 \\\\\n\\mathbb{E}(\\text{Right Node}) &= 0.5*1 + 0.5*-2 = -0.5\n\\end{align*}\n\\]\n\nThe values for Fudd are inverse, so Fudd prefers the Left Node, which has a value of \\(1.5\\). This means that if Fudd is in the Forest, he prefers to Hunt_Near.\n\n\nWe computed that Fudd prefers Hunt_Near in the Forest scenario and can see on the game tree that he also prefers Hunt_Near in the Opera scenario, so can already know that he will always choose Hunt_Near. Fudd will then have the following expected values:\n\n\\[\n\\begin{equation}\n\\begin{split}\n\\mathbb{E}(\\text{Hunt\\_Near}) &= \\Pr(\\text{Opera})*u(\\text{Hunt\\_Near}) + \\Pr(\\text{Forest})*u(\\text{Hunt\\_Near}) \\\\\n  &= 0.5*0 + 0.5*1.5 \\\\\n  &= 0.75\n\\end{split}\n\\end{equation}\n\\]\n\n\n\nIn imperfect information games, we consider probabilities over two different sources of uncertainty, after assuming a particular P1 strategy and P2 strategy:\n\nUncertainty about which node we are actually in, given that we know that we’re in one of multiple nodes that we can’t tell apart. The probabilities of being in node 1, node 2, … of an information set can be calculated by the probabilities of strategies upwards in the game tree (and the probabilites of chance events upwards in the game that have already happened). For example, Fudd doesn’t know if he’s in the Forest or Opera at the beginning.\nUncertainty about what will happen after we go to a node downwards in the game tree, coming from chance events or strategy probabilities in the players’ following actions. For example, after Fudd selects Hunt_Near, there is uncertainty about the outcome since it depends on Bugs’s actions.\n\n\n\nWe will focus on zero-sum two-player games, so the value to one player is simply the negative of the value to the other. Therefore, we can represent value in the game as a single number that the maximizing player wishes to make positive and the minimizing player wishes to make negative.\nWe will focus on maximizing (or minimizing) expected value as our goal for all of session 1. One thing that makes it natural to care about expected value is that it’s usually the best way to predict what your score will be after a very large number of games, whether they are the same game or different from each other.",
    "crumbs": [
      "Signup: AI Poker Camp Summer 2024",
      "Challenges",
      "1: Kuhn Poker",
      "Reading"
    ]
  },
  {
    "objectID": "aipcs24/1kuhn_reading.html#concept-regret",
    "href": "aipcs24/1kuhn_reading.html#concept-regret",
    "title": "Kuhn Poker (1): Reading",
    "section": "Concept: Regret",
    "text": "Concept: Regret\nFor a given P1 strategy and P2 strategy, a player has regret when they take an action at an infoset that was not the highest-EV action at that infoset. The amount of regret is the difference between the highest-EV action and the selected action.\n\n\n\n\n\n\nExercise\n\n\n\n\nWhat is the regret for each action?\n\n\n\nAction\nRegret\n\n\n\n\nA\n\n\n\nB\n\n\n\nC\n\n\n\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\n\nAction\nRegret\n\n\n\n\nA\n4\n\n\nB\n2\n\n\nC\n0\n\n\n\n\n\n\nThere are other things that “regret” can mean in English, that are separate from this technical concept:\n\nBased on what chance events later happened, I wish I had taken a different action instead.\nI was wrong about what strategy my opponent was playing, and I wish I had taken a different action instead.\n\nHowever, we will use “regret” as a technical concept to mean how much worse actions that are not-highest-EV perform compared to highest-EV actions given a particular P1 strategy and P2 strategy.",
    "crumbs": [
      "Signup: AI Poker Camp Summer 2024",
      "Challenges",
      "1: Kuhn Poker",
      "Reading"
    ]
  },
  {
    "objectID": "aipcs24/1kuhn_reading.html#exercises",
    "href": "aipcs24/1kuhn_reading.html#exercises",
    "title": "Kuhn Poker (1): Reading",
    "section": "Exercises",
    "text": "Exercises\n\n\n\n\n\n\nInformation Set\n\n\n\n\nWhat are Fudd’s information set(s)?\nWhat are Bugs’s information set(s)?\n\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nBoth Fudd nodes are a single information set because when Fudd is making the Hunt_Near or Hunt_Far decision, he doesn’t know whether he’s in the Opera or the Forest, so his information is the same in both nodes. We can label these simply [Fudd].\nBoth Bugs nodes are also a single information set because although Bugs knows that he’s in the Forest, he doesn’t know which action Fudd has taken, so his information is the same in both nodes. We can label these [Bugs, Forest].\n\n\n\n\n\n\n\n\n\n\nExpected Value\n\n\n\nSay that Fudd chooses to Hunt_Near with probability \\(p\\).\n\n\nAt the Bugs infoset where Bugs knows he’s in the Forest, what is the expected value of choosing to Play_Near?\nWhat is the expected value of choosing to Play_Far?\n\n(Reminder: The payoffs are from the perspective of Fudd and are the opposite for Bugs.)\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\\(\\mathbb{E}(\\text{Play\\_Near}) = -3*p + 1*(1-p) = -4*p + 1\\)\n\\(\\mathbb{E}(\\text{Play\\_Far}) = 0*p + -2*(1-p) = 2*p - 2\\)\n\n\n\n\n\n\n\n\n\n\nExpected Value 2\n\n\n\nSay that Bugs chooses to Play_Near with probability \\(q\\).\n\n\nWhat is Fudd’s expected value of choosing Hunt_Near at his infoset?\nWhat is Fudd’s EV of choosing Hunt_Far?\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\\(\\mathbb{E}(\\text{Hunt\\_Near}) = 0.5*0 +0.5*[3*q + 0*(1-q)] = 1.5*q\\)\n\\(\\mathbb{E}(\\text{Hunt\\_Far}) = 0.5*(-1) + 0.5*[-1*q + 2*(1-q)] = -0.5 - 0.5*q + 1 - q = 0.5 - 1.5*q\\)\n\n\n\n\n\n\n\n\n\n\nRegret\n\n\n\nFind a \\(p\\) and a \\(q\\) such that both:\n\nBugs never chooses an action with regret\nFudd never chooses an action with regret\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nIn order to have no regret, the expected value of both actions should be equal\n\n\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nFor Bugs to never choose an action with regret, EV of Play_Near and EV of Play_Far should be equal.\n\n\\(\\mathbb{E}(\\text{Bugs Play\\_Near}) = -4*p + 1\\)\n\\(\\mathbb{E}(\\text{Bugs Play\\_Far}) = 2*p - 2\\)\nSetting equal, we have \\(-4*p + 1 = 2*p - 2 \\Rightarrow 3 = 6*p \\Rightarrow p = \\frac{1}{2} = 0.5\\)\n\\(\\Pr(\\text{Fudd Hunt\\_Near}) = p = 0.5\\)\n\\(\\Pr(\\text{Fudd Hunt\\_Far}) = 1 - p = 0.5\\)\n\nFor Fudd to never choose an action with regret, EV of Hunt_Near and EV of Hunt_Far should be equal.\n\n\\(\\mathbb{E}(\\text{Fudd Hunt\\_Near}) = 1.5*q\\)\n\\(\\mathbb{E}(\\text{Fudd Hunt\\_Far}) = 0.5 - 1.5*q\\)\nSetting equal, we have \\(1.5*q = 0.5 - 1.5*q \\Rightarrow 3*q = 0.5 \\Rightarrow q = \\frac{1}{6} = 0.167\\)\n\\(\\Pr(\\text{Bugs Play\\_Near}) = q = 0.167\\)\n\\(\\Pr(\\text{Bugs Play\\_Far}) = 1 - q = 0.833\\)\nNotice that each player is inducing the other player to have no regret (to be indifferent to both actions) by playing actions at these probabilities, which then result in an equilibrium as shown below:\n\nWe will go deeper into indifference in the next reading.\n\n\n\n\n\n\n\n\n\nGame Value\n\n\n\nWhat is the value of the game (i.e. the expected value as Fudd over the entire game) at the equilibrium strategies found in the previous question?\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nSince the game is zero-sum, we can find Fudd’s expected value and it is equivalent to the game value:\n\\[\\begin{align}\n\\mathbb{E} &= \\Pr(\\text{Opera}) * \\Pr(\\text{Hunt\\_Near}) * 0 \\\\\n              & + \\Pr(\\text{Opera}) * \\Pr(\\text{Hunt\\_Far}) * -1 \\\\\n              & + \\Pr(\\text{Forest}) * \\Pr(\\text{Hunt\\_Near}) * \\Pr(\\text{Play\\_Near}) * 3 \\\\\n              & + \\Pr(\\text{Forest}) * \\Pr(\\text{Hunt\\_Near}) * \\Pr(\\text{Play\\_Far}) * 0 \\\\\n              & + \\Pr(\\text{Forest}) * \\Pr(\\text{Hunt\\_Far}) * \\Pr(\\text{Play\\_Near}) * -1 \\\\\n              & + \\Pr(\\text{Forest}) * \\Pr(\\text{Hunt\\_Far}) * \\Pr(\\text{Play\\_Far}) * 2 \\\\ \\\\\n    &= 0.5 * 0.5 * 0 \\\\\n    &+ 0.5 * 0.5 * -1 \\\\\n    &+ 0.5 * 0.5 * 0.17 * 3 \\\\\n    &+ 0.5 * 0.5 * 0.83 * 0 \\\\\n    &+ 0.5 * 0.5 * 0.17 * -1 \\\\\n    &+ 0.5 * 0.5 * 0.83 * 2 \\\\ \\\\\n    &= 0 \\\\\n    &+ -0.25 \\\\\n    &+ 0.125 \\\\\n    &+ 0 \\\\\n    &+ -0.042 \\\\\n    &+ 0.42 \\\\ \\\\\n    &= 0.25 \\\\\n\\end{align}\\]\nThis means that if they play repeatedly, we expect Fudd to average a payoff of \\(+0.25\\), while Bugs has a payoff of \\(-0.25\\).",
    "crumbs": [
      "Signup: AI Poker Camp Summer 2024",
      "Challenges",
      "1: Kuhn Poker",
      "Reading"
    ]
  },
  {
    "objectID": "aipcs24/1kuhn_challenge.html",
    "href": "aipcs24/1kuhn_challenge.html",
    "title": "Kuhn Poker (1): Challenge",
    "section": "",
    "text": "Kuhn Poker is the most simple version of poker. Here is the setup:\n\n3 card deck: Queen, King, Ace (in ascending order, so Ace is highest)\n\n\n\n\n\n\n\nAlternative Decks\n\n\n\n\n\n\nAny deck of 3 ranked cards works\nThis could be numbers like {0, 1, 2} or other cards like {Jack, Queen, King}\n\n\n\n\n\n2 players\nEach player starts with 2 chips and antes 1 chip\nDeal 1 card to each player (discard the third)\n\n\n\nThere is one betting round in which players can take these actions:\n\nUp ↑ (putting a chip into the pot)\nDown ↓ (not putting a chip into the pot)\n\n\n\n\n\n\n\n\nPoker Terms for Actions\n\n\n\n\n\n\nUp ↑ actions indicate a Bet or Call\nDown ↓ actions indicate a Check or Fold\n\n\n\n\nExample betting round:\n\n Player 1: Up ↑ (1 chip into pot)\n Player 2: Up ↑ (1 chip into pot)\nHigh Card wins +2 (+1 from the ante, +1 from the ↑ bet)\nIf  P1 has card A and P2 has card K,  P1 wins\n\nHands can end in two ways:\n\nA showdown resulting from two ↑ (Bet and Call) or two ↓ actions (Check and Check). Then the player with the highest card wins the pot.\nA fold resulting from ↑ and ↓ actions (Bet and Fold). Then the ↑ (betting) player automatically wins the pot.\n\nHere is a list of all possible betting sequences:\n\n\n\n\n\n\n\n\n\n Player 1\n Player 2\n Player 1\nWinner\n\n\n\n\nUp ↑\nDown ↓\n\n Player 1 (+1)\n\n\nUp ↑\nUp ↑\n\nHigh Card Player (+2)\n\n\nDown ↓\nDown ↓\n\nHigh Card Player (+1)\n\n\nDown ↓\nUp ↑\nDown ↓\n Player 2 (+1)\n\n\nDown ↓\nUp ↑\nUp ↑\nHigh Card Player (+2)\n\n\n\n\n\n\nPlay a few hands\nGet used to the game setup\nThink about which situations are automatic and which are not",
    "crumbs": [
      "Signup: AI Poker Camp Summer 2024",
      "Challenges",
      "1: Kuhn Poker",
      "Challenge"
    ]
  },
  {
    "objectID": "aipcs24/1kuhn_challenge.html#kuhn-poker",
    "href": "aipcs24/1kuhn_challenge.html#kuhn-poker",
    "title": "Kuhn Poker (1): Challenge",
    "section": "",
    "text": "Kuhn Poker is the most simple version of poker. Here is the setup:\n\n3 card deck: Queen, King, Ace (in ascending order, so Ace is highest)\n\n\n\n\n\n\n\nAlternative Decks\n\n\n\n\n\n\nAny deck of 3 ranked cards works\nThis could be numbers like {0, 1, 2} or other cards like {Jack, Queen, King}\n\n\n\n\n\n2 players\nEach player starts with 2 chips and antes 1 chip\nDeal 1 card to each player (discard the third)\n\n\n\nThere is one betting round in which players can take these actions:\n\nUp ↑ (putting a chip into the pot)\nDown ↓ (not putting a chip into the pot)\n\n\n\n\n\n\n\n\nPoker Terms for Actions\n\n\n\n\n\n\nUp ↑ actions indicate a Bet or Call\nDown ↓ actions indicate a Check or Fold\n\n\n\n\nExample betting round:\n\n Player 1: Up ↑ (1 chip into pot)\n Player 2: Up ↑ (1 chip into pot)\nHigh Card wins +2 (+1 from the ante, +1 from the ↑ bet)\nIf  P1 has card A and P2 has card K,  P1 wins\n\nHands can end in two ways:\n\nA showdown resulting from two ↑ (Bet and Call) or two ↓ actions (Check and Check). Then the player with the highest card wins the pot.\nA fold resulting from ↑ and ↓ actions (Bet and Fold). Then the ↑ (betting) player automatically wins the pot.\n\nHere is a list of all possible betting sequences:\n\n\n\n\n\n\n\n\n\n Player 1\n Player 2\n Player 1\nWinner\n\n\n\n\nUp ↑\nDown ↓\n\n Player 1 (+1)\n\n\nUp ↑\nUp ↑\n\nHigh Card Player (+2)\n\n\nDown ↓\nDown ↓\n\nHigh Card Player (+1)\n\n\nDown ↓\nUp ↑\nDown ↓\n Player 2 (+1)\n\n\nDown ↓\nUp ↑\nUp ↑\nHigh Card Player (+2)\n\n\n\n\n\n\nPlay a few hands\nGet used to the game setup\nThink about which situations are automatic and which are not",
    "crumbs": [
      "Signup: AI Poker Camp Summer 2024",
      "Challenges",
      "1: Kuhn Poker",
      "Challenge"
    ]
  },
  {
    "objectID": "aipcs24/1kuhn_challenge.html#kuhn-game-tree",
    "href": "aipcs24/1kuhn_challenge.html#kuhn-game-tree",
    "title": "Kuhn Poker (1): Challenge",
    "section": "Kuhn Game Tree",
    "text": "Kuhn Game Tree\nWe’ll represent Kuhn poker in terms of a visual game tree. This will become infeasible for more complicated games, but here it will let us visualize what is going on.\nGoing forward, we will refer to Player 1 as  and Player 2 as .\nFirst, deal a card to  and to . Each possible deal of the cards forms a separate game node. For example, AK means that  has A and  has K:\n\n\n\n will act first. He knows what his card is, but not what card  has, so he can be in one of 3 infosets. We’ll name them A_, K_, and Q_ based on what card he holds:\n\n\n\nAt each infoset,  can choose ↑Up or ↓Down. Note that for example ↑Up from a certain infoset will take the same action probability for both nodes, so A_↑ will take the same probability in AK↑ and AQ↑ since they are indistinguishable to .\n\n\n\nEach ↑Up or ↓Down action will each take us to a distinct game node, based on a unique set of cards and action history:\n\n\n\nNext,  will act. He can observe his card and ’s first action, but not ’s card. ’s 6 infosets are identified by IDs like _K↓ and _Q↑ based on his card and previous actions. In this case the previous actions are ’s first action.\n\n\n\nAt each of these infosets,  can choose ↑Up or ↓Down:\n\n\n\nIf the actions were ↓↑, then  will have to act again (equivalent to acting after a Check and Bet). Otherwise, the game is now over (with a payoff determined by the cards and the action sequence).\nWe write the payoffs from ’s perspective, and remember that ’s payoffs will be the inverse.\nYou can refer to the sequence table above to recall the 5 sequences of betting, which end either in one player folding (after ↑ and ↓ actions) or high card winning at showdown, which results from either two ↑ (Bet and Call) or ↓ (Check and Check) actions.\n\n\n\nIf  still has to act, he’ll be in one of just three infosets, A_↓↑, K_↓↑, or Q_↓↑:\n\n\n\n…and can choose ↑Up or ↓Down…\n\n\n\n…but whatever he chooses, the game will end after that move. We now have the entire game tree for Kuhn Poker:",
    "crumbs": [
      "Signup: AI Poker Camp Summer 2024",
      "Challenges",
      "1: Kuhn Poker",
      "Challenge"
    ]
  },
  {
    "objectID": "aipcs24/1kuhn_challenge.html#kuhn-strategies",
    "href": "aipcs24/1kuhn_challenge.html#kuhn-strategies",
    "title": "Kuhn Poker (1): Challenge",
    "section": "Kuhn Strategies",
    "text": "Kuhn Strategies\nEach player’s strategy can be completely described in terms of their action probabilities at each infoset—6 of them per player.\nFor example, let’s consider ’s decision at the infoset K_↓↑. When  is at K_↓↑, he doesn’t know whether the true state of the world is KA_↓↑ or KQ_↓↑. This is what that setting looks like in the game from ’s perspective:\n\nWhatever action (or randomized mix of actions) his strategy says to make, he will be doing so in all of situations where he arrives at K_↓↑, without the ability to do different things for the different underlying states.\nIf both players play randomly (50% ↑Up on each action), then  will arrive at K_↓↑ via KA↓↑ equally often as via KQ↓↑.\n\n\n\nIn this case, his expected payoff for playing ↑Up is a 50%-50% weighted sum of the payoffs KA↓↑↑ (-2, calling with K when opponent has A) and KQ↓↑↑ (+2, calling with K when opponent has Q), for an expected value of 0.\nSimilarly, his expected payoff for ↓Down is a 50%-50% weighted sum of the payoffs for KA↓↑↓ and KQ↓↑↓, though in this case they’re both -1 and the EV doesn’t depend on the composition weights of the infoset (since this is equivalent to always folding).\nWith these expected values, ’s “best response” at this infoset (holding everything else about both strategies constant) is to play ↑Up 100% of the time.\n\n\n\n\n\n\nExercise\n\n\n\nHow much should ’s expected value from the whole game (a random deal) increase if he moves from playing ↑Up (i.e. always calling the Bet) at K_↓↑ 50% of the time to 100% instead? Once you’ve figured out what you expect the answer to be, set the strategy probability to 100% in the text box above and check your answer.\n\n\nBut what if the probabilities of reaching K_↓↑ via KA↓↑ versus KQ↓↑ aren’t equal? If the players don’t play 50-50 randomly, then we’ll have to calculate the probability of reaching KA↓↑ (including the probability of the initial KA deal) and the probability of reaching KQ↓↑ (likewise), and let the composition of the infoset be proportional to the the probabilities of reaching these states. We can see this in our next example. (In larger games, we might approximate these reach-state probabilities by sampling games with simulated play instead of calculating them analytically.)\nConsider ’s actions at _Q↓ (the poker situation of having a Q facing a Check).  will always get a payoff of -1.0 (+1.0) if he plays ↓Down (Folding).\nWhen  plays randomly at K_↓↑ (the poker situation of having a K and Checking and then facing a Bet), then  will get a payoff of -0.5 (+0.5) if he played ↑Up (always Betting) since half the time  will play ↓Down (Fold, +1) and half the time he will ↑Up (Call, -2).\nNote that ’s Q is always has the worst hand and an ↑Up action is a bluff, but if  plays ↓Down (Folds) often enough, then  will win +1s often enough to come out ahead regardless.\nBut if  always plays ↑Up at K_↓↑, then ’s payoff for playing ↑Up at _Q↓ becomes -1.25 (+1.25), worse than playing ↓Down, and so he should play ↓Down instead of ↑Up.\n\n\n\n\n\n\n\n\n\n\n\n\nExercise\n\n\n\nSet the K_↓↑ strategy prob to ’s local best response. Set the _Q↓ strategy prob to ’s local best response. Observe that ’s best-response has changed again. Try to set the strategies to a pair of values such that neither player has regret.\n\n\nNow we get to the core difference between solving perfect information games and imperfect information games. In our game of Kuhn Poker, when  switches to only playing ↓Down at _Q↓, ’s payoff EVs at K_↓↑ change—because now that infoset is only composed of KA↓↑ and no KQ↓↑.\nWith the resulting EVs,  has a payoff of -2 for playing ↑Up, and should play ↓Down instead.\nIn a perfect information game, we can solve the game inductively by passing up the tree from the end-states and solving each situation in terms of known solutions to its successor nodes. Whatever the best thing to do at the downtree node was, it’s still the best thing to do, no matter how you get there.\nBut in an imperfect information game, changing an uptree strategy parameter can change the “right solution” is at downtree infosets, so we can’t solve a game in a single pass from the endgames to the beginning. Nearly every game-solving technique for imperfect information games, then, is based on iteratively improving on a pair of strategies (one for each player) based on local improvements until it hopefully resolves into something good.\nRecall the two types of uncertainty:\n\nAbout which node we are actually in, which depends on probabilities upwards in the game tree\nAbout what happens after an action, which depends on the probabilities downwards in the game tree",
    "crumbs": [
      "Signup: AI Poker Camp Summer 2024",
      "Challenges",
      "1: Kuhn Poker",
      "Challenge"
    ]
  },
  {
    "objectID": "aipcs24/1kuhn_challenge.html#challenge-1",
    "href": "aipcs24/1kuhn_challenge.html#challenge-1",
    "title": "Kuhn Poker (1): Challenge",
    "section": "Challenge 1",
    "text": "Challenge 1\nThe goal of this challenge is to find optimal strategies for Kuhn Poker by adjusting the individual strategies at each of the 12 infosets (6 for each player). Optimal here means Nash equilibrium regret-minimizing strategies for both players.\nBy adjusting strategies for both players, we can find an equilibrium and then\n\nPart 1: Manual Strategies\nTry to find a pair of no-regret full strategies in the boxes below for  and .\nHint: There are some “automatic” infoset strategies such that it only makes sense to play 100% of one action in the game.\nWhen you get each infoset to at most 0.1 chips of EV away from regret-free, a link to the next stage will appear below.\n\n\nPart 2: Automatic Strategies\nNow you can update the strategies by selecting different algorithmic methods, iterations, and speeds.\nEnter Part 2\n\n\nCheat\n\n\n\nHere is the link to the next stage.\n\n\n\n\n\n\n\n  &lt;button id=\"start_simulator\" style=\"padding: 0.5em 1em;\" onclick=\"startSimulator()\"&gt;Run Solver&lt;/button&gt;\n\n\n  &lt;label for=\"update_mode\" style=\"margin-right: 0.5em;\"&gt;Update mode:&lt;/label&gt;\n  &lt;select id=\"update_mode\" style=\"padding: 0.2em;\"&gt;\n      &lt;option value=\"1.0\" selected=\"selected\"&gt;100% CFR&lt;/option&gt;\n      &lt;option value=\"0.1\"&gt;10% CFR&lt;/option&gt;\n      &lt;option value=\"0.01\"&gt;1% CFR&lt;/option&gt;\n      &lt;option value=\"0.001\"&gt;0.1% CFR&lt;/option&gt;\n      &lt;option value=\"0.0001\"&gt;0.0001% CFR&lt;/option&gt;\n      &lt;option value=\"0.00001\"&gt;0.00001% CFR&lt;/option&gt;\n  &lt;/select&gt;\n\n\n&lt;label for=\"num_iterations\" style=\"margin-right: 0.5em;\"&gt;Iterations:&lt;/label&gt;\n&lt;select id=\"num_iterations\" style=\"padding: 0.2em;\"&gt;\n    &lt;option value=\"1\" selected=\"selected\"&gt;1&lt;/option&gt;\n    &lt;option value=\"10\"&gt;10&lt;/option&gt;\n    &lt;option value=\"100\"&gt;100&lt;/option&gt;\n    &lt;option value=\"1000\"&gt;1,000&lt;/option&gt;\n    &lt;option value=\"10000\"&gt;10,000&lt;/option&gt;\n    &lt;option value=\"100000\"&gt;100,000&lt;/option&gt;\n&lt;/select&gt;\n\n\n&lt;label for=\"wait_per_step\" style=\"margin-right: 0.5em;\"&gt;Speed:&lt;/label&gt;\n&lt;select id=\"wait_per_step\" style=\"padding: 0.2em;\"&gt;\n    &lt;option value=\"0\" selected=\"selected\"&gt;Max&lt;/option&gt;\n    &lt;option value=\"10\"&gt;Fast&lt;/option&gt;\n    &lt;option value=\"100\"&gt;10/sec&lt;/option&gt;\n    &lt;option value=\"1000\"&gt;1/sec&lt;/option&gt;\n&lt;/select&gt;\n\n\n&lt;label for=\"tolerance\" style=\"margin-right: 0.5em;\"&gt;Tolerance:&lt;/label&gt;\n&lt;select id=\"tolerance\" style=\"padding: 0.2em;\"&gt;\n    &lt;option value=\"0.01\"&gt;0.01&lt;/option&gt;\n    &lt;option value=\"0.03\"&gt;0.03&lt;/option&gt;\n    &lt;option value=\"0.1\" selected=\"selected\"&gt;0.1&lt;/option&gt;\n    &lt;option value=\"0.3\"&gt;0.3&lt;/option&gt;\n&lt;/select&gt;\n\n\n\n\n\nTodo (solver stage):\n\n\nsplit off auto-solver page\n\n\nadd update modes to auto-solver\n\n\nadd “unlock” to auto-solver\n\n\nadd leaderboard\n\n\nadd “submit” button\n\n\nreceive submissions on backend\n\n\nauto-run tournaments (on submission?)\n\n\nset up instructor bot submissions\n\n\n\nadd pause button to solver\n\n\n\n\n\n\nDo we really need mixed strategies?\nYou can test this for yourself. If you set the learning rate to 1.0, updating a cell will move it all the way to playing the locally better action 100% of the time.\nFollow these steps:\n\nReset\nSet the learning rate to 1.0.\nIn any order you like, pick cells and update them.\nRepeat 3 while there are still suboptimal moves in your strategy.\n\nA bad thing is happening to you. Why? ## Challenge 1 Submissions Parts 1 and 2 will each generate and submit a bot that plays with your chosen probabilities. (It has no memory of other rounds, and plays each round based on the probabilities.)\nYour submission will play 100,000 times against all other submissions (including 10 bots that we added to start the challenge). It will play as P1 half the time and P2 half the time. Each 1v1 will be played in duplicate so that the card dealings are reversed to reduce variance.\nAfter a short delay, the leaderboard should update; your score will be compared to other players and selected bots. You will be listed as “tied” with another entry if your total score against all opponents is within one standard error of theirs.",
    "crumbs": [
      "Signup: AI Poker Camp Summer 2024",
      "Challenges",
      "1: Kuhn Poker",
      "Challenge"
    ]
  },
  {
    "objectID": "aipcs24/index.html",
    "href": "aipcs24/index.html",
    "title": "AI Poker Camp Summer 2024",
    "section": "",
    "text": "Welcome to AI Poker Camp Summer 2024 Beta in San Francisco!"
  },
  {
    "objectID": "aipcs24/sessions.html",
    "href": "aipcs24/sessions.html",
    "title": "Sessions and Challenges",
    "section": "",
    "text": "This is the schedule for AI Poker Camp Summer 2024. We are using the “flipped classroom” concept, where readings are meant to be done in advance, mostly outside of the sessions, and the sessions are mostly spent on games, challenges, and questions that came up from the readings.\n\n\nThe calendar below includes all dates, locations, and times. You can add the whole thing to your own calendar with the button in the bottom right.",
    "crumbs": [
      "Signup: AI Poker Camp Summer 2024",
      "About the Course",
      "Sessions and Challenges"
    ]
  },
  {
    "objectID": "signup/index.html",
    "href": "signup/index.html",
    "title": "AI Poker Camp (2024 Summer Beta SF)",
    "section": "",
    "text": "Sign up now!\n\n\n\nSignup Form\n\nAfter signing up, we’ll be in touch with all details\nThe beta program is free!"
  },
  {
    "objectID": "signup/index.html#when",
    "href": "signup/index.html#when",
    "title": "AI Poker Camp (2024 Summer Beta SF)",
    "section": "When?",
    "text": "When?\n6pm-8pm Mondays and Thursdays, July 15 through August 15."
  },
  {
    "objectID": "signup/index.html#what",
    "href": "signup/index.html#what",
    "title": "AI Poker Camp (2024 Summer Beta SF)",
    "section": "What?",
    "text": "What?\nA five-week, twice-weekly course on applied game theory through you writing AIs to play games. By the end, you should be able to write an AI to play poker.\nThis is a beta test of a course we’re planning to run online in the fall, so it’ll be small. We’ll cap signups somewhere between 16 and 24 students."
  },
  {
    "objectID": "signup/index.html#where",
    "href": "signup/index.html#where",
    "title": "AI Poker Camp (2024 Summer Beta SF)",
    "section": "Where?",
    "text": "Where?\nStrictly in-person in San Francisco. Location to be announced.\n\nWait, really? I can’t make that!\nWe’re also planning to run an online version of the course starting in late September. You can join our mailing list to learn more when it’s announced."
  },
  {
    "objectID": "signup/index.html#who",
    "href": "signup/index.html#who",
    "title": "AI Poker Camp (2024 Summer Beta SF)",
    "section": "Who?",
    "text": "Who?\n\nWe recommend students be able to program in Python and perform a Bayesian update (though it’s fine to lean on an LLM for help on either).\nKnowledge of the game of poker is not necessary. (We are offering a 2 hour Poker Basics workshop on Sun Jul 7.)"
  },
  {
    "objectID": "signup/index.html#whats-the-curriculum",
    "href": "signup/index.html#whats-the-curriculum",
    "title": "AI Poker Camp (2024 Summer Beta SF)",
    "section": "What’s the curriculum?",
    "text": "What’s the curriculum?\nThe course is built around six or seven practical challenges – think “kaggle competition for game-playing programs”. These will cover:\n\nTutorial: One-card / Kuhn Poker\n\nTopic: Algorithms for solving incomplete-information games.\n\nLarger one-card poker formats and other simple games\n\nTopic: Scaling up algorithms to larger game trees.\n\nRock-Paper-Scissors against imperfect opponents\n\nTopic: Techniques for modeling empirical opponent behavior.\n\nHidden-information version of Probabilistic Tic-Tac-Toe\n\nTopic: Modeling hidden information from opponent actions.\n\nTexas Holdem with simplified betting\n\nTopic: Putting it together!\n\n\nWe’re intending for the Summer Beta to have about the intensity of one (1) college course in applied CS. You should expect to make at least 9 out of 10 class sessions."
  },
  {
    "objectID": "signup/index.html#whos-teaching",
    "href": "signup/index.html#whos-teaching",
    "title": "AI Poker Camp (2024 Summer Beta SF)",
    "section": "Who’s teaching?",
    "text": "Who’s teaching?\nRoss Rheingans-Yoo wrote the advanced trading simulations for the Jane Street trading internship program from 2018 to 2021.\nMax Chiswick is a former poker pro who has played more than 10 million hands of online poker, and created AI Poker Tutorial.\nRicki Heicklen is a curriculum advisor (but will not be teaching in the SF Beta)."
  },
  {
    "objectID": "signup/index.html#how",
    "href": "signup/index.html#how",
    "title": "AI Poker Camp (2024 Summer Beta SF)",
    "section": "How?",
    "text": "How?\nSignup Form\n\nAfter signing up, we’ll be in touch with all details\nThe beta program is free!"
  },
  {
    "objectID": "signup/index.html#something-else",
    "href": "signup/index.html#something-else",
    "title": "AI Poker Camp (2024 Summer Beta SF)",
    "section": "Something else?",
    "text": "Something else?\nGet in touch!"
  },
  {
    "objectID": "aipcs24/1kuhn_challenge2.html",
    "href": "aipcs24/1kuhn_challenge2.html",
    "title": "Kuhn Poker (1): Challenge",
    "section": "",
    "text": "Part 2: Automatic Strategies\nBack to Part 1\nNow you can update the strategies by selecting different algorithmic methods, iterations, and speeds.\n\n\nCheat\n\n\n\nHere is the link to the next stage.\n\n\n\n\n\n\n\n  &lt;button id=\"start_simulator\" style=\"padding: 0.5em 1em;\" onclick=\"startSimulator()\"&gt;Run Solver&lt;/button&gt;\n\n\n  &lt;label for=\"update_mode\" style=\"margin-right: 0.5em;\"&gt;Update mode:&lt;/label&gt;\n  &lt;select id=\"update_mode\" style=\"padding: 0.2em;\"&gt;\n      &lt;option value=\"1.0\" selected=\"selected\"&gt;100% CFR&lt;/option&gt;\n      &lt;option value=\"0.1\"&gt;10% CFR&lt;/option&gt;\n      &lt;option value=\"0.01\"&gt;1% CFR&lt;/option&gt;\n      &lt;option value=\"0.001\"&gt;0.1% CFR&lt;/option&gt;\n      &lt;option value=\"0.0001\"&gt;0.0001% CFR&lt;/option&gt;\n      &lt;option value=\"0.00001\"&gt;0.00001% CFR&lt;/option&gt;\n  &lt;/select&gt;\n\n\n&lt;label for=\"num_iterations\" style=\"margin-right: 0.5em;\"&gt;Iterations:&lt;/label&gt;\n&lt;select id=\"num_iterations\" style=\"padding: 0.2em;\"&gt;\n    &lt;option value=\"1\" selected=\"selected\"&gt;1&lt;/option&gt;\n    &lt;option value=\"10\"&gt;10&lt;/option&gt;\n    &lt;option value=\"100\"&gt;100&lt;/option&gt;\n    &lt;option value=\"1000\"&gt;1,000&lt;/option&gt;\n    &lt;option value=\"10000\"&gt;10,000&lt;/option&gt;\n    &lt;option value=\"100000\"&gt;100,000&lt;/option&gt;\n&lt;/select&gt;\n\n\n&lt;label for=\"wait_per_step\" style=\"margin-right: 0.5em;\"&gt;Speed:&lt;/label&gt;\n&lt;select id=\"wait_per_step\" style=\"padding: 0.2em;\"&gt;\n    &lt;option value=\"0\" selected=\"selected\"&gt;Max&lt;/option&gt;\n    &lt;option value=\"10\"&gt;Fast&lt;/option&gt;\n    &lt;option value=\"100\"&gt;10/sec&lt;/option&gt;\n    &lt;option value=\"1000\"&gt;1/sec&lt;/option&gt;\n&lt;/select&gt;\n\n\n&lt;label for=\"tolerance\" style=\"margin-right: 0.5em;\"&gt;Tolerance:&lt;/label&gt;\n&lt;select id=\"tolerance\" style=\"padding: 0.2em;\"&gt;\n    &lt;option value=\"0.01\"&gt;0.01&lt;/option&gt;\n    &lt;option value=\"0.03\"&gt;0.03&lt;/option&gt;\n    &lt;option value=\"0.1\" selected=\"selected\"&gt;0.1&lt;/option&gt;\n    &lt;option value=\"0.3\"&gt;0.3&lt;/option&gt;\n&lt;/select&gt;\n\n\n\n\n\nTodo (solver stage):\n\n\nsplit off auto-solver page\n\n\nadd update modes to auto-solver\n\n\nadd “unlock” to auto-solver\n\n\nadd leaderboard\n\n\nadd “submit” button\n\n\nreceive submissions on backend\n\n\nauto-run tournaments (on submission?)\n\n\nset up instructor bot submissions\n\n\n\nadd pause button to solver\n\n\n\n\n\n\nDo we really need mixed strategies?\nYou can test this for yourself. If you set the learning rate to 1.0, updating a cell will move it all the way to playing the locally better action 100% of the time.\nFollow these steps:\n\nReset\nSet the learning rate to 1.0.\nIn any order you like, pick cells and update them.\nRepeat 3 while there are still suboptimal moves in your strategy.\n\nA bad thing is happening to you. Why? ## Challenge 1 Submissions Parts 1 and 2 will each generate and submit a bot that plays with your chosen probabilities. (It has no memory of other rounds, and plays each round based on the probabilities.)\nYour submission will play 100,000 times against all other submissions (including 10 bots that we added to start the challenge). It will play as P1 half the time and P2 half the time. Each 1v1 will be played in duplicate so that the card dealings are reversed to reduce variance.\nAfter a short delay, the leaderboard should update; your score will be compared to other players and selected bots. You will be listed as “tied” with another entry if your total score against all opponents is within one standard error of theirs."
  },
  {
    "objectID": "aipcs24/1kuhn_challenge_review.html",
    "href": "aipcs24/1kuhn_challenge_review.html",
    "title": "Kuhn Poker (1): Challenge Review",
    "section": "",
    "text": "A Nash equililibrium is a set of strategies for both players such that neither player ever plays an action with regret (when they take those strategies as given).\nRecall that regret only makes sense in the context of a particular strategy and assumed opponent’s strategy. When you submitted to Challenge 1, you submitted a strategy for being P1, and a strategy for being P2, but you won’t ever play against yourself – so why is it helpful to find a pair that plays against itself without regret?\nThe other paradigm for game strategies is opponent exploitation, which we will address in future sections.\n\n\nConsider the Soccer Penalty Kick game where a Kicker is trying to score a goal and the Goalie is trying to block it.\n\n\n\nKicker/Goalie\nLean Left\nLean Right\n\n\n\n\nKick Left\n0, 0\n+2, -2\n\n\nKick Right\n+1, -1\n0, 0\n\n\n\nThe game setup is zero-sum. If Kicker and Goalie both go in one direction, then it’s assumed that the goal will miss and both get \\(0\\) payoffs. If the Kicker plays Kick Right when the Goalie plays Lean Left, then the Kicker is favored and gets a payoff of \\(+1\\). If the Kicker plays Kick Left when the Goalie plays Lean Right, then the kicker is even more favored, because it’s easier to kick left than right, and gets \\(+2\\).\n\n\n\n\n\n\nNash Equilibrium Exercise\n\n\n\nWhich of these, if any, is a Nash Equilibrium? You can check by seeing if either player would benefit by changing their action.\n\n\n\nKicker\nGoalie\nEquilibrium or Change?\n\n\n\n\nLeft\nLeft\n\n\n\nLeft\nRight\n\n\n\nRight\nLeft\n\n\n\nRight\nRight\n\n\n\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThere are no pure Nash equilibrium solutions because when the actions match, the Kicker will always want to change, and when they don’t match, the Goalie will always want to change.\n\n\n\nKicker\nGoalie\nEquilibrium or Change?\n\n\n\n\nLeft\nLeft\nKicker changes to right\n\n\nLeft\nRight\nGoalie changes to left\n\n\nRight\nLeft\nGoalie changes to right\n\n\nRight\nRight\nKicker changes to left\n\n\n\n\n\n\n\n\n\n\n\n\nExpected Value Exercise\n\n\n\nAssume that they both play Left 50% and Right 50% – what is the expected value of the game?\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\n\nKicker/Goalie\nLean Left (0.5)\nLean Right (0.5)\n\n\n\n\nKick Left (0.5)\n0, 0\n+2, -2\n\n\nKick Right (0.5)\n+1, -1\n0, 0\n\n\n\nWe apply these probabilities to each of the 4 outcomes:\n\n\n\nKicker/Goalie\nLean Left (0.5)\nLean Right (0.5)\n\n\n\n\nKick Left (0.5)\n0, 0 (0.25)\n+2, -2 (0.25)\n\n\nKick Right (0.5)\n+1, -1 (0.25)\n0, 0 (0.25)\n\n\n\nNow for the Kicker, we have \\(\\mathbb{E} = 0.25*0 + 0.25*2 + 0.25*1 + 0.25*0 = 0.75\\).\nSince it’s zero-sum, we have \\(\\mathbb{E} = -0.75\\) for the Goalie.\nNote that, for example, the Kicker playing 50% Left and 50% Right could be interpreted as a single player having these probabilities or a field of players averaging to these probabilities. So out of 100 players, this could mean:\n\n100 players playing 50% Left and 50% Right\n50 players playing 100% Left and 50 players playing 100% Right\n50 players playing 75% Left/25% Right and 50 players playing 25% Left/75% right\n\n\n\n\nWhen the Goalie plays left with probability \\(p\\) and right with probability \\(1-p\\), we can find the expected value of the Kicker actions.\n\n\n\nKicker/Goalie\nLean Left (p)\nLean Right (1-p)\n\n\n\n\nKick Left\n0, 0\n+2, -2\n\n\nKick Right\n+1, -1\n0, 0\n\n\n\n\\(\\mathbb{E}(\\text{Kick Left}) = 0*p + 2*(1-p) = 2 - 2*p\\)\n\\(\\mathbb{E}(\\text{Kick Right}) = 1*p + 0*(1-p) = 1*p\\)\nThe Kicker is going to play the best response to the Goalie’s strategy. The Goalie wants to make the Kicker indifferent to Kick Left and Kick Right because if the Kicker was not going to be indifferent, then he would prefer one of the actions, meaning that action would be superior to the other. Therefore the Kicker will play a mixed strategy in response that will result in a Nash equilibrium where neither player benefits from unilaterally changing strategies. (Note that indifferent does not mean 50% each, but means the expected value is the same for each.)\n\nBy setting the values equal, we get \\(2 - 2*p = 1*p \\Rightarrow p = \\frac{2}{3}\\) as shown in the plot. This means that \\(1-p = 1 - \\frac{2}{3} = \\frac{1}{3}\\). Therefore the Goalie should play Lean Left \\(\\frac{2}{3}\\) and Lean Right \\(\\frac{1}{3}\\). The value for the Kicker is \\(\\frac{2}{3}\\), or \\((0.67)\\), for both actions, regardless of the Kicker’s mixing strategy.\nNote that the Kicker is worse off now (\\(0.67\\) now compared to \\(0.75\\)) than when both players played 50% each action. Why?\nIf the Kicker plays Left with probability \\(q\\) and Right with probability \\(1-q\\), then the Goalie’s values are:\n\\(\\mathbb{E}(\\text{Lean Left}) = 0*q - 1*(1-q) = -1 + q\\)\n\\(\\mathbb{E}(\\text{Lean Right}) = -2*q + 0 = -2*q\\)\nSetting equal,\n\\[\n\\begin{equation}\n\\begin{split}\n-1 + q &= -2*q \\\\\n-1 &= -3*q  \\\\\n\\frac{1}{3} &= q\n\\end{split}\n\\end{equation}\n\\]\nTherefore the Kicker should play Left \\(\\frac{1}{3}\\) and Right \\(\\frac{2}{3}\\), giving a value of \\(-\\frac{2}{3}\\) to the Goalie.\nWe can see this from the game table:\n\n\n\n\n\n\n\n\nKicker/Goalie\nLean Left (\\(\\frac{2}{3}\\))\nLean Right (\\(\\frac{1}{3}\\))\n\n\n\n\nKick Left (\\(\\frac{1}{3}\\))\n0, 0 (\\(\\frac{2}{9}\\))\n+2, -2 (\\(\\frac{1}{9}\\))\n\n\nKick Right (\\(\\frac{2}{3}\\))\n+1, -1 (\\(\\frac{4}{9}\\))\n0, 0 (\\(\\frac{2}{9}\\))\n\n\n\nTherefore the expected payoffs in this game are \\(\\frac{2}{9}*0 + \\frac{1}{9}*2 + \\frac{4}{9}*1 + \\frac{2}{9}*0 = \\frac{6}{9} = 0.67\\) for the Kicker and \\(-0.67\\) for the Goalie.\nIn an equilibrium, no player should be able to unilaterally improve by changing their strategy. What if the Kicker switches to always Kick Left?\n\n\n\n\n\n\n\n\nKicker/Goalie\nLean Left (\\(\\frac{2}{3}\\))\nLean Right (\\(\\frac{1}{3}\\))\n\n\n\n\nKick Left (\\(1\\))\n0, 0 (\\(\\frac{2}{3}\\))\n+2, -2 (\\(\\frac{1}{3}\\))\n\n\nKick Right (\\(0\\))\n+1, -1 (\\(0\\))\n0, 0 (\\(0\\))\n\n\n\nNow the Kicker’s payoff is still \\(\\frac{1}{3}*2 = 0.67\\).\nWhen a player makes their opponent indifferent, this means that any action the opponent takes (within the set of equilibrium actions) will result in the same payoff!\nSo if you know your opponent is playing the equilibrium strategy, then you can actually do whatever you want with no penalty with the mixing actions. Sort of.\nThe risk is that the opponent can now deviate from equilibrium and take advantage of your new strategy. For example, if the Goalie caught on and moved to always Lean Left, then expected value is reduced to \\(0\\) for both players.\nTo summarize, you can only be penalized for not playing the equilibrium mixing strategy if your opponent plays a non-equilibrium strategy that exploits your strategy.\n\n\n\n\n\n\nIndifference\n\n\n\nWhy do players make their opponent indifferent?\n\n\n\n\n\nBack to poker. We can apply this indifference principle in computing equilibrium strategies in poker. When you make your opponent indifferent, then you don’t give them any best play.\nIf you play an equilibrium strategy, opponents will only get penalized for playing hands outside of the set of hands in the mixed strategy equilibrium (also known as the support, or the set of pure strategies that are played with non-zero probability under the mixed strategy). If opponents are not playing equilibrium, though, then they open themselves up to exploitation.\nLet’s look at one particular situation in Kuhn Poker and work it out by hand. Suppose that you are Player 2 with card Q after a Check from Player 1.\n\n\n\n\n\n\n\nExpected Value Exercise\n\n\n\nWhat indifference is Player 2 trying to induce?\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nMaking P1 indifferent between calling and folding with a K\n\n\n\nWe can work out Player 2’s betting strategy by calculating the indifference. Let \\(b\\) be the probability that P2 bets with a Q after P1 checks.\n$$ \\[\\begin{equation}\n\\begin{split}\n\\mathbb{E}(\\text{P1 Check K then Fold to Bet}) &= 0 \\\\\n\\\\\n\n\\mathbb{E}(\\text{P1 Check K then Call Bet}) &= -1*\\Pr(\\text{P2 has A and Bets}) + 3*\\Pr(\\text{P2 has Q and Bets}) \\\\\n  &= -1*\\frac{1}{2} + 3*\\frac{1}{2}*b \\\\\n  &= -0.5 + 1.5*b\n\\end{split}\n\\end{equation}\\] $$\nSetting these equal:\n\\(0 = -0.5 + 1.5*b\\)\n\\(b = \\frac{1}{3}\\)\nTherefore in equilibrium, P2 should bet \\(\\frac{1}{3}\\) with Q after P1 checks.\n\n\n\n\n\n\nEquilibrium Mixed Strategy Change Exercise\n\n\n\nIf P2 bet \\(\\frac{2}{3}\\) instead of \\(\\frac{1}{3}\\) with Q after P1 checks and P1 is playing an equilibrium strategy, how would P2’s expected value change?\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nIt wouldn’t! As long as P1 doesn’t modify their strategy, then P2 can mix his strategy however he wants and have the same EV.\n\n\n\n\n\n\n\n\n\nBluff:Value Ratio Exercise\n\n\n\nGiven that P2 has bet after P1 checks and is playing the equilibrium strategy, what is the probability that they are bluffing?\n(Note: Including cases where you have an A, so Q bets are bluffs and A bets are value bets.)\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nP2 has Q and A each \\(\\frac{1}{2}\\) of the time.\nP2 is betting Q \\(\\frac{1}{3}\\) of the time (bluffing).\nP2 is betting A always (value betting).\nTherefore for every 3 times you have Q you will bet once and for every 3 times you have A you will bet 3 times. Out of the 4 bets, 1 of them is a bluff.\n\\(\\Pr(\\text{P2 Bluff after P1 Check}) = \\frac{1}{4}\\)",
    "crumbs": [
      "Signup: AI Poker Camp Summer 2024",
      "Challenges",
      "1: Kuhn Poker",
      "Challenge Review"
    ]
  },
  {
    "objectID": "aipcs24/1kuhn_challenge_review.html#concept-nash-equilibrium",
    "href": "aipcs24/1kuhn_challenge_review.html#concept-nash-equilibrium",
    "title": "Kuhn Poker (1): Challenge Review",
    "section": "",
    "text": "A Nash equililibrium is a set of strategies for both players such that neither player ever plays an action with regret (when they take those strategies as given).\nRecall that regret only makes sense in the context of a particular strategy and assumed opponent’s strategy. When you submitted to Challenge 1, you submitted a strategy for being P1, and a strategy for being P2, but you won’t ever play against yourself – so why is it helpful to find a pair that plays against itself without regret?\nThe other paradigm for game strategies is opponent exploitation, which we will address in future sections.\n\n\nConsider the Soccer Penalty Kick game where a Kicker is trying to score a goal and the Goalie is trying to block it.\n\n\n\nKicker/Goalie\nLean Left\nLean Right\n\n\n\n\nKick Left\n0, 0\n+2, -2\n\n\nKick Right\n+1, -1\n0, 0\n\n\n\nThe game setup is zero-sum. If Kicker and Goalie both go in one direction, then it’s assumed that the goal will miss and both get \\(0\\) payoffs. If the Kicker plays Kick Right when the Goalie plays Lean Left, then the Kicker is favored and gets a payoff of \\(+1\\). If the Kicker plays Kick Left when the Goalie plays Lean Right, then the kicker is even more favored, because it’s easier to kick left than right, and gets \\(+2\\).\n\n\n\n\n\n\nNash Equilibrium Exercise\n\n\n\nWhich of these, if any, is a Nash Equilibrium? You can check by seeing if either player would benefit by changing their action.\n\n\n\nKicker\nGoalie\nEquilibrium or Change?\n\n\n\n\nLeft\nLeft\n\n\n\nLeft\nRight\n\n\n\nRight\nLeft\n\n\n\nRight\nRight\n\n\n\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThere are no pure Nash equilibrium solutions because when the actions match, the Kicker will always want to change, and when they don’t match, the Goalie will always want to change.\n\n\n\nKicker\nGoalie\nEquilibrium or Change?\n\n\n\n\nLeft\nLeft\nKicker changes to right\n\n\nLeft\nRight\nGoalie changes to left\n\n\nRight\nLeft\nGoalie changes to right\n\n\nRight\nRight\nKicker changes to left\n\n\n\n\n\n\n\n\n\n\n\n\nExpected Value Exercise\n\n\n\nAssume that they both play Left 50% and Right 50% – what is the expected value of the game?\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\n\nKicker/Goalie\nLean Left (0.5)\nLean Right (0.5)\n\n\n\n\nKick Left (0.5)\n0, 0\n+2, -2\n\n\nKick Right (0.5)\n+1, -1\n0, 0\n\n\n\nWe apply these probabilities to each of the 4 outcomes:\n\n\n\nKicker/Goalie\nLean Left (0.5)\nLean Right (0.5)\n\n\n\n\nKick Left (0.5)\n0, 0 (0.25)\n+2, -2 (0.25)\n\n\nKick Right (0.5)\n+1, -1 (0.25)\n0, 0 (0.25)\n\n\n\nNow for the Kicker, we have \\(\\mathbb{E} = 0.25*0 + 0.25*2 + 0.25*1 + 0.25*0 = 0.75\\).\nSince it’s zero-sum, we have \\(\\mathbb{E} = -0.75\\) for the Goalie.\nNote that, for example, the Kicker playing 50% Left and 50% Right could be interpreted as a single player having these probabilities or a field of players averaging to these probabilities. So out of 100 players, this could mean:\n\n100 players playing 50% Left and 50% Right\n50 players playing 100% Left and 50 players playing 100% Right\n50 players playing 75% Left/25% Right and 50 players playing 25% Left/75% right\n\n\n\n\nWhen the Goalie plays left with probability \\(p\\) and right with probability \\(1-p\\), we can find the expected value of the Kicker actions.\n\n\n\nKicker/Goalie\nLean Left (p)\nLean Right (1-p)\n\n\n\n\nKick Left\n0, 0\n+2, -2\n\n\nKick Right\n+1, -1\n0, 0\n\n\n\n\\(\\mathbb{E}(\\text{Kick Left}) = 0*p + 2*(1-p) = 2 - 2*p\\)\n\\(\\mathbb{E}(\\text{Kick Right}) = 1*p + 0*(1-p) = 1*p\\)\nThe Kicker is going to play the best response to the Goalie’s strategy. The Goalie wants to make the Kicker indifferent to Kick Left and Kick Right because if the Kicker was not going to be indifferent, then he would prefer one of the actions, meaning that action would be superior to the other. Therefore the Kicker will play a mixed strategy in response that will result in a Nash equilibrium where neither player benefits from unilaterally changing strategies. (Note that indifferent does not mean 50% each, but means the expected value is the same for each.)\n\nBy setting the values equal, we get \\(2 - 2*p = 1*p \\Rightarrow p = \\frac{2}{3}\\) as shown in the plot. This means that \\(1-p = 1 - \\frac{2}{3} = \\frac{1}{3}\\). Therefore the Goalie should play Lean Left \\(\\frac{2}{3}\\) and Lean Right \\(\\frac{1}{3}\\). The value for the Kicker is \\(\\frac{2}{3}\\), or \\((0.67)\\), for both actions, regardless of the Kicker’s mixing strategy.\nNote that the Kicker is worse off now (\\(0.67\\) now compared to \\(0.75\\)) than when both players played 50% each action. Why?\nIf the Kicker plays Left with probability \\(q\\) and Right with probability \\(1-q\\), then the Goalie’s values are:\n\\(\\mathbb{E}(\\text{Lean Left}) = 0*q - 1*(1-q) = -1 + q\\)\n\\(\\mathbb{E}(\\text{Lean Right}) = -2*q + 0 = -2*q\\)\nSetting equal,\n\\[\n\\begin{equation}\n\\begin{split}\n-1 + q &= -2*q \\\\\n-1 &= -3*q  \\\\\n\\frac{1}{3} &= q\n\\end{split}\n\\end{equation}\n\\]\nTherefore the Kicker should play Left \\(\\frac{1}{3}\\) and Right \\(\\frac{2}{3}\\), giving a value of \\(-\\frac{2}{3}\\) to the Goalie.\nWe can see this from the game table:\n\n\n\n\n\n\n\n\nKicker/Goalie\nLean Left (\\(\\frac{2}{3}\\))\nLean Right (\\(\\frac{1}{3}\\))\n\n\n\n\nKick Left (\\(\\frac{1}{3}\\))\n0, 0 (\\(\\frac{2}{9}\\))\n+2, -2 (\\(\\frac{1}{9}\\))\n\n\nKick Right (\\(\\frac{2}{3}\\))\n+1, -1 (\\(\\frac{4}{9}\\))\n0, 0 (\\(\\frac{2}{9}\\))\n\n\n\nTherefore the expected payoffs in this game are \\(\\frac{2}{9}*0 + \\frac{1}{9}*2 + \\frac{4}{9}*1 + \\frac{2}{9}*0 = \\frac{6}{9} = 0.67\\) for the Kicker and \\(-0.67\\) for the Goalie.\nIn an equilibrium, no player should be able to unilaterally improve by changing their strategy. What if the Kicker switches to always Kick Left?\n\n\n\n\n\n\n\n\nKicker/Goalie\nLean Left (\\(\\frac{2}{3}\\))\nLean Right (\\(\\frac{1}{3}\\))\n\n\n\n\nKick Left (\\(1\\))\n0, 0 (\\(\\frac{2}{3}\\))\n+2, -2 (\\(\\frac{1}{3}\\))\n\n\nKick Right (\\(0\\))\n+1, -1 (\\(0\\))\n0, 0 (\\(0\\))\n\n\n\nNow the Kicker’s payoff is still \\(\\frac{1}{3}*2 = 0.67\\).\nWhen a player makes their opponent indifferent, this means that any action the opponent takes (within the set of equilibrium actions) will result in the same payoff!\nSo if you know your opponent is playing the equilibrium strategy, then you can actually do whatever you want with no penalty with the mixing actions. Sort of.\nThe risk is that the opponent can now deviate from equilibrium and take advantage of your new strategy. For example, if the Goalie caught on and moved to always Lean Left, then expected value is reduced to \\(0\\) for both players.\nTo summarize, you can only be penalized for not playing the equilibrium mixing strategy if your opponent plays a non-equilibrium strategy that exploits your strategy.\n\n\n\n\n\n\nIndifference\n\n\n\nWhy do players make their opponent indifferent?\n\n\n\n\n\nBack to poker. We can apply this indifference principle in computing equilibrium strategies in poker. When you make your opponent indifferent, then you don’t give them any best play.\nIf you play an equilibrium strategy, opponents will only get penalized for playing hands outside of the set of hands in the mixed strategy equilibrium (also known as the support, or the set of pure strategies that are played with non-zero probability under the mixed strategy). If opponents are not playing equilibrium, though, then they open themselves up to exploitation.\nLet’s look at one particular situation in Kuhn Poker and work it out by hand. Suppose that you are Player 2 with card Q after a Check from Player 1.\n\n\n\n\n\n\n\nExpected Value Exercise\n\n\n\nWhat indifference is Player 2 trying to induce?\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nMaking P1 indifferent between calling and folding with a K\n\n\n\nWe can work out Player 2’s betting strategy by calculating the indifference. Let \\(b\\) be the probability that P2 bets with a Q after P1 checks.\n$$ \\[\\begin{equation}\n\\begin{split}\n\\mathbb{E}(\\text{P1 Check K then Fold to Bet}) &= 0 \\\\\n\\\\\n\n\\mathbb{E}(\\text{P1 Check K then Call Bet}) &= -1*\\Pr(\\text{P2 has A and Bets}) + 3*\\Pr(\\text{P2 has Q and Bets}) \\\\\n  &= -1*\\frac{1}{2} + 3*\\frac{1}{2}*b \\\\\n  &= -0.5 + 1.5*b\n\\end{split}\n\\end{equation}\\] $$\nSetting these equal:\n\\(0 = -0.5 + 1.5*b\\)\n\\(b = \\frac{1}{3}\\)\nTherefore in equilibrium, P2 should bet \\(\\frac{1}{3}\\) with Q after P1 checks.\n\n\n\n\n\n\nEquilibrium Mixed Strategy Change Exercise\n\n\n\nIf P2 bet \\(\\frac{2}{3}\\) instead of \\(\\frac{1}{3}\\) with Q after P1 checks and P1 is playing an equilibrium strategy, how would P2’s expected value change?\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nIt wouldn’t! As long as P1 doesn’t modify their strategy, then P2 can mix his strategy however he wants and have the same EV.\n\n\n\n\n\n\n\n\n\nBluff:Value Ratio Exercise\n\n\n\nGiven that P2 has bet after P1 checks and is playing the equilibrium strategy, what is the probability that they are bluffing?\n(Note: Including cases where you have an A, so Q bets are bluffs and A bets are value bets.)\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nP2 has Q and A each \\(\\frac{1}{2}\\) of the time.\nP2 is betting Q \\(\\frac{1}{3}\\) of the time (bluffing).\nP2 is betting A always (value betting).\nTherefore for every 3 times you have Q you will bet once and for every 3 times you have A you will bet 3 times. Out of the 4 bets, 1 of them is a bluff.\n\\(\\Pr(\\text{P2 Bluff after P1 Check}) = \\frac{1}{4}\\)",
    "crumbs": [
      "Signup: AI Poker Camp Summer 2024",
      "Challenges",
      "1: Kuhn Poker",
      "Challenge Review"
    ]
  },
  {
    "objectID": "aipcs24/1kuhn_challenge_review.html#to-do-below",
    "href": "aipcs24/1kuhn_challenge_review.html#to-do-below",
    "title": "Kuhn Poker (1): Challenge Review",
    "section": "TO DO BELOW",
    "text": "TO DO BELOW\nTO DO: - What if the bet size was 2 instead of 1?\n\nCan you come up with a general formula for how often to bluff given a pot and bet size?\nRange of hands vs. range of hands vs. your hand vs. their strategy. Different if their strategy never changes.\nWhat is a creative strategy to try against a player who studies poker and is trying to play an equilibrium strategy, but might be open to changing their strategy? Bet sizes they haven’t studied, do something crazy and then counterexploit?\nNode locking question\n\nIn what case could we deduce our opponent’s strategy with a large sample size? Note: We can assume that they are playing the pure strategy decisions correctly.\nFor example, if we as P1 Check with K, we can know how often they’re betting Q since we know they’ll always bet A.\nOver a sample of 1000 hands that we Check with K, we expect that 500 times they would have Q and 500 times they would have A. We expect that they would always bet with A and bet some percentage \\(p\\) of their Q hands.\nFormula for how often they’re betting Q.\nWhat’s another case we could determine? Bet Q, how often call/fold K. How to use this?",
    "crumbs": [
      "Signup: AI Poker Camp Summer 2024",
      "Challenges",
      "1: Kuhn Poker",
      "Challenge Review"
    ]
  },
  {
    "objectID": "aipcs24/1kuhn_challenge_review.html#review-of-challenge-1",
    "href": "aipcs24/1kuhn_challenge_review.html#review-of-challenge-1",
    "title": "Kuhn Poker (1): Challenge Review",
    "section": "Review of Challenge 1",
    "text": "Review of Challenge 1\nGoal: Equilibrium agent, later opponent modeling\nWhat does best bot vs you look like? What is your exploitability? How do we evaluate agents?\n\nTournament Results\nYep, just a slight variation here where the dealer burns 26/52 cards and then you play against the house; make it something other than just off-the-shelf and make your MC solver have to do some real work.\n\n\nHow the Interactive Works\nOdds vs. probs Regret Full CFR details next session Show equations and graphs of how things correct when a single probability is thrown off\n\n\nOptimal Strategies\nSurprise! There are multiple Nash equilibria in Kuhn!\nValue of the game\nPosition thing\nPrinciple of 3 general types of hands and how it applies to regular poker\n\n\nPure vs. Mixed Strategies\nGradient Descent: Involves iteratively adjusting parameters to minimize a cost function. Each step moves the parameters in the direction of the negative gradient of the cost function, gradually converging to a local or global minimum. Aims to converge to the optimal parameters that minimize the cost function. With an appropriate learning rate and sufficient iterations, it can find the minimum. Utilizes feedback from the gradient of the cost function at each iteration to update the parameters. Focuses on a static objective function (cost function) and aims to find its minimum.\nRegret Minimization: In online learning and decision-making contexts, it involves iteratively updating strategies to minimize regret, which is the difference between the actual cumulative loss and the best possible cumulative loss in hindsight. Each step adjusts the strategy based on past performance to improve future decisions. Aims to minimize regret over time, which means the strategy becomes nearly as good as the best fixed strategy in hindsight. With enough iterations, the average regret per iteration tends to zero. Utilizes feedback from past performance (losses) to update the strategy, aiming to reduce future regret. Focuses on a dynamic objective (minimizing regret over time) in potentially changing environments, where the best action may vary over time.\nRegret Matching: Involves iteratively updating the probability distribution over actions based on past regrets. Actions with higher regrets (indicating they would have performed better in the past) are chosen more frequently in the future. Uses past regrets to update the probability distribution over actions. The probability of selecting each action increases proportionally to the regret of not having taken that action.\nPruning and compare to CFR",
    "crumbs": [
      "Signup: AI Poker Camp Summer 2024",
      "Challenges",
      "1: Kuhn Poker",
      "Challenge Review"
    ]
  },
  {
    "objectID": "aipcs24/2othergames_challenge.html",
    "href": "aipcs24/2othergames_challenge.html",
    "title": "Challenge 2: Other Games",
    "section": "",
    "text": "Game/Opponent\nFixed/Probabilistic Opponent\nAdversarial Opponent\n\n\n\n\nImperfect Info, No Player Agency/Decisions\n\n\n\n\nPerfect Info, Player Actions Always Perfect Info\n\n\n\n\nImperfect Info, Imperfect from Randomness\n\n\n\n\nImperfect Info, Imperfect from Randomness AND Game States",
    "crumbs": [
      "Signup: AI Poker Camp Summer 2024",
      "Challenges",
      "2: Other Games",
      "Challenge"
    ]
  },
  {
    "objectID": "aipcs24/2othergames_challenge.html#types-of-games",
    "href": "aipcs24/2othergames_challenge.html#types-of-games",
    "title": "Challenge 2: Other Games",
    "section": "",
    "text": "Game/Opponent\nFixed/Probabilistic Opponent\nAdversarial Opponent\n\n\n\n\nImperfect Info, No Player Agency/Decisions\n\n\n\n\nPerfect Info, Player Actions Always Perfect Info\n\n\n\n\nImperfect Info, Imperfect from Randomness\n\n\n\n\nImperfect Info, Imperfect from Randomness AND Game States",
    "crumbs": [
      "Signup: AI Poker Camp Summer 2024",
      "Challenges",
      "2: Other Games",
      "Challenge"
    ]
  },
  {
    "objectID": "aipcs24/2othergames_challenge.html#simulator",
    "href": "aipcs24/2othergames_challenge.html#simulator",
    "title": "Challenge 2: Other Games",
    "section": "Simulator",
    "text": "Simulator\n\n\n\n\n\nArm\n\n\nAverage Reward\n\n\nPulls\n\n\nActions\n\n\n\n\n\n\n\n\n\nReset",
    "crumbs": [
      "Signup: AI Poker Camp Summer 2024",
      "Challenges",
      "2: Other Games",
      "Challenge"
    ]
  },
  {
    "objectID": "camp/index.html",
    "href": "camp/index.html",
    "title": "Poker and Applied Rationality Camp",
    "section": "",
    "text": "Coming soon:\n\nAug/Sep TBD: Beta in NYC or virtual\nSep 23-Nov 25: Full course virtual"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Poker Camp is a project by Ross Rheingans-Yoo and Max Chiswick."
  },
  {
    "objectID": "about.html#mission",
    "href": "about.html#mission",
    "title": "About",
    "section": "Mission",
    "text": "Mission\nTeaching practical decision-making and rationality through poker and other games, blending real-world skills with theoretical insights"
  },
  {
    "objectID": "about.html#max-bio",
    "href": "about.html#max-bio",
    "title": "About",
    "section": "Max bio",
    "text": "Max bio\n\nFormer online poker pro on PokerStars, playing over 10m hands including 990k in 1 month (record for most ever played in a month above microstakes)\nAI poker work:\n\nMSc with thesis on AI poker at Technion Israel Institute of Technology with Nahum Shimkin\nAI Poker Tutorial website (needs updating!)\nTwo poker research papers with Sam Ganzfried\n\nSubstack"
  },
  {
    "objectID": "about.html#ross-bio",
    "href": "about.html#ross-bio",
    "title": "About",
    "section": "Ross bio",
    "text": "Ross bio\n\nFormer Jane Street trader\nWrote the advanced trading simulations for the Jane Street trading internship program from 2018 to 2021\nHomepage\nTwitter"
  },
  {
    "objectID": "about.html#contribute",
    "href": "about.html#contribute",
    "title": "About",
    "section": "Contribute",
    "text": "Contribute\n\nWe are in the process of becoming a 501(c)(3) nonprofit. If you are interested in contributing monetarily, please see our .org site: https://pokercamp.org.\nIf you are interested in contributing to materials or working with us, be in touch"
  }
]