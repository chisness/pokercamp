[
  {
    "objectID": "aicamp/index.html",
    "href": "aicamp/index.html",
    "title": "AI Poker Camp",
    "section": "",
    "text": "Build reinforcement learning AI agents to play games\n\nJul 15-Aug 15: Beta in SF\nJan 6-31, 2025: Intensive, structure TBD\nFeb 3-Apr 11, 2025: 10 sessions meeting once/week"
  },
  {
    "objectID": "odpcf24/riskandluck.html",
    "href": "odpcf24/riskandluck.html",
    "title": "ODPC #5: Risk and Luck",
    "section": "",
    "text": "Risk and Luck",
    "crumbs": [
      "About",
      "Sessions",
      "#5: Risk and Luck"
    ]
  },
  {
    "objectID": "odpcf24/gametheory.html",
    "href": "odpcf24/gametheory.html",
    "title": "ODPC #3: Game Theory",
    "section": "",
    "text": "Game Theory",
    "crumbs": [
      "About",
      "Sessions",
      "#3: Game Theory"
    ]
  },
  {
    "objectID": "odpcf24/predictingthefuture.html",
    "href": "odpcf24/predictingthefuture.html",
    "title": "ODPC #6: Predicting the Future",
    "section": "",
    "text": "Predicting the Future",
    "crumbs": [
      "About",
      "Sessions",
      "#6: Predicting the Future"
    ]
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Poker Camp is run by Ross Rheingans-Yoo and Max Chiswick"
  },
  {
    "objectID": "about.html#max-bio",
    "href": "about.html#max-bio",
    "title": "About",
    "section": "Max bio",
    "text": "Max bio\n\nFormer online poker pro on PokerStars. Played over 10m hands, including 990k in Dec, 2009.\nAI poker work:\n\nMSc with thesis on AI poker at Technion Israel Institute of Technology with Nahum Shimkin\nAI Poker Tutorial v1 website (needs updating!)\nTwo poker research papers with Sam Ganzfried\n\nBlog"
  },
  {
    "objectID": "about.html#ross-bio",
    "href": "about.html#ross-bio",
    "title": "About",
    "section": "Ross bio",
    "text": "Ross bio\n\nFormer Jane Street trader\nWrote the advanced trading simulations for the Jane Street trading internship program from 2018 to 2021\nHomepage\nTwitter"
  },
  {
    "objectID": "about.html#contribute",
    "href": "about.html#contribute",
    "title": "About",
    "section": "Contribute",
    "text": "Contribute\n\nWe are in the process of becoming a 501(c)(3) nonprofit. If you are interested in contributing monetarily, please see our .org site: https://pokercamp.org.\nIf you are interested in contributing to materials or working with us, be in touch"
  },
  {
    "objectID": "camp/index.html",
    "href": "camp/index.html",
    "title": "Optimal Decision Poker Camp",
    "section": "",
    "text": "Think about probability and decision making through interactive games\n\nSep 9-Oct 14: 6 weekly Monday sessions in NYC\n\nBehind the Cards and Beyond the Bets: Optimal Decision Making in Gambling and Life\n\nOct: Maybe 1-day workshop in Tel Aviv\nFeb 3-Apr 11, 2025: 10 sessions meeting once/week"
  },
  {
    "objectID": "aipcs24/1kuhn_rules.html",
    "href": "aipcs24/1kuhn_rules.html",
    "title": "#1: Kuhn Poker | Rules",
    "section": "",
    "text": "Kuhn Poker is the simplest nontrivial version of poker. Here is the setup:\n\n3 card deck: Queen, King, Ace (in ascending order, so Ace is highest)\n\n\n\n\n\n\n\nAlternative (but equivalent) decks\n\n\n\n\n\nAny deck of 3 ranked cards works. For example:\n\nThe python game engine that we’ll use in later challenges uses {0, 1, 2}.\nWikipedia’s description uses {Jack, Queen, King}.\n\n\n\n\n \n\nKuhn Poker is played with 2 players. For this exercise, they’ll be  and .\nEach player starts with 2 chips and both players ante 1 chip.\nDeal 1 card to each player (discard the third).\n\n\n\nThere is one street of betting in which players can take these actions:\n\n↑Up (putting a chip into the pot)\n↓Down (not putting a chip into the pot)\n\n\n\n\n\n\n\n\n↑Up and ↓Down in traditional poker terms\n\n\n\n\n↑Up actions indicate a Bet or Call.\n↓Down actions indicate a Check or Fold.\n\n\n\n\n\n\n\n\n\nExample game\n\n\n\n\nPlayers ante and cards are dealt.\n sees a A and plays ↑Up (1 more chip into pot).\n sees a K and plays ↑Up (1 more chip into pot).\nBoth cards are revealed in a 2-chip showdown.  has an A and  has a K.\n has the better hand and wins +2 chips (+1 from the ante, +1 from ’s ↑Up).\n\n\n\nThe betting (and the game) can go in three ways:\n\nOne player plays ↑Up, then the other player plays ↓Down. The player who played ↓Down folds. The winner wins the loser’s ante (and gets their own chip back). The players’ cards are not revealed. Note that this happens if the action is :\n\n\n↑Up, ↓Down, or\n↓Down, ↑Up, ↓Down.\n\n\nBoth players play ↓Down. They go to a showdown and the winner wins the one chip that the loser anted (and their own back).\nA player plays ↑Up, then the other player plays ↑Up. They go to a showdown and the winner wins the two chips the loser has put in the pot (and gets their own chips back). Note that the game will proceed to a 2-chip showdown if the action is:\n\n\n↑Up, ↑Up or - ↓Down, ↑Up , ↑Up.\n\nHere is a list of all possible betting sequences:\n\n\n\n\n\n\n\n\n\n\n\n\nWinner\n\n\n\n\n↑Up\n↓Down\n\n (+1)\n\n\n↑Up\n↑Up\n\nHigher Card (+2)\n\n\n↓Down\n↓Down\n\nHigher Card (+1)\n\n\n↓Down\n↑Up\n↓Down\n (+1)\n\n\n↓Down\n↑Up\n↑Up\nHigher Card (+2)\n\n\n\n\n\n\n\n\n\nPartner Exercise: Get started with Kuhn Poker\n\n\n\n\nGet cards, chips, paper, pen\nPlay 3 hands of Kuhn Poker and get used to how the game runs\nUse the pen and paper to start writing down all deterministic situations in the game (situations where there is clearly a correct move that you should take 100% of the time)\nPlay more hands as you think helpful\nOnce you have what you think is a full list of the deterministic states, you can stop and review the optional reading",
    "crumbs": [
      "About",
      "Challenges",
      "#1: Kuhn Poker",
      "Rules"
    ]
  },
  {
    "objectID": "aipcs24/1kuhn_rules.html#kuhn-poker-rules",
    "href": "aipcs24/1kuhn_rules.html#kuhn-poker-rules",
    "title": "#1: Kuhn Poker | Rules",
    "section": "",
    "text": "Kuhn Poker is the simplest nontrivial version of poker. Here is the setup:\n\n3 card deck: Queen, King, Ace (in ascending order, so Ace is highest)\n\n\n\n\n\n\n\nAlternative (but equivalent) decks\n\n\n\n\n\nAny deck of 3 ranked cards works. For example:\n\nThe python game engine that we’ll use in later challenges uses {0, 1, 2}.\nWikipedia’s description uses {Jack, Queen, King}.\n\n\n\n\n \n\nKuhn Poker is played with 2 players. For this exercise, they’ll be  and .\nEach player starts with 2 chips and both players ante 1 chip.\nDeal 1 card to each player (discard the third).\n\n\n\nThere is one street of betting in which players can take these actions:\n\n↑Up (putting a chip into the pot)\n↓Down (not putting a chip into the pot)\n\n\n\n\n\n\n\n\n↑Up and ↓Down in traditional poker terms\n\n\n\n\n↑Up actions indicate a Bet or Call.\n↓Down actions indicate a Check or Fold.\n\n\n\n\n\n\n\n\n\nExample game\n\n\n\n\nPlayers ante and cards are dealt.\n sees a A and plays ↑Up (1 more chip into pot).\n sees a K and plays ↑Up (1 more chip into pot).\nBoth cards are revealed in a 2-chip showdown.  has an A and  has a K.\n has the better hand and wins +2 chips (+1 from the ante, +1 from ’s ↑Up).\n\n\n\nThe betting (and the game) can go in three ways:\n\nOne player plays ↑Up, then the other player plays ↓Down. The player who played ↓Down folds. The winner wins the loser’s ante (and gets their own chip back). The players’ cards are not revealed. Note that this happens if the action is :\n\n\n↑Up, ↓Down, or\n↓Down, ↑Up, ↓Down.\n\n\nBoth players play ↓Down. They go to a showdown and the winner wins the one chip that the loser anted (and their own back).\nA player plays ↑Up, then the other player plays ↑Up. They go to a showdown and the winner wins the two chips the loser has put in the pot (and gets their own chips back). Note that the game will proceed to a 2-chip showdown if the action is:\n\n\n↑Up, ↑Up or - ↓Down, ↑Up , ↑Up.\n\nHere is a list of all possible betting sequences:\n\n\n\n\n\n\n\n\n\n\n\n\nWinner\n\n\n\n\n↑Up\n↓Down\n\n (+1)\n\n\n↑Up\n↑Up\n\nHigher Card (+2)\n\n\n↓Down\n↓Down\n\nHigher Card (+1)\n\n\n↓Down\n↑Up\n↓Down\n (+1)\n\n\n↓Down\n↑Up\n↑Up\nHigher Card (+2)\n\n\n\n\n\n\n\n\n\nPartner Exercise: Get started with Kuhn Poker\n\n\n\n\nGet cards, chips, paper, pen\nPlay 3 hands of Kuhn Poker and get used to how the game runs\nUse the pen and paper to start writing down all deterministic situations in the game (situations where there is clearly a correct move that you should take 100% of the time)\nPlay more hands as you think helpful\nOnce you have what you think is a full list of the deterministic states, you can stop and review the optional reading",
    "crumbs": [
      "About",
      "Challenges",
      "#1: Kuhn Poker",
      "Rules"
    ]
  },
  {
    "objectID": "aipcs24/4nlrhe_reading.html",
    "href": "aipcs24/4nlrhe_reading.html",
    "title": "#5: No Limit Royal Hold’em | Reading",
    "section": "",
    "text": "This game is the same as regular No Limit Texas Hold’em, except it uses a deck size of 20 cards instead of 52 and antes instead of blinds. The deck consists of all cards Ten and higher (T, J, Q, K, A) and all suits.\nThe variation we will use has each player starting with 20 chips and blinds of 1 and 2. We will have 2 betting rounds: preflop and the flop.\nBet sizes that are legal are:\n\nMin: \\(\\min(1, \\text{previous bet or raise size})\\)\nMax: \\(\\text{current stack size}\\)\n\n\n\nHand ranks are based on the probability of each type of hand occurring. Note that since we are making 5-card hands, we always use the 5-card probabilities. In Texas Hold’em, with 7 cards, you are more likely to have a pair (43.8%) than high-card (17.4%), but we use the 5-card probabilities of pair (42.2%) and high-card (50.1%) when determining hand ranks.\nFor Royal Hold’em, the total number of deals is: \\({20 \\choose 5} = \\frac{20!}{15!*5!} = \\frac{20*19*18*17*16}{5*4*3*2*1} = 15504\\)\nNow we compute the frequency of every hand ranking and divide by the total number of deals to get the probability. Note that there is only one kind of flush, which is the Royal Flush from Ten to Ace. Also that it is not possible to have only high-card.\n\n\n\n\n\n\n\n\n\n\nHand\nFrequency Calculation\nFrequency\nProbability\nCumulative Probability\n\n\n\n\nRoyal Flush\n\\({4 \\choose 1}\\)\n\\(4\\)\n\\(0.000258\\)\n\\(0.000258\\)\n\n\nFour of a Kind\n\\({5 \\choose 1}{4 \\choose 4}{4 \\choose 1}{4 \\choose 1}\\)\n\\(80\\)\n\\(0.00516\\)\n\\(0.005418\\)\n\n\nFull House\n\\({5 \\choose 1}{4 \\choose 3}{4 \\choose 1}{4 \\choose 2}\\)\n\\(480\\)\n\\(0.03096\\)\n\\(0.036378\\)\n\n\nStraight (excluding Royal Flushes)\n\\({4 \\choose 1}^5 - {4 \\choose 1}\\)\n\\(1020\\)\n\\(0.065789\\)\n\\(0.102167\\)\n\n\nThree of a Kind\n\\({5 \\choose 1}{4 \\choose 3}{4 \\choose 2}{4 \\choose 1}^2\\)\n\\(1920\\)\n\\(0.123839\\)\n\\(0.226006\\)\n\n\nTwo Pair\n\\({5 \\choose 2}{4 \\choose 2}^2{3 \\choose 1}{4 \\choose 1}\\)\n\\(4320\\)\n\\(0.278638\\)\n\\(0.504644\\)\n\n\nOne Pair\n\\({5 \\choose 1}{4 \\choose 2}{4 \\choose 3}{4 \\choose 1}^3\\)\n\\(7680\\)\n\\(0.495356\\)\n\\(1\\)\n\n\n\nThe rankings work out to be the same as in regular Texas Hold’em, except that there are no high-card hands and no regular flush hands.",
    "crumbs": [
      "About",
      "Challenges",
      "#4: Royal Hold'em",
      "Class Materials"
    ]
  },
  {
    "objectID": "aipcs24/4nlrhe_reading.html#no-limit-royal-holdem",
    "href": "aipcs24/4nlrhe_reading.html#no-limit-royal-holdem",
    "title": "#5: No Limit Royal Hold’em | Reading",
    "section": "",
    "text": "This game is the same as regular No Limit Texas Hold’em, except it uses a deck size of 20 cards instead of 52 and antes instead of blinds. The deck consists of all cards Ten and higher (T, J, Q, K, A) and all suits.\nThe variation we will use has each player starting with 20 chips and blinds of 1 and 2. We will have 2 betting rounds: preflop and the flop.\nBet sizes that are legal are:\n\nMin: \\(\\min(1, \\text{previous bet or raise size})\\)\nMax: \\(\\text{current stack size}\\)\n\n\n\nHand ranks are based on the probability of each type of hand occurring. Note that since we are making 5-card hands, we always use the 5-card probabilities. In Texas Hold’em, with 7 cards, you are more likely to have a pair (43.8%) than high-card (17.4%), but we use the 5-card probabilities of pair (42.2%) and high-card (50.1%) when determining hand ranks.\nFor Royal Hold’em, the total number of deals is: \\({20 \\choose 5} = \\frac{20!}{15!*5!} = \\frac{20*19*18*17*16}{5*4*3*2*1} = 15504\\)\nNow we compute the frequency of every hand ranking and divide by the total number of deals to get the probability. Note that there is only one kind of flush, which is the Royal Flush from Ten to Ace. Also that it is not possible to have only high-card.\n\n\n\n\n\n\n\n\n\n\nHand\nFrequency Calculation\nFrequency\nProbability\nCumulative Probability\n\n\n\n\nRoyal Flush\n\\({4 \\choose 1}\\)\n\\(4\\)\n\\(0.000258\\)\n\\(0.000258\\)\n\n\nFour of a Kind\n\\({5 \\choose 1}{4 \\choose 4}{4 \\choose 1}{4 \\choose 1}\\)\n\\(80\\)\n\\(0.00516\\)\n\\(0.005418\\)\n\n\nFull House\n\\({5 \\choose 1}{4 \\choose 3}{4 \\choose 1}{4 \\choose 2}\\)\n\\(480\\)\n\\(0.03096\\)\n\\(0.036378\\)\n\n\nStraight (excluding Royal Flushes)\n\\({4 \\choose 1}^5 - {4 \\choose 1}\\)\n\\(1020\\)\n\\(0.065789\\)\n\\(0.102167\\)\n\n\nThree of a Kind\n\\({5 \\choose 1}{4 \\choose 3}{4 \\choose 2}{4 \\choose 1}^2\\)\n\\(1920\\)\n\\(0.123839\\)\n\\(0.226006\\)\n\n\nTwo Pair\n\\({5 \\choose 2}{4 \\choose 2}^2{3 \\choose 1}{4 \\choose 1}\\)\n\\(4320\\)\n\\(0.278638\\)\n\\(0.504644\\)\n\n\nOne Pair\n\\({5 \\choose 1}{4 \\choose 2}{4 \\choose 3}{4 \\choose 1}^3\\)\n\\(7680\\)\n\\(0.495356\\)\n\\(1\\)\n\n\n\nThe rankings work out to be the same as in regular Texas Hold’em, except that there are no high-card hands and no regular flush hands.",
    "crumbs": [
      "About",
      "Challenges",
      "#4: Royal Hold'em",
      "Class Materials"
    ]
  },
  {
    "objectID": "aipcs24/4nlrhe_reading.html#game-sizes",
    "href": "aipcs24/4nlrhe_reading.html#game-sizes",
    "title": "#5: No Limit Royal Hold’em | Reading",
    "section": "Game Sizes",
    "text": "Game Sizes\nSourced from Michael Johanson’s 2013 paper Measuring the Size of Large No-Limit Poker Games\n\nThe size of a game is a simple heuristic that can be used to describe its complexity and compare it to other games, and a game’s size can be measured in several ways.\n\n\nThe most commonly used measurement is to count the number of game states in a game: the number of possible sequences of actions by the players or by chance, as viewed by a third party that observes all of the players’ actions.\n\n\nIn the poker setting, this would include all of the ways that the players’ private and public cards can be dealt and all of the possible betting sequences.\n\n\nThis number allows us to compare a game against other games such as chess or backgammon, which have \\(10^{47}\\) and \\(10^{20}\\) distinct game states respectively (not including transpositions).\n\nThe card frequencies are the same for limit and no limit variations of Hold’em. These can be calculated separately from the betting sequences.\nLimit Hold’em game sizes are easier to measure because there is a fixed maximum size of bets (usually 4 per round) that have no dependence on previous rounds/actions. The infosets and infoset-actions are the most relevant size values since each infoset and action has two tabular values, one for accumulated regret and one for accumulating average strategy. The game size for Limit Hold’em is \\(3.589 \\times 10^{13}\\) infoset-actions.\nHowever No Limit Hold’em game sizes are more complex and depend on three factors:\n\nAmount of money remaining\nSize of the bet facing\nIf a check is legal\n\n\nWithin one betting round, any two decision points that are identical in these three factors will have the same legal actions and the same betting subtrees for the remainder of the game, regardless of other aspects of their history.\n\n\nSecond,each of these factors only increases or decreases during a round. A player’s stack size only decreases as they make bets or call an opponent’s bets. The bet being faced is zero at the start of a round (or if the opponent has checked), and can only remain the same or increase during a round.\n\n\nFinally, a check is only allowed as the first action of a round.\n\n\nThese observations mean that we do not have to walk the entire game in order to count the decision points. Instead of considering each betting history independently, we will instead consider the relatively small number of possible configurations of round, stack-size, bet-faced, and check-allowed, and do so one round at a time, starting from the start of the game.\n\nRoyal No Limit Hold’em was suggested as a testbed game since it is small enough to be tractable on consumer-grade computers and resembles regular No Limit Texas Hold’em. The relatively small size means that unabstracted equilibrium and best-response computations would be tractable and therefore abstractions could be evaluated to motivate research that translates to larger domains.",
    "crumbs": [
      "About",
      "Challenges",
      "#4: Royal Hold'em",
      "Class Materials"
    ]
  },
  {
    "objectID": "aipcs24/4nlrhe_reading.html#abstraction-in-poker",
    "href": "aipcs24/4nlrhe_reading.html#abstraction-in-poker",
    "title": "#5: No Limit Royal Hold’em | Reading",
    "section": "Abstraction in Poker",
    "text": "Abstraction in Poker\nAbstraction refers to shrinking the game in some way to make it more tractable to solve.\nThe main ways to do this in poker are through card bucketing and bet size restrictions.\n\nCard Abstraction\nCard abstraction entails putting sets of cards into buckets. For example, in 100 card Kuhn Poker, we could have a bucket of cards for every 10 card ranks. This means that cards 1-10, 11-20, …, 91-100 are all in a separate bucket. A bucket would be represented by an infoset and so you would play any hand within a bucket the same way – only if there is a showdown is the true hand and outcome revealed.\nThere are more complex ways to bucket by hand strength in Texas Hold’em. The simplest bucketing method is preflop lossless abstraction such that hands can be reduced from \\({52 \\choose 2} = 1326\\) to \\(169\\) starting hands.\nOf the \\(169\\), there are:\n\n\\(13\\) pairs, one for each rank\n\\(78\\) suited hands\n\\(78\\) offsuit hands\n\nThe suited and offsuit hands are \\(78\\) each because there are \\(13*12 = 156\\) total types of unpaired hands. The only thing that matters when playing them preflop is whether they are suited or offsuit. We split this into \\(78\\) suited and \\(78\\) offsuit hands.\n\n\n\n\n\n\nHand Combinations\n\n\n\nHow many unabstracted hand combinations of each of these would there have been in the \\(1326\\) starting hands?\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\\(13 * {4 \\choose 2} = 13*6 = 78\\) pairs\n\\(78 * {4 \\choose 1} = 4 = 312\\) suited hands\n\\(78 * (4*3) = 936\\) offsuit hands\n\n\\(78 + 312 + 936 = 1326\\) total hands\n\n\n\n\n\nBet Abstraction\nBet abstraction requires allowing only a limited number of bet sizes when solving the game. A limited number of bet sizes is important to reduce game size since in no limit games, players can generally bet any amount up to their stack size.\nThe abstracted actions could include things like:\n\nMinimum bet\n\\(25\\%\\) pot\n\\(50\\%\\) pot\n\\(75\\%\\) pot\n\\(100\\%\\) pot\n\\(200\\%\\) pot\nAllin\n\nThis works for training, but then a problem arises when playing when the opponent makes a bet that does not match the abstracted bet sizes that were learned during training.\nThe agent has to translate the bet to understand how to respond.\nIn 2013, Ganzfried and Sandholm proposed a mapping as follows:\n\\(f_{A,B}(x) = \\frac{(B-x)(1+A)}{(B-A)(1+x)}\\)\nWhere \\(x\\) is the opponent bet and it is an element of \\([A, B]\\), where \\(A\\) is the largest betting size in the abstraction that is \\(\\leq x\\) and \\(B\\) is the smallest betting size in the abstraction that is \\(\\geq x\\), assuming \\(0 \\leq A \\leq B\\).\nThe result is the probability that we map the action to \\(A\\) and \\((1-f_{A,B}(x))\\) is the probability that we map the action to \\(B\\).\n\n\n\n\n\n\nAction Translation\n\n\n\nSuppose that the pot size is \\(100\\) with a minimum bet of \\(2\\) and that our abstraction has {minimum bet, half pot bet, full pot bet, allin}. How would we translate a bet of \\(33\\)?\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\\[\n\\begin{align}\nf_{A,B}(x) &= \\frac{(B-x)(1+A)}{(B-A)(1+x)} \\\\\n&= \\frac{(50-33)(1+2)}{(50-2)(1+33)} \\\\\n&= \\frac{(17)(3)}{(48)(34)} \\\\\n&= .031\n\\end{align}\n\\]\nWe would translate this to minimum bet about \\(3\\%\\) of the time and to half pot bet about \\(97\\%\\) of the time.",
    "crumbs": [
      "About",
      "Challenges",
      "#4: Royal Hold'em",
      "Class Materials"
    ]
  },
  {
    "objectID": "aipcs24/2leduc_leaderboard.html#leduc-poker",
    "href": "aipcs24/2leduc_leaderboard.html#leduc-poker",
    "title": "#2: Leduc Poker | Challenge Leaderboard",
    "section": "Leduc Poker",
    "text": "Leduc Poker",
    "crumbs": [
      "About",
      "Challenges",
      "#2: Leduc Poker",
      "Leaderboard"
    ]
  },
  {
    "objectID": "aipcs24/1kuhn_extrareadings.html",
    "href": "aipcs24/1kuhn_extrareadings.html",
    "title": "#1: Kuhn Poker | Extra Readings",
    "section": "",
    "text": "Exercise\n\n\n\nFill in the table below with games you know about. Thanks to Eliezer for getting us started.\n\n\n\n\n\n\n\n\n\n\nGame/Opponent\nFixed/Probabilistic Opponent\nAdversarial Opponent\n\n\n\n\nImperfect Info, No Player Agency/Decisions\n\n\n\n\nPerfect Info, Player Actions Always Perfect Info\n\n\n\n\nImperfect Info, Imperfect from Randomness\n\n\n\n\nImperfect Info, Imperfect from Randomness AND Game States\n\n\n\n\n\n\n\n\n\n\n\nPre-Filled Table\n\n\n\n\n\n\n\n\n\n\n\n\n\nGame/Opponent\nFixed/Probabilistic Opponent\nAdversarial Opponent\n\n\n\n\nImperfect Info, No Player Agency/Decisions\nCandy Land, War, Dreidel, Bingo, Chutes and Ladders, Slot machine\n\n\n\nPerfect Info, Player Actions Always Perfect Info\nPuzzles\nTictactoe, Checkers, Chess, Arimaa, Go\n\n\nImperfect Info, Imperfect from Randomness\nBlackjack\nBackgammon\n\n\nImperfect Info, Imperfect from Randomness AND Game States\nPartial Info Multi-armed Bandit\nPoker, Rock Paper Scissors, Liar’s Dice, Figgie"
  },
  {
    "objectID": "aipcs24/1kuhn_extrareadings.html#types-of-games",
    "href": "aipcs24/1kuhn_extrareadings.html#types-of-games",
    "title": "#1: Kuhn Poker | Extra Readings",
    "section": "",
    "text": "Exercise\n\n\n\nFill in the table below with games you know about. Thanks to Eliezer for getting us started.\n\n\n\n\n\n\n\n\n\n\nGame/Opponent\nFixed/Probabilistic Opponent\nAdversarial Opponent\n\n\n\n\nImperfect Info, No Player Agency/Decisions\n\n\n\n\nPerfect Info, Player Actions Always Perfect Info\n\n\n\n\nImperfect Info, Imperfect from Randomness\n\n\n\n\nImperfect Info, Imperfect from Randomness AND Game States\n\n\n\n\n\n\n\n\n\n\n\nPre-Filled Table\n\n\n\n\n\n\n\n\n\n\n\n\n\nGame/Opponent\nFixed/Probabilistic Opponent\nAdversarial Opponent\n\n\n\n\nImperfect Info, No Player Agency/Decisions\nCandy Land, War, Dreidel, Bingo, Chutes and Ladders, Slot machine\n\n\n\nPerfect Info, Player Actions Always Perfect Info\nPuzzles\nTictactoe, Checkers, Chess, Arimaa, Go\n\n\nImperfect Info, Imperfect from Randomness\nBlackjack\nBackgammon\n\n\nImperfect Info, Imperfect from Randomness AND Game States\nPartial Info Multi-armed Bandit\nPoker, Rock Paper Scissors, Liar’s Dice, Figgie"
  },
  {
    "objectID": "aipcs24/1kuhn_extrareadings.html#concept-nash-equilibrium",
    "href": "aipcs24/1kuhn_extrareadings.html#concept-nash-equilibrium",
    "title": "#1: Kuhn Poker | Extra Readings",
    "section": "Concept: Nash Equilibrium",
    "text": "Concept: Nash Equilibrium\nA Nash equililibrium is a set of strategies for both players such that neither player ever plays an action with regret. Under Nash equilibrium, no player can gain by unilaterally deviating from their strategy. The other paradigm for game strategies is opponent exploitation, which we will address in future sections.\nRecall that regret only makes sense in the context of a particular strategy and assumed opponent’s strategy. When you submit a strategy to Challenge 1, you submit a strategy for being P1, and a strategy for being P2, but you won’t ever play against yourself – so why is it helpful to find a pair that plays against itself without regret?"
  },
  {
    "objectID": "aipcs24/1kuhn_extrareadings.html#indifference-and-penalty-kicks",
    "href": "aipcs24/1kuhn_extrareadings.html#indifference-and-penalty-kicks",
    "title": "#1: Kuhn Poker | Extra Readings",
    "section": "Indifference and Penalty Kicks",
    "text": "Indifference and Penalty Kicks\nConsider the Soccer Penalty Kick game where a Kicker is trying to score a goal and the Goalie is trying to block it.\n\n\n\nKicker/Goalie\nLean Left\nLean Right\n\n\n\n\nKick Left\n0, 0\n+2, -2\n\n\nKick Right\n+1, -1\n0, 0\n\n\n\nThe game setup is zero-sum. If Kicker and Goalie both go in one direction, then it’s assumed that the goal will miss and both get \\(0\\) payoffs. If the Kicker plays Kick Right when the Goalie plays Lean Left, then the Kicker is favored and gets a payoff of \\(+1\\). If the Kicker plays Kick Left when the Goalie plays Lean Right, then the kicker is even more favored, because it’s easier to kick left than right, and gets \\(+2\\).\n\n\n\n\n\n\nNash Equilibrium Exercise\n\n\n\nWhich of these, if any, is a Nash equilibrium? You can check by seeing if either player would benefit by changing their action.\n\n\n\nKicker\nGoalie\nEquilibrium or Change?\n\n\n\n\nLeft\nLeft\n\n\n\nLeft\nRight\n\n\n\nRight\nLeft\n\n\n\nRight\nRight\n\n\n\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThere are no pure Nash equilibrium solutions because when the actions match, the Kicker will always want to change, and when they don’t match, the Goalie will always want to change.\n\n\n\nKicker\nGoalie\nEquilibrium or Change?\n\n\n\n\nLeft\nLeft\nKicker changes to right\n\n\nLeft\nRight\nGoalie changes to left\n\n\nRight\nLeft\nGoalie changes to right\n\n\nRight\nRight\nKicker changes to left\n\n\n\n\n\n\n\n\n\n\n\n\nExpected Value Exercise\n\n\n\nAssume that they both play Left 50% and Right 50% – what is the expected value of the game?\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\n\nKicker/Goalie\nLean Left (0.5)\nLean Right (0.5)\n\n\n\n\nKick Left (0.5)\n0, 0\n+2, -2\n\n\nKick Right (0.5)\n+1, -1\n0, 0\n\n\n\nWe apply these probabilities to each of the 4 outcomes:\n\n\n\nKicker/Goalie\nLean Left (0.5)\nLean Right (0.5)\n\n\n\n\nKick Left (0.5)\n0, 0 (0.25)\n+2, -2 (0.25)\n\n\nKick Right (0.5)\n+1, -1 (0.25)\n0, 0 (0.25)\n\n\n\nNow for the Kicker, we have \\(\\mathbb{E} = 0.25*0 + 0.25*2 + 0.25*1 + 0.25*0 = 0.75\\).\nSince it’s zero-sum, we have \\(\\mathbb{E} = -0.75\\) for the Goalie.\nNote that, for example, the Kicker playing 50% Left and 50% Right could be interpreted as a single player having these probabilities or a field of players averaging to these probabilities. So out of 100 players, this could mean:\n\n100 players playing 50% Left and 50% Right\n50 players playing 100% Left and 50 players playing 100% Right\n50 players playing 75% Left/25% Right and 50 players playing 25% Left/75% right\n\n\n\n\nWhen the Goalie plays left with probability \\(p\\) and right with probability \\(1-p\\), we can find the expected value of the Kicker actions.\n\n\n\nKicker/Goalie\nLean Left (p)\nLean Right (1-p)\n\n\n\n\nKick Left\n0, 0\n+2, -2\n\n\nKick Right\n+1, -1\n0, 0\n\n\n\n\\(\\mathbb{E}(\\text{Kick Left}) = 0*p + 2*(1-p) = 2 - 2*p\\)\n\\(\\mathbb{E}(\\text{Kick Right}) = 1*p + 0*(1-p) = 1*p\\)\nThe Kicker is going to play the best response to the Goalie’s strategy. The Goalie wants to make the Kicker indifferent to Kick Left and Kick Right because if the Kicker was not going to be indifferent, then he would prefer one of the actions, meaning that action would be superior to the other. Therefore the Kicker will play a mixed strategy in response that will result in a Nash equilibrium where neither player benefits from unilaterally changing strategies. (Note that indifferent does not mean 50% each, but means the expected value is the same for each.)\n\nBy setting the values equal, we get \\(2 - 2*p = 1*p \\Rightarrow p = \\frac{2}{3}\\) as shown in the plot. This means that \\(1-p = 1 - \\frac{2}{3} = \\frac{1}{3}\\). Therefore the Goalie should play Lean Left \\(\\frac{2}{3}\\) and Lean Right \\(\\frac{1}{3}\\). The value for the Kicker is \\(\\frac{2}{3}\\), or \\((0.67)\\), for both actions, regardless of the Kicker’s mixing strategy.\nNote that the Kicker is worse off now (\\(0.67\\) now compared to \\(0.75\\)) than when both players played 50% each action. Why?\nIf the Kicker plays Left with probability \\(q\\) and Right with probability \\(1-q\\), then the Goalie’s values are:\n\\(\\mathbb{E}(\\text{Lean Left}) = 0*q - 1*(1-q) = -1 + q\\)\n\\(\\mathbb{E}(\\text{Lean Right}) = -2*q + 0 = -2*q\\)\nSetting equal,\n\\[\n\\begin{equation}\n\\begin{split}\n-1 + q &= -2*q \\\\\n-1 &= -3*q  \\\\\n\\frac{1}{3} &= q\n\\end{split}\n\\end{equation}\n\\]\nTherefore the Kicker should play Left \\(\\frac{1}{3}\\) and Right \\(\\frac{2}{3}\\), giving a value of \\(-\\frac{2}{3}\\) to the Goalie.\nWe can see this from the game table:\n\n\n\n\n\n\n\n\nKicker/Goalie\nLean Left (\\(\\frac{2}{3}\\))\nLean Right (\\(\\frac{1}{3}\\))\n\n\n\n\nKick Left (\\(\\frac{1}{3}\\))\n0, 0 (\\(\\frac{2}{9}\\))\n+2, -2 (\\(\\frac{1}{9}\\))\n\n\nKick Right (\\(\\frac{2}{3}\\))\n+1, -1 (\\(\\frac{4}{9}\\))\n0, 0 (\\(\\frac{2}{9}\\))\n\n\n\nTherefore the expected payoffs in this game are \\(\\frac{2}{9}*0 + \\frac{1}{9}*2 + \\frac{4}{9}*1 + \\frac{2}{9}*0 = \\frac{6}{9} = 0.67\\) for the Kicker and \\(-0.67\\) for the Goalie.\nIn an equilibrium, no player should be able to unilaterally improve by changing their strategy. What if the Kicker switches to always Kick Left?\n\n\n\n\n\n\n\n\nKicker/Goalie\nLean Left (\\(\\frac{2}{3}\\))\nLean Right (\\(\\frac{1}{3}\\))\n\n\n\n\nKick Left (\\(1\\))\n0, 0 (\\(\\frac{2}{3}\\))\n+2, -2 (\\(\\frac{1}{3}\\))\n\n\nKick Right (\\(0\\))\n+1, -1 (\\(0\\))\n0, 0 (\\(0\\))\n\n\n\nNow the Kicker’s payoff is still \\(\\frac{1}{3}*2 = 0.67\\).\nWhen a player makes their opponent indifferent, this means that any action the opponent takes (within the set of equilibrium actions) will result in the same payoff!\nSo if you know your opponent is playing the equilibrium strategy, then you can actually do whatever you want with no penalty with the mixing actions. Sort of.\nThe risk is that the opponent can now deviate from equilibrium and take advantage of your new strategy. For example, if the Goalie caught on and moved to always Lean Left, then expected value is reduced to \\(0\\) for both players.\nTo summarize, you can only be penalized for not playing the equilibrium mixing strategy if your opponent plays a non-equilibrium strategy that exploits your strategy.\n\n\n\n\n\n\nIndifference\n\n\n\nWhy do players make their opponent indifferent?"
  },
  {
    "objectID": "aipcs24/1kuhn_extrareadings.html#indifference-in-poker",
    "href": "aipcs24/1kuhn_extrareadings.html#indifference-in-poker",
    "title": "#1: Kuhn Poker | Extra Readings",
    "section": "Indifference in Poker",
    "text": "Indifference in Poker\nBack to poker. We can apply this indifference principle in computing equilibrium strategies in poker. When you make your opponent indifferent, then you don’t give them any best play.\nImportant note: If you play an equilibrium strategy, opponents will only get penalized for playing hands outside of the set of hands in the mixed strategy equilibrium (also known as the support, or the set of pure strategies that are played with non-zero probability under the mixed strategy). If opponents are not playing equilibrium, though, then they open themselves up to exploitation.\nLet’s look at one particular situation in Kuhn Poker and work it out by hand. Suppose that you are Player 2 with card Q after a Check from Player 1.\n\n\n\n\n\n\n\nExpected Value Exercise\n\n\n\nWhat indifference is Player 2 trying to induce? Compute it.\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nMaking P1 indifferent between calling and folding with a K\nWe can work out Player 2’s betting strategy by calculating the indifference. Let \\(b\\) be the probability that P2 bets with a Q after P1 checks.\n\\[\\begin{equation}\n\\begin{split}\n\\mathbb{E}(\\text{P1 Check K then Fold to Bet}) &= 0 \\\\\n\\\\\n\n\\mathbb{E}(\\text{P1 Check K then Call Bet}) &= -1*\\Pr(\\text{P2 has A and Bets}) + 3*\\Pr(\\text{P2 has Q and Bets}) \\\\\n  &= -1*\\frac{1}{2} + 3*\\frac{1}{2}*b \\\\\n  &= -0.5 + 1.5*b\n\\end{split}\n\\end{equation}\\]\nSetting these equal:\n\\(0 = -0.5 + 1.5*b\\)\n\\(b = \\frac{1}{3}\\)\nTherefore in equilibrium, P2 should bet \\(\\frac{1}{3}\\) with Q after P1 checks.\n\n\n\n\n\n\n\n\n\nEquilibrium Mixed Strategy Change Exercise\n\n\n\nIf P2 bet \\(\\frac{2}{3}\\) instead of \\(\\frac{1}{3}\\) with Q after P1 checks and P1 is playing an equilibrium strategy, how would P2’s expected value change?\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nIt wouldn’t! As long as P1 doesn’t modify their equilibrium strategy, then P2 can mix his strategy (at mixing infosets) however he wants and have the same EV.\n\n\n\n\n\n\n\n\n\nBluff:Value Ratio Exercise\n\n\n\nGiven that P2 has bet after P1 checks and is playing the equilibrium strategy, what is the probability that they are bluffing?\n(Note: Including cases where you have an A, so Q bets are bluffs and A bets are value bets.)\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nP2 has Q and A each \\(\\frac{1}{2}\\) of the time.\nP2 is betting Q \\(\\frac{1}{3}\\) of the time (bluffing).\nP2 is betting A always (value betting).\nTherefore for every 3 times you have Q you will bet once and for every 3 times you have A you will bet 3 times. Out of the 4 bets, 1 of them is a bluff.\n\\(\\Pr(\\text{P2 Bluff after P1 Check}) = \\frac{1}{4}\\)"
  },
  {
    "objectID": "aipcs24/1kuhn_extrareadings.html#extremely-optional-exercises",
    "href": "aipcs24/1kuhn_extrareadings.html#extremely-optional-exercises",
    "title": "#1: Kuhn Poker | Extra Readings",
    "section": "Extremely Optional Exercises",
    "text": "Extremely Optional Exercises\n\nDeeper into Kuhn Math\n\nSolve the above indifference exercise (Player 2 with card Q after Player 1 checks) if the bet size was 2 instead of 1. Can you come up with a general equation for how often to bluff given a pot size and bet size? What about how often to call?\nAnalytically solve for an additional Kuhn infoset\nSelect a set of infosets in Kuhn Poker and formula a system of equations for how they affect each other\nAt equilibrium in Kuhn Poker, Player 1 should bet A 3 times the amount that they bet Q. Against a Nash opponent, a mixing mistake will not result in any EV loss at those infosets. But are there any effects down the game tree?\nIf we as P1 Check with K, with a large sample size, we can estimate how often our opponent will bluff with a Q. Over a sample of 1000 hands that we Check with K, we expect that 500 times they would have Q and 500 times they would have A. We expect that they would always bet with A and bet some percentage \\(q\\) of their Q hands. Write an equation for how often we should call in terms of \\(q\\). In what other cases could we write equations for how to optimally play against our opponent?\n\n\n\nCoding a Kuhn Solver\n\nCode a Kuhn solver\nCode a Kuhn solver for the 100-card version"
  },
  {
    "objectID": "aipcs24/cfr.html",
    "href": "aipcs24/cfr.html",
    "title": "Ultimate Guide to CFR",
    "section": "",
    "text": "As we think about solving larger games, we start to look at iterative algorithms.\nThe most popular method for iteratively solving poker games is the Counterfactual Regret Minimization (CFR) algorithm. CFR is an iterative algorithm developed in 2007 at the University of Alberta that converges to Nash equilibrium in two player zero-sum games.\nWhat is a counterfactual? Here’s an example:\nActual event: I didn’t bring an umbrella, and I got wet in the rain\nCounterfactual event: If I had brought an umbrella, I wouldn’t have gotten wet\n\n\n(But still slightly long)\nCFR is a self play algorithm that learns by playing against itself repeatedly. It starts play with a uniform random strategy (each action at each decision point is equally likely) and iterates on these strategies to nudge closer to the game theory optimal Nash equilibrium strategy as the self play continues. The Nash equilibrium strategy is a “defensive” strategy that can’t be beaten, but also doesn’t take advantage of opponents. The counterfactual part comes from computing values based on assuming that our opponent plays to certain game states.\n\n\n\nKuhn Poker Tree from Alberta\n\n\nAt each information set in the game tree, the algorithm keeps a counter of regret values for each possible action. The regret means how much better the agent would have done if it had always played that action rather than the actual strategy that could be a mixture of actions. Positive regret means we should have taken that action more and negative regret means we would have done better by not taking that action.\nThe values computed with the regret are called counterfactual values, which are the value of playing an action at a certain game state weighted by the probability (counterfactual assumption) of the other agent playing to that game state.\nFor example, if the agent was playing a game in which it had 5 action options at a certain game state and Action 1 had a value of 3 while the game state average over all 5 actions was 1, then the regret would be 3-1 = 2. This means that Action 1 was better than average and we should favor taking that action more.\nThe CFR algorithm updates the strategy after each iteration to play in proportion to the regrets, meaning that if an action did well in the past, the agent would be more likely to play it in the future.\nThe final Nash equilibrium strategy is the average strategy over each iteration. This strategy cannot lose in expectation and is considered optimal since it’s theoretically robust and neither player would have incentive to change strategies if both playing an equilibrium. This is what we mean when we say “solve” a poker game.\nMichael Johanson, one of the authors on the original paper, gave his intuitive explanation of CFR in a post on Quora.\n\n\nA strategy at an infoset is a probability distribution over each possible action.\nRegret is a measure of how much each strategy at an infoset is preferred and is used as a way to update strategies.\nFor a given P1 strategy and P2 strategy, a player has regret when they take an action at an infoset that was not the highest-EV action at that infoset.\n\n\n\n\n\n\nRegret Exercise\n\n\n\n\nWhat is the regret for each action?\n\n\n\nAction\nRegret\n\n\n\n\nA\n\n\n\nB\n\n\n\nC\n\n\n\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\n\nAction\nRegret\n\n\n\n\nA\n4\n\n\nB\n2\n\n\nC\n0\n\n\n\n\n\n\n\n\n\n\n\n\nExpected Value Exercise\n\n\n\n\nIf taking a uniform strategy at this node (i.e. \\(\\frac{1}{3}\\) for each action), then what is the expected value of the node?\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\\(\\mathbb{E} = \\frac{1}{3}*1 + \\frac{1}{3}*3+\\frac{1}{3}*5 = 0.33+1+1.67 = 3\\)\n\n\n\n\n\n\n\n\n\nPoker Regret Exercise\n\n\n\n\nIn poker games, the regret for each action is defined as the value for that action minus the expected value of the node. Give the regret values for each action under this definition.\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\n\nAction\nValue\nPoker Regret\n\n\n\n\nA\n1\n-2\n\n\nB\n3\n0\n\n\nC\n5\n2\n\n\n\nAt a node in a poker game, the player prefers actions with higher regrets by this definition.\n\n\n\nEach infoset maintains a strategy and regret tabular counter for each action. These accumulate the sum of all strategies and the sum of all regrets.\nIn a game like Rock Paper Scissors, there is effectively only one infoset, so only one table for strategy over each action (Rock, Paper, Scissors) and one table for regret over each action (Rock, Paper, Scissors).\nRegrets are linked to strategies through a policy called regret matching.\n\n\n\n\n\n\n\n\n\nRPS Regret Details\n\n\n\n\n\nIn general, we define regret as:\n\\(\\text{Regret} = u(\\text{Alternative Strategy}) − u(\\text{Current Strategy})\\)\nWe prefer alternative actions with high regret and wish to minimize our overall regret.\nWe play Rock and opponent plays Paper \\(\\implies \\text{u(rock,paper)} = -1\\)\n\\(\\text{Regret(scissors)} = \\text{u(scissors,paper)} - \\text{u(rock,paper)} = 1-(-1) = 2\\)\n\\(\\text{Regret(paper)} = \\text{u(paper,paper)} - \\text{u(rock,paper)} = 0-(-1) = 1\\)\n\\(\\text{Regret(rock)} = \\text{u(rock,paper)} - \\text{u(rock,paper)} = -1-(-1) = 0\\)\nWe play Scissors and opponent plays Paper \\(\\implies \\text{u(scissors,paper)} = 1\\)\n\\(\\text{Regret(scissors)} = \\text{u(scissors,paper)} - \\text{u(scissors,paper)} = 1-1 = 0\\)\n\\(\\text{Regret(paper)} = \\text{u(paper,paper)} - \\text{u(scissors,paper)} = 0-1 = -1\\)\n\\(\\text{Regret(rock)} = \\text{u(rock,paper)} - \\text{u(scissors,paper)} = -1-1 = -2\\)\nWe play Paper and opponent plays Paper \\(\\implies \\text{u(paper,paper)} = 0\\)\n\\(\\text{Regret(scissors)} = \\text{u(scissors,paper)} - \\text{u(paper,paper)} = 1-0 = 1\\)\n\\(\\text{Regret(paper)} = \\text{u(paper,paper)} - \\text{u(paper,paper)} = 0-0 = 0\\)$\n\\(\\text{Regret(rock)} = \\text{u(rock,paper)} - \\text{u(paper,paper)} = -1-0 = -1\\)\nTo generalize:\n\nThe action played always gets a regret of 0 since the “alternative” is really just that same action\nWhen we play a tying action, the alternative losing action gets a regret of -1 and the alternative winning action gets a regret of +1\nWhen we play a winning action, the alternative tying action gets a regret of -1 and the alternative losing action gets a regret of -2\nWhen we play a losing action, the alternative winning action gets a regret of +2 and the alternative tying action gets a regret of +1\n\nAfter each play, we accumulate regrets for each of the 3 actions.\n\n\n\nWe decide our strategy probability distribution using regret matching, which means playing a strategy that normalizes over the positive accumulated regrets, i.e. playing in proportion to the positive regrets.\nExample from Marc Lanctot’s CFR Tutorial:\n\nGame 1: Choose Rock and opponent chooses Paper\n\nLose 1\nRock: Regret 0\nPaper: Regret 1\nScissors: Regret 2\n\nNext Action: Proportional \\[\n\\begin{pmatrix}\n\\text{Rock} & 0/3 = 0 \\\\\n\\text{Paper} & 1/3 = 0.333 \\\\\n\\text{Scissors} & 2/3 = 0.667\n\\end{pmatrix}\n\\]\nGame 2: Choose Scissors (With probability \\(2/3\\)) and opponent chooses Rock\n\nLose 1\nRock: Regret 1\nPaper: Regret 2\nScissors: Regret 0\n\nCumulative regrets:\n\nRock: 1\nPaper: 3\nScissors: 2\n\nNext Action: Proportional \\[\n\\begin{pmatrix}\n\\text{Rock} & 1/6 = 0167 \\\\\n\\text{Paper} & 3/6 = 0.500 \\\\\n\\text{Scissors} & 2/6 = 0.333\n\\end{pmatrix}\n\\]\n\nRegret matching definitions:\n\n\\(a\\) is actions\n\\(\\sigma\\) is strategy\n\\(t\\) is time\n\\(i\\) is player\n\\(R\\) is cumulative regret\n\n\\[\n\\sigma_i^t(a) = \\begin{cases}\n\\frac{\\max(R_i^t(a), 0)}{\\sum_{a' \\in A} \\max(R_i^t(a'), 0)} & \\text{if } \\sum_{a' \\in A} \\max(R_i^t(a'), 0) &gt; 0 \\\\\n\\frac{1}{|A|} & \\text{otherwise}\n\\end{cases}\n\\]\nThis is showing that we take the cumulative regret for an action divided by the cumulative regrets for all actions (normalizing) and then play that strategy for this action on the next iteration.\nIf all cumulative regrets are \\(\\leq 0\\) then we use the uniform distribution.\nIf cumulative regrets are positive, but are are \\(&lt;0\\) for a specific action, then we use \\(0\\) for that action.\nIn code:\n    def get_strategy(self):\n  #First find the normalizing sum\n        normalizing_sum = 0\n        for a in range(NUM_ACTIONS):\n            if self.regret_sum[a] &gt; 0:\n                self.strategy[a] = self.regret_sum[a]\n            else:\n                self.strategy[a] = 0\n            normalizing_sum += self.strategy[a]\n\n    #Then normalize each action\n        for a in range(NUM_ACTIONS):\n            if normalizing_sum &gt; 0:\n                self.strategy[a] /= normalizing_sum\n            else:\n                self.strategy[a] = 1.0/NUM_ACTIONS\n            self.strategy_sum[a] += self.strategy[a]\n\n        return self.strategy\nAfter using regret matching and after many iterations, we can minimize expected regret by using the average strategy at the end, which is the strategy that converges to equilibrium.\nIf two players were training against each other using regret matching, they would converge to the Nash Equilibrium of \\(1/3\\) for each action using the average strategy in Rock Paper Scissors.\n\n\n\nHere we show that regret matching converges only using the average strategy over 10,000 iterations:\n\nThe bottom shows both players converging to \\(1/3\\), while the top shows Player 1’s volatile current strategies that are cycling around.\nSuppose that your opponent Player 2 is playing 40% Rock, 30% Paper, and 30% Scissors. Here is a regret matching 10,000 game experiment. It shows that it takes around 1,600 games before Player 1 plays only Paper (this will vary).\n\nWe see that if there is a fixed player, regret matching converges to the best strategy.\nBut what if your opponent is not using a fixed strategy? We’ll talk about that soon.\n\n\n\nThe core feature of the iterative algorithms is self-play by traversing the game tree over all infosets and tracking the strategies and regrets at each.\nFrom above, we know how to find the strategy and regret in the simple Rock Paper Scissors environment.\nIn poker:\n\nStrategies are determined the same as above, through regret matching from the previous regret values at the specific information set for each action\nCFR definitions:\n\n\\(a\\) is actions\n\\(I\\) is infoset\n\\(\\sigma\\) is strategy\n\\(t\\) is time\n\\(i\\) is player\n\\(R\\) is cumulative regret\n\\(z\\) is a terminal node\n\\(u\\) is utility (payoffs)\n\\(p\\) is the current player who plays at this node\n\\(-p\\) is the the opponent player and chance\n\\(v\\) is counterfactual value\n\nCounterfactual values are effectively the value of an information set. They are weighted by the probability of opponent and chance playing to this node (in other words, the probability of playing to this node if this player tried to do so).\n\nCounterfactual value: \\(v^\\sigma (I) = \\sum_{z\\in Z_I} \\pi^{\\sigma}_{-p}(z[I])\\pi^{\\sigma}(z[I] \\rightarrow z)u_p(z)\\)\n\\(\\sum_{z\\in Z_I}\\) is summing over all terminal histories reachable from this node\n\\(\\pi^{\\sigma}_{-p}(z[I])\\) is the probability of opponents and chance reaching this node\n\\(\\pi^{\\sigma}(z[I] \\rightarrow z)\\) is the probability of playing from this node to terminal history \\(z\\), i.e. the weight component of the expected value\n\\(u_p(z)\\) is the utility at terminal history \\(z\\), i.e. the value component of the expected value\n\nInstantaneous regrets are based on action values compared to infoset EV. Each action EV then adds to its regret counter:\n\n\\(r^t(I,a) = v^{\\sigma^t}(I,a) - v^{\\sigma^t}(I)\\)\n\nCumulative (counterfactual) regrets are the sum of the individual regrets:\n\n\\(R^T(I,a) = \\sum_{t=1}^T r^t(I,a)\\)\n\n\n\n\n\n\nCFR+ variation such that regrets can’t be \\(\\leq 0\\)\nLinear CFR such that regrets are weighted by their recency\nSampling\n\nExternal: Sample chance and opponent nodes\nChance: Sample chance only\nOutcome: Sample outcomes\n\n\nThe Counterfactual Regret Minimization (CFR) algorithm was first published in a 2007 paper from the University of Alberta by Martin Zinkevich et al. called “Regret Minimization in Games with Incomplete Information”. Counterfactual means “relating to or expressing what has not happened or is not the case”.\n\n\n\n\nDue to the constraints of solving imperfect information games with MCTS and the memory limits of solving games with linear programs, CFR was developed as a novel solution. CFR also benefits from being computationally cheap and doesn’t require parameter tuning. It is an iterative Nash equilibrium approximation method that works through the process of repeated self-play between two regret minimizing agents.\nCFR is an extension of regret minimization into sequential games, where players play a sequence of actions to reach a terminal game state. Instead of storing and minimizing regret for the exponential number of strategies, CFR stores and minimizes a regret for each information set and its actions, which can be used to form an upper bound on the regret for any deterministic strategy. This means that we must also consider the probabilities of reaching each information set given the players’ strategies, as well as passing forward game state information and probabilities of player actions, and passing backward utility information through the game information states. The algorithm stores a strategy and regret value for each action at each node, such that the space requirement is on the order O(|I|), where |I| is the number of information sets in the game.\nCFR is an offline self-play algorithm, as it learns to play by repeatedly playing against itself. It begins with a strategy that is completely uniformly random and adjusts the strategy each iteration using regret matching such that the strategy at each node is proportional to the regrets for each action. The regrets are, as explained previously, measures of how the current strategy would have performed compared to a fixed strategy of always taking one particular action. Positive regret means that we would have done better by taking that action more often and negative regret means that we would have done better by not taking that action at all. The average strategy is then shown to approach a Nash equilibrium in the long run.\nIn the vanilla CFR algorithm, each iteration involves passing through every node in the extensive form of the game. Each pass evaluates strategies for both players by using regret matching, based on the prior cumulative regrets at each player’s information sets. Before looking at the CFR equations, we will refresh some definitions that were given in previous sections here when they are relevant to the forthcoming equations.\nLet A denote the set of all game actions. We refer to a strategy profile that excludes player i’s strategy as sigma_(-i). A history h is a sequence of actions, including chance outcomes, starting from the root of the game. Let pi^(sigma)(h) be the reach probability of game history h with strategy profile sigma and pi^sigma(h,z) be the reach probability that begins at h and ends at z.\nLet Z denote the set of all terminal game histories and then we have h ⊏ z for z ∈ Z is a nonterminal game history. Let u_i(z) denote the utility to player i of terminal history z.\nWe can now define the counterfactual value at nonterminal history h as follows: v_i(sigma, h) = sum (z in Z),h ⊏ z of pi^sigma_(-i) * pi^sigma(h,z) * u_i(z)\nThis is the expected utility to player i of reaching nonterminal history h and taking action a under the counterfactual assumption that player i takes actions to do so, but otherwise player i and all other players follow the strategy profile sigma.\nThe counterfactual value takes a player’s strategy and history and returns a value that is the product of the reach probability of the opponent (and chance) to arrive to that history and the expected value of the player for all possible terminal histories from that point. This is counterfactual because we ignore the probabilities that factually came into player i’s play to reach position h, which means that he is not biasing his future strategy with his current strategy. This weights the regrets by how often nature (factors outside the player’s control, including chance and opponents) reach this information state. This is intuitive because states that are more frequently played to by opponents are more important to play profitably.\nAn information set is a group of histories that a player cannot distinguish between. Let I denote an information set and let A(I) denote the set of legal actions for information set I. Let sigma_(I–&gt;a) denote a profile equivalent to sigma, except that action a is always chosen at information set I. The counterfactual regret of not taking action a at history h is defined as:\nr(h,a) = v_i(sigma_(i–&gt;a),h) - v_i(sigma, h)\nThis is the difference between the value when always selecting action a at the history node and the value of the history node itself (which will be defined in more detail shortly).\nLet pi^sigma(I) be the probability of reaching information set I through all possible game histories in I. Therefore we have that pi^sigma(I) = sum h∈I pi^sigma(h). The counterfactual reach probability of information state I, p^sigma_(-i)(I), is the probability of reaching I with strategy profile sigma except that, we treat current player I actions to reach the state as having probability 1.\nThe counterfactual regret of not taking action a at information set I is: r(I,a) = sum h∈I r(h,a)\nThis calculation simply includes all histories in the information set.\nLet t and T denote time steps, where t is with respect to each fixed information set and is incremented with each visit to an information set. A strategy sigma^t_i for player i maps each player i information set I_i and legal player i action a∈A(I_i) to the probability that the player will choose a in I_i at time t. All player strategies together at time t form a strategy profile sigma^t, to be detailed shortly.\nIf we define r^t_i(I,a) as the regret when players use sigma_t of not taking action a at information set I belonging to player i, then we can define the cumulative counterfactual regret as follows, which is the summation over all time steps:\nR^T_i(I,a) = sum t=1 to T r^t_i(I,a)\nIn recent years, researchers have redefined the counterfactual value in terms of information sets. This formulation shows the counterfactual value for a particular information set and action, given a player and his strategy:\nv^sigma_i(I,a) = sum h∈I sum z∈Z: h⊏z u_i(z)pisigma_(-i)(z)pisigma:I–&gt;a _i(h,z)\nWe see that this is similar to the first equation for the counterfactual value, but has some differences. Because we are now calculating the value for an information set, we must sum over all of the relevant histories. The inner summation adds all possible leaf nodes that can be reached from the current history (same as the original one) and the outer summation adds all histories that are part of the current information set.\nFrom left to right, the three terms on the right hand side represent the main player’s utility at the leaf node z, the opponent and chance combined reach probability for the leaf node z, and the reach probability of the main player to go from the current history to the leaf node z, while always taking action a. The differences between this formulation and that of the original equation will be reconciled with the next equation.\nThe counterfactual regret of player i for action a at information set I can be written as follows:\nR^T_i(I,a) = sum t=1,T vsigmat _i(I,a) - sum t=1,T sum a’∈A vsigmaT _i (I,a’)sigma^t_i(a’\\|I)\nThis formulation combines the three equations, where one had introduced the cumulative summation, one added all histories in the information set, and one defined the counterfactual regret difference equation. The first part of the difference in the counterfactual regret equation computes this value for the given a value, while the second part computes the expected value of all other a value options at the information set.\nThe inner summation of this part of the equation is over all non-a strategies and the outer summation is over all times. The first term in the summations computes the counterfactual value for each non-a strategy and the second term multiplies the counterfactual value by the player’s probability of playing that particular strategy at the given information set.\nWe can show the regret-matching algorithm by first defining the nonnegative counterfactual regret as R^T,+ _i (I,a) = max(R^T _i(I,a),0). Now we can use the cumulative regrets to obtain the strategy decision for the next iteration using regret matching:\nCase 1 when sum a’∈A R^(t-1) _i (I,a’))^+ &gt; 0 then sigma^t _i(a\\|I) = (R^(t-1) _i (i,a))^+ / (sum a’∈A R^(t-1) _i (I,a’))^+)\nCase 2 otherwise then sigma^t _i(a\\|I) = 1/\\| A \\|\nThis regret matching formula calculates the action probabilities for each action at each information set in proportion to the positive cumulative regrets. First we check to see if the cumulative regrets at the previous time step are positive. If not, the strategy is set to be uniformly random, determined by the number of available actions. If it is, then the strategy is the ratio of the cumulative regret of the defined action over the sum of the cumulative regrets of all other actions.\nThe CFR algorithm works by taking these action probabilities and then producing the next state in the game and computing the utilities of each action recursively. Regrets are computed from the returned values and the value of playing to the current node is then computed and returned. Regrets are updated at the end of each iteration.\n\n\n\nCFR has been shown to eliminate all dominated strategies from its final average strategy solution.\nBy following regret matching, the following bound, showing that the counterfactual regret at each information set grows sublinearly with the number of iterations, is guaranteed, given that delta = maximum difference in leaf node utilities (|u_i(z) − u_i(z’)| ≤ delta for all i ∈ N and z,z’ ∈ Z), A = number of actions, T = iteration number.\nR^T _i_infoset(I,a) &lt;= delta * sqrt(\\| A \\| * T)\nWith a specific set of strategy profiles, we can define a player’s overall regret as: R^T i_overall = max sigma_i ∈ sum i (sum t=1 to T u_i(sigma_i, sigma^T -i)) - sum t=1 to T u_i(sigma)\nThis is the amount of extra utility that player i could have achieved in expectation if he had chosen the best fixed strategy in hindsight. Assuming perfect recall, this can be bounded by the per information set counterfactual regrets of CFR:\nR^T _i_overall &lt;= sum I∈I_i max a∈A R^T _i_infoset(I,a) &lt;= \\|I_i\\| * delta * sqrt(\\|A\\|*T)\nThe fact that minimizing regret at each information set results in minimizing overall regret is a key insight for why CFR works and since CFR indeed achieves sublinear regret, this means that it is a regret minimizing algorithm.\nIn a two-player zero-sum game with perfect recall, for R^t _i ≤ ε for all players, then the average strategy profile is known to be a 2ε Nash equilibrium. We can therefore use the regret minimizing properties of CFR to solve games like poker by computing average strategies as follows:\nsigmahat(a|I) = [sum t=1,T (sum h∈I pisigmat _i (h)) * sigma^t(a|I)] / [sum t=1,T (sum h∈I pisigmat _i (h)))]\nwhere sum t=1,T (sum h∈I pisigmat _i (h))) is each player’s contribution to the probability of reaching a history in information set I, and is therefore the weighting term on sigma^T _i.\nThe strategies are combined such that they select an action at an information set in proportion to that strategy’s probability of playing to reach that information set. We run the CFR algorithm for a sufficient number of iterations in order to reduce the ε sufficiently.\nIn the end, it is the average strategy profile that converges to Nash equilibrium. The best available guarantees for CFR require ~1/ε^2 iterations over the game tree to reach an ε-equilibrium, that is, strategies for players such that no player can be exploited by more than ε by any strategy.\nThe gradient-based algorithms, which match the optimal number of iterations needed, require only ~1/ε or ~log (1/ε) iterations. However, due to effective CFR sampling methods, quick approximate iterations can be used such that sampling CFR is still the preferred solution method.\n\n\n\n\nA good intuitive way to think about why at the end of running CFR, the average strategy is the Nash equilibrium rather than the final strategy being Nash equilibrium comes from looking at rock paper scissors.\nSuppose that our opponent is playing Rock too much, then CFR moves us towards playing 100% paper (moving towards the best response to their strategy, i.e. the goal of regret minimization). The current strategy can be mixed (and it starts off uniform random), but it gets updated to maximize exploiting opponents and tends to cycle between pure-ish strategies (assuming that we are playing against a real opponent and not using self-play).\nSo the algorithm moves us to 100% paper and then the opponent might move to 100% scissors and then we move to 100% rock, and so on! While the current strategy is making sharp bounces around the strategy space without stopping at equilibrium, the average strategy cycles in closer and closer to converging at equilibrium, which in rock paper scissors is playing each action a third of the time. Intuitively it makes sense that the average strategy would be more robust than just taking the final strategy, which could be at a strange point that clearly wouldn’t be an equilibrium.\nThat said, recent research teams have simply used the final strategy after many many iterations and have had good results, which saves a lot of memory and computation since all of the strategies throughout don’t need to be stored and averaged.\n\n\n\n\nVanilla CFR is the default CFR method where each pass is a full iteration of the game tree. This has been used less ever since the Monte Carlo sampling CFR methods began from Marc Lanctot’s research. Although sampling means a larger number of iterations are needed to reach near equilibrium levels, each iteration is generally much faster and overall calculation time is generally significantly reduced since regrets are updated very quickly, instead of only after every long iteration as is the case with vanilla CFR. Vanilla CFR works by passing forward probabilities and game states and passing back utility values and can solve games as large as about 10^12 game states.\n\n\n\nIn Monte Carlo CFR, some sampling method is used to sample to terminal state(s) of the game and then updates are applied only to the sample and not the full tree. Other than that, the updates occur the same way as they do in the standard algorithm.\nThe benefit of Monte Carlo CFR is that it uses sampling to make quicker updates to the game tree, even though these updates may be noisy, in practice they are much faster than the precise and slow updates of the vanilla implementation. Memory requirements remain the same as in vanilla CFR since the same information is being stored.\nIn practice, this means updating the counterfactual value equation with a 1/q(z) factor to define the probability of sampling a particular terminal history z. Also the summation is defined only for the information set when z is in the sample and passes through the information set I on the way to z. It can be shown that the sampled counterfactual values are equivalent in expectation to the vanilla version, meaning that MCCFR also will be guaranteed to converge to a Nash equilibrium.\nAn issue arises with sampling that when taking the average strategy, all nodes may not have been sampled equal times. This can be addressed in two ways, each of which has its own disadvantages: 1. Optimistic averaging: A counter is placed at each node and updates are weighted by the last time they were seen at this node, which requires\nextra memory, is only an approximation, and requires a final iteration\nsince some counters will be outdated.\n2. Stochastically weighted averaging: Increase the magnitude of each\nstrategy by sampling the strategy profile probability. This is unbiased, but results in large variance.\nWe touch on two of the most common sampling methods below, called external sampling and chance sampling. Outcome sampling is another method that is the most extreme possible sampling – it samples one action down the whole tree.\n\n\nExternal Sampling entails sampling the actions of the opponent and of chance only. This means that these samples are based on how likely the opponent’s plays are to occur, which is sensible, since then regret values corresponding to these plays are updated faster. We go into this in more detail in Section 4.4 regarding Deep CFR, but provide code here in Python for Kuhn Poker and Leduc Poker.\nKuhn External CFR code:\nimport numpy as np\nimport random\n\nclass Node:\n    def __init__(self, num_actions):\n        self.regret_sum = np.zeros(num_actions)\n        self.strategy = np.zeros(num_actions)\n        self.strategy_sum = np.zeros(num_actions)\n        self.num_actions = num_actions\n\n    def get_strategy(self):\n        normalizing_sum = 0\n        for a in range(self.num_actions):\n            if self.regret_sum[a] &gt; 0:\n                self.strategy[a] = self.regret_sum[a]\n            else:\n                self.strategy[a] = 0\n            normalizing_sum += self.strategy[a]\n\n        for a in range(self.num_actions):\n            if normalizing_sum &gt; 0:\n                self.strategy[a] /= normalizing_sum\n            else:\n                self.strategy[a] = 1.0/self.num_actions\n\n        return self.strategy\n\n    def get_average_strategy(self):\n        avg_strategy = np.zeros(self.num_actions)\n        normalizing_sum = 0\n        \n        for a in range(self.num_actions):\n            normalizing_sum += self.strategy_sum[a]\n        for a in range(self.num_actions):\n            if normalizing_sum &gt; 0:\n                avg_strategy[a] = self.strategy_sum[a] / normalizing_sum\n            else:\n                avg_strategy[a] = 1.0 / self.num_actions\n        \n        return avg_strategy\n\nclass KuhnCFR:\n    def __init__(self, iterations, decksize):\n        self.nbets = 2\n        self.iterations = iterations\n        self.decksize = decksize\n        self.cards = np.arange(decksize)\n        self.bet_options = 2\n        self.nodes = {}\n\n    def cfr_iterations_external(self):\n        util = np.zeros(2)\n        for t in range(1, self.iterations + 1): \n            for i in range(2):\n                random.shuffle(self.cards)\n                util[i] += self.external_cfr(self.cards[:2], [], 2, 0, i, t)\n                print(i, util[i])\n        print('Average game value: {}'.format(util[0]/(self.iterations)))\n        for i in sorted(self.nodes):\n            print(i, self.nodes[i].get_average_strategy())\n\n    def external_cfr(self, cards, history, pot, nodes_touched, traversing_player, t):\n        print('THIS IS ITERATION', t)\n        print(cards, history, pot)\n        plays = len(history)\n        acting_player = plays % 2\n        opponent_player = 1 - acting_player\n        if plays &gt;= 2:\n            if history[-1] == 0 and history[-2] == 1: #bet fold\n                if acting_player == traversing_player:\n                    return 1\n                else:\n                    return -1\n            if (history[-1] == 0 and history[-2] == 0) or (history[-1] == 1 and history[-2] == 1): #check check or bet call, go to showdown\n                if acting_player == traversing_player:\n                    if cards[acting_player] &gt; cards[opponent_player]:\n                        return pot/2 #profit\n                    else:\n                        return -pot/2\n                else:\n                    if cards[acting_player] &gt; cards[opponent_player]:\n                        return -pot/2\n                    else:\n                        return pot/2\n\n        infoset = str(cards[acting_player]) + str(history)\n        if infoset not in self.nodes:\n            self.nodes[infoset] = Node(self.bet_options)\n\n        nodes_touched += 1\n\n        if acting_player == traversing_player:\n            util = np.zeros(self.bet_options) #2 actions\n            node_util = 0\n            strategy = self.nodes[infoset].get_strategy()\n            for a in range(self.bet_options):\n                next_history = history + [a]\n                pot += a\n                util[a] = self.external_cfr(cards, next_history, pot, nodes_touched, traversing_player, t)\n                node_util += strategy[a] * util[a]\n\n            for a in range(self.bet_options):\n                regret = util[a] - node_util\n                self.nodes[infoset].regret_sum[a] += regret\n            return node_util\n\n        else: #acting_player != traversing_player\n            strategy = self.nodes[infoset].get_strategy()\n            util = 0\n            if random.random() &lt; strategy[0]:\n                next_history = history + [0]\n            else: \n                next_history = history + [1]\n                pot += 1\n            util = self.external_cfr(cards, next_history, pot, nodes_touched, traversing_player, t)\n            for a in range(self.bet_options):\n                self.nodes[infoset].strategy_sum[a] += strategy[a]\n            return util\n\nif __name__ == \"__main__\":\n    k = KuhnCFR(100000, 10)\n    k.cfr_iterations_external()\nThe Kuhn Poker External Sampling CFR code uses two main classes, the Node class to track information sets of the game and the KuhnCFR class to run the actual CFR function. The Node class stores the relevant variables (regret_sum, strategy, strategy_sum, and num_actions). The get_strategy function is called throughout CFR to compute the strategy using regret matching, while the get_average_strategy function is called only at the end of all of the iterations to produce the final Nash equilibrium strategy.\nIn the main KuhnCFR class, the cfr_iterations_external function runs the main parts of the algorithm, with a loop for the number of iterations and a loop for each player, after which the CFR function is called and the output is summed in a utility value for each player to compute the game value by the end of the game. The CFR function itself first determines who the acting player is, then does various checks to see whether the game state is terminal, and the value with respect to the traversing player is returned (passed up). The game is terminal either after bet fold (no showdown) or check check (showdown) or bet call (showdown).\nIn the case where the state was not terminal, we set the information set, which is defined as the acting player’s cards concatenated with the history. A new Node is created if the information set was not seen before.\nThe rest of the algorithm is split into two parts, depending on if the acting player is the traversing player or not. If it is the traversing player, then we compute the strategy using the get_strategy() function based on regret matching with the accumulated regrets at the node, then cycle through every possible bet option and recursively call the cfr function using the updated history and pot size based on the bet option taking place. This is then added to the accumulating node_util variable that computes the average value of the node (each action’s utility weighted by how often that action is played). Once again, we iterate through all of the bet options and now compute the regrets, which are derived as the utility of the action minus the node utility. These regrets are then added to the regret_sum of the node for that particular action. Finally, the node utility is returned.\nIf the acting player is not the traversing player, then we once again find the strategy using the get_strategy() function and now we sample a single action from that strategy. In Kuhn Poker this is simple because there are only two possible actions, so we just generate a random number from 0 to 1 and if it’s less than the probability of passing, then we take the pass action, otherwise we take the bet action and add 1 to the pot. We recursively call the CFR function using the updated next_history and pot and then update the strategy_sum for each strategy at this information set. Finally, we return the utility of the sampled action.\nLeduc External CFR code:\nimport numpy as np\nimport random\nfrom collections import defaultdict\n\nclass Node:\n    def __init__(self, bet_options):\n        self.num_actions = len(bet_options)\n        self.regret_sum = defaultdict(int)\n        self.strategy = defaultdict(int)\n        self.strategy_sum = defaultdict(int)\n        self.bet_options = bet_options\n\n    def get_strategy(self):\n        normalizing_sum = 0\n        for a in self.bet_options:\n            if self.regret_sum[a] &gt; 0:\n                self.strategy[a] = self.regret_sum[a]\n            else:\n                self.strategy[a] = 0\n            normalizing_sum += self.strategy[a]\n\n        for a in self.bet_options:\n            if normalizing_sum &gt; 0:\n                self.strategy[a] /= normalizing_sum\n            else:\n                self.strategy[a] = 1.0/self.num_actions\n\n        return self.strategy\n\n    def get_average_strategy(self):\n        avg_strategy = defaultdict(int)\n        normalizing_sum = 0\n        \n        for a in self.bet_options:\n            normalizing_sum += self.strategy_sum[a]\n        for a in self.bet_options:\n            if normalizing_sum &gt; 0:\n                avg_strategy[a] = self.strategy_sum[a] / normalizing_sum\n            else:\n                avg_strategy[a] = 1.0 / self.num_actions\n        \n        return avg_strategy\n\nclass LeducCFR:\n    def __init__(self, iterations, decksize, starting_stack):\n        #self.nbets = 2\n        self.iterations = iterations\n        self.decksize = decksize\n        self.bet_options = starting_stack\n        self.cards = sorted(np.concatenate((np.arange(decksize),np.arange(decksize))))\n        self.nodes = {}\n\n    def cfr_iterations_external(self):\n        util = np.zeros(2)\n        for t in range(1, self.iterations + 1): \n            for i in range(2):\n                    random.shuffle(self.cards)\n                    util[i] += self.external_cfr(self.cards[:3], [[], []], 0, 2, 0, i, t)\n        print('Average game value: {}'.format(util[0]/(self.iterations)))\n        \n        with open('leducnlstrat.txt', 'w+') as f:\n            for i in sorted(self.nodes):\n                f.write('{}, {}\\n'.format(i, self.nodes[i].get_average_strategy()))\n                print(i, self.nodes[i].get_average_strategy())\n\n    def winning_hand(self, cards):\n        if cards[0] == cards[2]:\n            return 0\n        elif cards[1] == cards[2]:\n            return 1\n        elif cards[0] &gt; cards[1]:\n            return 0\n        elif cards[1] &gt; cards[0]:\n            return 1\n        elif cards[1] == cards[0]:\n            return -1\n\n    def valid_bets(self, history, rd, acting_player):\n        if acting_player == 0:\n            acting_stack = int(19 - (np.sum(history[0][0::2]) + np.sum(history[1][0::2])))\n        elif acting_player == 1:\n            acting_stack = int(19 - (np.sum(history[0][1::2]) + np.sum(history[1][1::2])))\n\n\n        # print('VALID BETS CHECK HISTORY', history)\n        # print('VALID BETS CHECK ROUND', rd)\n        # print('VALID BETS CHECK ACTING STACK', acting_stack)\n        curr_history = history[rd]\n\n\n        if len(history[rd]) == 0:\n            # print('CASE LEN 0', [*np.arange(acting_stack+1)])\n            return [*np.arange(acting_stack+1)]\n\n        elif len(history[rd]) == 1:\n            min_raise = curr_history[0]*2\n            call_amount = curr_history[0]\n            if min_raise &gt; acting_stack:\n                if history[rd] == [acting_stack]:\n                    # print('CASE LEN 1', [0, acting_stack])\n                    return [0, acting_stack]\n                else:\n                    # print('CASE LEN 1', [0, call_amount, acting_stack])\n                    return [0, call_amount, acting_stack]\n            else:\n                if history[rd] == [0]:\n                    # print('CASE LEN 1', [*np.arange(min_raise, acting_stack+1)])\n                    return [*np.arange(min_raise, acting_stack+1)]\n                else:\n                    # print('CASE LEN 1', [0, call_amount, *np.arange(min_raise, acting_stack+1)])\n                    return [0, call_amount, *np.arange(min_raise, acting_stack+1)]\n\n        elif len(history[rd]) == 2:\n            min_raise = 2*(curr_history[1] - curr_history[0])\n            call_amount = curr_history[1] - curr_history[0]\n            if min_raise &gt; acting_stack:\n                if call_amount == acting_stack:\n                    # print('CASE LEN 2', [0, acting_stack])\n                    return [0, acting_stack]\n                else:\n                    # print('CASE LEN 2', [0, call_amount, acting_stack])\n                    return [0, call_amount, acting_stack]\n            else:\n                # print('CASE LEN 2', [0, call_amount, *np.arange(min_raise, acting_stack+1)])\n                return [0, call_amount, *np.arange(min_raise, acting_stack+1)]\n\n        elif len(history[rd]) == 3:\n            call_amount = np.abs(curr_history[1] - curr_history[2] - curr_history[0])\n            # print('CASE LEN 3', [0, call_amount])\n            return [0, call_amount] #final bet (4 maximum per rd)\n\n    def external_cfr(self, cards, history, rd, pot, nodes_touched, traversing_player, t):\n        if t % 1000 == 0 and t&gt;0:\n            print('THIS IS ITERATION', t)\n        plays = len(history[rd])\n        acting_player = plays % 2\n        # print('*************')\n        # print('HISTORY RD', history[rd])\n        # print('PLAYS', plays)\n\n        if plays &gt;= 2:\n            p0total = np.sum(history[rd][0::2])\n            p1total = np.sum(history[rd][1::2])\n            # print('P0 TOTAL', p0total)\n            # print('P1 TOTAL', p1total)\n            # print('ROUND BEG', rd)\n                \n            if p0total == p1total:\n                if rd == 0 and p0total != 19:\n                    rd = 1\n                    # print('ROUND TO 1')\n                else:\n                    # print('SHOWDOWN RETURN')\n                    winner = self.winning_hand(cards)\n                    if winner == -1:\n                        return 0\n                    elif traversing_player == winner:\n                        return pot/2\n                    elif traversing_player != winner:\n                        return -pot/2\n\n            elif history[rd][-1] == 0: #previous player folded\n                # print('FOLD RETURN')\n                if acting_player == 0 and acting_player == traversing_player:\n                    return p1total+1\n                elif acting_player == 0 and acting_player != traversing_player:\n                    return -(p1total +1)\n                elif acting_player == 1 and acting_player == traversing_player:\n                    return p0total+1\n                elif acting_player == 1 and acting_player != traversing_player:\n                    return -(p0total +1)\n        # print('ROUND AFTER', rd)\n        if rd == 0:\n            infoset = str(cards[acting_player]) + str(history)\n        elif rd == 1:\n            infoset = str(cards[acting_player]) + str(cards[2]) + str(history)\n\n        if acting_player == 0:\n            infoset_bets = self.valid_bets(history, rd, 0)\n        elif acting_player == 1:\n            infoset_bets = self.valid_bets(history, rd, 1)\n        # print('ROUND', rd)\n        # print('INFOSET BETS', infoset_bets)\n        if infoset not in self.nodes:\n            self.nodes[infoset] = Node(infoset_bets)\n\n        # print(self.nodes[infoset])\n        # print(infoset)\n\n        nodes_touched += 1\n\n        if acting_player == traversing_player:\n            util = defaultdict(int)\n            node_util = 0\n            strategy = self.nodes[infoset].get_strategy()\n            for a in infoset_bets:\n                if rd == 0:\n                    next_history = [history[0] + [a], history[1]]\n                elif rd == 1:\n                    next_history = [history[0], history[1] + [a]]\n                pot += a\n                util[a] = self.external_cfr(cards, next_history, rd, pot, nodes_touched, traversing_player, t)\n                node_util += strategy[a] * util[a]\n\n            for a in infoset_bets:\n                regret = util[a] - node_util\n                self.nodes[infoset].regret_sum[a] += regret\n            return node_util\n\n        else: #acting_player != traversing_player\n            strategy = self.nodes[infoset].get_strategy()\n            # print('STRATEGY', strategy)\n            dart = random.random()\n            # print('DART', dart)\n            strat_sum = 0\n            for a in strategy:\n                strat_sum += strategy[a]\n                if dart &lt; strat_sum:\n                    action = a\n                    break\n            # print('ACTION', action)\n            if rd == 0:\n                next_history = [history[0] + [action], history[1]]\n            elif rd == 1:\n                next_history = [history[0], history[1] + [action]]\n            pot += action\n            # if acting_player == 0:\n            #   p0stack -= action\n            # elif acting_player == 1:\n            #   p1stack -= action\n            # print('NEXT HISTORY2', next_history)\n            util = self.external_cfr(cards, next_history, rd, pot, nodes_touched, traversing_player, t)\n            for a in infoset_bets:\n                self.nodes[infoset].strategy_sum[a] += strategy[a]\n            return util\n\nif __name__ == \"__main__\":\n    k = LeducCFR(1000, 3, 20)\n    k.cfr_iterations_external()\n    # for i in range(20):\n    #   print(k.valid_bets([[i],[]], 0, 19))\n    #a = k.valid_bets([[4, 18],[]], 0, 15)\n    #print(a)\nThe External Sampling Leduc Poker CFR code works similarly to the Kuhn Poker, but has some additional complications resulting from Leduc Poker having a second betting round and in general having a slightly more complex hand structure whereby there are six cards in the deck and the first round is each player getting dealt a single card followed by a limit hold’em betting round capped at four bets, then potentially a second flop round where another community card is revealed and hand strengths can improve if the card is paired with the card on the board.\n\n\n\nThe Chance Sampling CFR variation selects a single chance node at the root of the tree. In poker, this is equivalent to selecting a specific dealing of the cards to both players. For example, in Kuhn poker where there are 3 cards and each player is dealt one of them, there are 6 combinations of possible dealings (KQ, KJ, QJ, QK, JK, JQ), each with equal probability. After this selection, CFR is run for all branches of the tree after this chance node. This is equivalent to using the non-sampled counterfactual values and ignoring chance in the counterfactual.\nHere are the steps for Chance Sampling: 1. Check to see if at a terminal node. If so, return the profit from the acting player’s perspective. 2. If not terminal, create or access an information set that is the card of the node’s acting player + the history up to this point. For example: qb. - Information set node call is set up with vectors for regret_sum, strategy, and strategy_sum 3. Get strategy vector of the acting player based on the normalized regret_sum at the node. We also pass int he reach probability of that player getting to this node so we can keep the strategy_sum vector (reach_prob * strategy[action]) 4. Iterate over the actions, update history, and make a recursive CFR call: - util[a] = -cfr(cards, next_history, p0 * strategy[a], p1) &lt;– Example for player 0 - Negative because the next node value will be in terms of the other player 5. Node utility is weighted sum of each strategy[a] * util[a] 6. Again iterate over each action to update regrets - Regret = util[a] - node_util - Update the regret_sum at the infoset node for the acting player to be the regret * the reach probability of the opponent (the counterfactual part of the regrets) 7. Return node_util\nNow we relate the steps to the algorithm below:\nBelow we show a figure of the MCCFR algorithm for Chance Sampling. The algorithm works by calling CFR for each player over T iterations (lines 32-37) given a single vector of cards of both players, history of plays, and each player’s reach probabilities. If the history h is terminal, then a utility value can be returned (lines 6-7). If this is the beginning of the game tree and a chance node, then a single outcome is sampled and CFR is recursively called again (lines 8- 10). If the node is neither a chance node or a terminal node, then for each action, CFR is recursively called with the new history and an updated reach probability (lines 15- 20). The weighted utilities of the actions is summed to find the node utility (line 21). On the iteration of the i player, regret and strategy sum values are stored for each action by adding the counterfactual regret (line 25) and the weighted strategy (line 26) to the previous values. The strategy values will be averaged at the end to find the Nash equilibrium strategy and the regret values are used with regret matching to find the next strategy (line 26).\nThe non-sampling Vanilla CFR would simply iterate over every chance outcome (every possible deal of the private cards) instead of sampling a single outcome on line 9.\nVanilla CFR has i iterations going through entire tree and Chance CFR has i iterations starting with a particular random deal of private cards. Each iteration updates nodes for both players. CFR returns utility of game state (initially called at root) from player 1’s perspective. The average of these over all the iterations from the root is the “game value”.\n\n\n\nChance Sampling Algorithm\n\n\nCode of the Vanilla version written in Java is available here.\nCode including a best response function for the Chance Sampling algorithm is available here in Java.\n\n\n\n\n\nHere we show two full iterations of Chance Sampled CFR where we assume that the chance node has selected P1 Queen and P2 King as the random draw and then iterates over the entire tree from there.\nFirst we show the initialization of the algorithm which has four information sets (the card + the history of actions). At each information set the regret sum is stored where the first number represents the accumulated regret for passing and the second number represents the accumulated regret for betting. The strategy column is the behavioral strategy at that information set node, based on using regret matching with the accumulated regrets. Finally, the strategy sum is what we average at the end to find the Nash equilibrium strategy.\n\n\n\nAlgorithm initialization\n\n\n\n\n\nIteration 1\n\n\nHere is the sequence of what the algorithm does in the first iteration:\nPlayer 1 plays p = 0.5 at node Q.\nPlayer 2 plays p = 0.5 at node Kp and gets utility of 1 for action p at node Kp.\nPlayer 2 plays b = 0.5 at node Kp.\nPlayer 1 plays p = 0.5 at node Qpb and gets utility of -1. Player 1 plays b = 0.5 at node Qpb and gets utility of -2. Node Qpb has overall utility of 0.5 * -1 + 0.5 * -2 = -1.5. Regret for playing p is -1 - (-1.5) = 0.5. Regret for playing b is -2 - (-1.5) = -0.5.\nRegret_sum updates are regret * p(opponent playing to node) so here we have regret_sum[p] += 0.5 * 0.5 = 0.25 and regret_sum[b] += -0.5 * 0.5 = -0.25.\nNode Qpb is valued at 1.5 for player 2 (opposite of what it was for player 1). Now from node Kp, player 2 had value 1 if playing p and value 1.5 if playing b, for a node_utility of 1.25. The regret for playing p is 1-1.25 = -0.25 and regret for playing b is 1.5-1.25 = 0.25.\nRegret_sum updates are regret_sum[p] += -0.25 * 0.5 = -0.125 and regret_sum[b] += 0.25 * 0.5 = 0.125.\nNode Kp is now valued at -1.25 for player 1 action p. Player 1 now takes action b = 0.5 from node Q. Then player 2 takes action p = 0.5 from node Kb and gets utility -1. Then player 2 takes action b = 0.5 from node Kb and gets utility 2. The node_util is 0.5. Regret for playing p is -1 - 0.5 = -1.5. Regret for playing b is 2 - 0.5 = 1.5.\nRegret_sum updates are regret_sum[p] += -1.5 * 0.5= -0.75 and regret_sum[b] += 1.5 * 0.5 = 0.75.\nNode Kb is now valued at -0.5 for player 1 action b. The node_util for node Q is 0.5 * -1.25 for action p and -0.5 * 0.5 for action b = -0.875. Regret for playing p is -1.25 - (-0.875) = -0.375 and regret for playing b is -0.5 - (-0.875) = 0.375. Regret_sum updates are regret_sum[p] += -0.375\nStrategy_sum updates are probabilities of the node player not including the opponent playing to that action. So after this iteration each node was updated to [0.5, 0.5] except for the bottom node Qpb, which is [0.25, 0.25] since reaching that node comes after playing p = 0.5 in node Q, so both are 0.5 * 0.5.\n\n\n\nAlgorithm before iteration 2\n\n\n\n\n\nIteration 2\n\n\nPlayer 1 plays p = 0 at node Q.\nPlayer 2 plays p = 0 at node Kp and gets utility of 1.\nPlayer 2 plays b = 0.5 at node Kp.\nPlayer 1 plays p = 0.5 at node Qpb and gets utility of -1. Player 1 plays b = 0.5 at node Qpb and gets utility of -2. Node Qpb has overall utility of 0.5 * -1 + 0.5 * -2 = -1.5. Regret for playing p is -1 - (-1.5) = 0.5. Regret for playing b is -2 - (-1.5) = -0.5.\nRegret_sum updates are regret * p(opponent playing to node) so here we have regret_sum[p] += 0.5 * 0.5 = 0.25 and regret_sum[b] += -0.5 * 0.5 = -0.25.\nNode Qpb is valued at 1.5 for player 2 (opposite of what it was for player 1). Now from node Kp, player 2 had value 1 if playing p and value 1.5 if playing b, for a node_utility of 1.25. The regret for playing p is 1-1.25 = -0.25 and regret for playing b is 1.5-1.25 = 0.25.\nRegret_sum updates are regret_sum[p] += -0.25 * 0.5 = -0.125 and regret_sum[b] += 0.25 * 0.5 = 0.125.\nNode Kp is now valued at -1.25 for player 1 action p. Player 1 now takes action b = 0.5 from node Q. Then player 2 takes action p = 0.5 from node Kb and gets utility -1. Then player 2 takes action b = 0.5 from node Kb and gets utility 2. The node_util is 0.5. Regret for playing p is -1 - 0.5 = -1.5. Regret for playing b is 2 - 0.5 = 1.5.\nRegret_sum updates are regret_sum[p] += -1.5 * 0.5= -0.75 and regret_sum[b] += 1.5 * 0.5 = 0.75.\nNode Kb is now valued at -0.5 for player 1 action b. The node_util for node Q is 0.5 * -1.25 for action p and -0.5 * 0.5 for action b = -0.875. Regret for playing p is -1.25 - (-0.875) = -0.375 and regret for playing b is -0.5 - (-0.875) = 0.375. Regret_sum updates are regret_sum[p] += -0.375\nStrategy_sum updates are probabilities of the node player not including the opponent playing to that action. So after this iteration each node was updated to [0.5, 0.5] except for the bottom node Qpb, which is [0.25, 0.25] since reaching that node comes after playing p = 0.5 in node Q, so both are 0.5 * 0.5.\n\n\nWe compared four CFR algorithms (Chance Sampling, External Sampling, Vanilla, and CFR+) in terms of exploitability vs. nodes touched (a measure of how long the algorithm has been running for) and then also look at two of those algorithms which are very similar, CFR and its recent update, CFR+, in terms of exploitability vs. time. Finally, we produce strategy charts that show a Nash equilibrium strategy for each player at all four stages of the game. CFR+ is explained more in the CFR Advances section, but in short resets regrets that have gone negative to 0 so they are more likely to have a chance to “rebound” in case they were actually good strategies that, for example, just got unlucky.\nThe simulations run for a set number of iterations and the regrets for all algorithms are updated after each iteration.\nAs the algorithms run, a best response function is called periodically, which iterates once through the game tree once for each player. The average of the best response values from each player is taken as the exploitability of the game at that point. All graphs show exploitability on the vertical axis on a log scale. CFR and CFR+ were run for 100,000 iterations and Chance and External Sampling were run for 10^9 iterations. Since the non-sampling algorithms require entire tree traversals for each iteration, they require far fewer iterations to reach the same number of nodes. The game value for all variants is -0.0566, as we have found in previous sections.\nWe examine nodes touched vs. exploitability for all four of our CFR algorithm types (Vanilla CFR vs. CFR+ vs. Chance Sampling vs. External Sampling) up to 4 * 10^9 nodes touched for each. Monte Carlo sampling methods require many more iterations than Vanilla CFR, while each iteration is relatively fast. Therefore, a nodes touched metric makes sense as a way of comparison.\n\n\n\nCFR Algs Compared\n\n\nWe can see that the sampled versions show a lower exploitability much faster than the Vanilla and CFR+ versions, although they are more erratic due to the sampling. While Chance Sampling is generally superior to External Sampling, they are quite close at the end of the experiment. Chance Sampling is the simplest algorithm, which may work in its favor since Kuhn Poker is also a very simple game. Vanilla CFR shows consistently lower exploitability than CFR+. Perhaps this is because CFR+ doesn’t allow regrets to become negative, it may then waste time on actions that would have gone negative.\nWe also experimented with time vs. exploitability. The algorithms and code for CFR+ and CFR are exactly the same except for how the regret is calculated and since this eliminates other sources of variability, we are able to reasonably compare CFR and CFR+ exploitability against time.\n\n\n\nCFR CFR+ Time vs. Exploitability\n\n\nThis graph shows that CFR+ takes significantly more time to complete its 100,000 iterations and yet is still at a higher exploitability. Since the only difference in the algorithms is that CFR+ does not allow regrets to become negative, this must be the cause of the additional calculation time needed.\n\n\n\n\nIn reinforcement learning, agents learn what actions to take in an environment based on the rewards they’ve seen in the past. This is very similar to how regret updates work in CFR – we can think of the regrets as advantage functions, which is the value of a certain action compared to the value of a state, and in fact this terminology has been seen in recent papers like the Deep CFR paper. We can also compare this to having independent multiarm bandits at each decision point, learning from all of them simultaneously."
  },
  {
    "objectID": "aipcs24/cfr.html#tldr-cfr-explanation",
    "href": "aipcs24/cfr.html#tldr-cfr-explanation",
    "title": "Ultimate Guide to CFR",
    "section": "",
    "text": "(But still slightly long)\nCFR is a self play algorithm that learns by playing against itself repeatedly. It starts play with a uniform random strategy (each action at each decision point is equally likely) and iterates on these strategies to nudge closer to the game theory optimal Nash equilibrium strategy as the self play continues. The Nash equilibrium strategy is a “defensive” strategy that can’t be beaten, but also doesn’t take advantage of opponents. The counterfactual part comes from computing values based on assuming that our opponent plays to certain game states.\n\n\n\nKuhn Poker Tree from Alberta\n\n\nAt each information set in the game tree, the algorithm keeps a counter of regret values for each possible action. The regret means how much better the agent would have done if it had always played that action rather than the actual strategy that could be a mixture of actions. Positive regret means we should have taken that action more and negative regret means we would have done better by not taking that action.\nThe values computed with the regret are called counterfactual values, which are the value of playing an action at a certain game state weighted by the probability (counterfactual assumption) of the other agent playing to that game state.\nFor example, if the agent was playing a game in which it had 5 action options at a certain game state and Action 1 had a value of 3 while the game state average over all 5 actions was 1, then the regret would be 3-1 = 2. This means that Action 1 was better than average and we should favor taking that action more.\nThe CFR algorithm updates the strategy after each iteration to play in proportion to the regrets, meaning that if an action did well in the past, the agent would be more likely to play it in the future.\nThe final Nash equilibrium strategy is the average strategy over each iteration. This strategy cannot lose in expectation and is considered optimal since it’s theoretically robust and neither player would have incentive to change strategies if both playing an equilibrium. This is what we mean when we say “solve” a poker game.\nMichael Johanson, one of the authors on the original paper, gave his intuitive explanation of CFR in a post on Quora.\n\n\nA strategy at an infoset is a probability distribution over each possible action.\nRegret is a measure of how much each strategy at an infoset is preferred and is used as a way to update strategies.\nFor a given P1 strategy and P2 strategy, a player has regret when they take an action at an infoset that was not the highest-EV action at that infoset.\n\n\n\n\n\n\nRegret Exercise\n\n\n\n\nWhat is the regret for each action?\n\n\n\nAction\nRegret\n\n\n\n\nA\n\n\n\nB\n\n\n\nC\n\n\n\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\n\nAction\nRegret\n\n\n\n\nA\n4\n\n\nB\n2\n\n\nC\n0\n\n\n\n\n\n\n\n\n\n\n\n\nExpected Value Exercise\n\n\n\n\nIf taking a uniform strategy at this node (i.e. \\(\\frac{1}{3}\\) for each action), then what is the expected value of the node?\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\\(\\mathbb{E} = \\frac{1}{3}*1 + \\frac{1}{3}*3+\\frac{1}{3}*5 = 0.33+1+1.67 = 3\\)\n\n\n\n\n\n\n\n\n\nPoker Regret Exercise\n\n\n\n\nIn poker games, the regret for each action is defined as the value for that action minus the expected value of the node. Give the regret values for each action under this definition.\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\n\nAction\nValue\nPoker Regret\n\n\n\n\nA\n1\n-2\n\n\nB\n3\n0\n\n\nC\n5\n2\n\n\n\nAt a node in a poker game, the player prefers actions with higher regrets by this definition.\n\n\n\nEach infoset maintains a strategy and regret tabular counter for each action. These accumulate the sum of all strategies and the sum of all regrets.\nIn a game like Rock Paper Scissors, there is effectively only one infoset, so only one table for strategy over each action (Rock, Paper, Scissors) and one table for regret over each action (Rock, Paper, Scissors).\nRegrets are linked to strategies through a policy called regret matching.\n\n\n\n\n\n\n\n\n\nRPS Regret Details\n\n\n\n\n\nIn general, we define regret as:\n\\(\\text{Regret} = u(\\text{Alternative Strategy}) − u(\\text{Current Strategy})\\)\nWe prefer alternative actions with high regret and wish to minimize our overall regret.\nWe play Rock and opponent plays Paper \\(\\implies \\text{u(rock,paper)} = -1\\)\n\\(\\text{Regret(scissors)} = \\text{u(scissors,paper)} - \\text{u(rock,paper)} = 1-(-1) = 2\\)\n\\(\\text{Regret(paper)} = \\text{u(paper,paper)} - \\text{u(rock,paper)} = 0-(-1) = 1\\)\n\\(\\text{Regret(rock)} = \\text{u(rock,paper)} - \\text{u(rock,paper)} = -1-(-1) = 0\\)\nWe play Scissors and opponent plays Paper \\(\\implies \\text{u(scissors,paper)} = 1\\)\n\\(\\text{Regret(scissors)} = \\text{u(scissors,paper)} - \\text{u(scissors,paper)} = 1-1 = 0\\)\n\\(\\text{Regret(paper)} = \\text{u(paper,paper)} - \\text{u(scissors,paper)} = 0-1 = -1\\)\n\\(\\text{Regret(rock)} = \\text{u(rock,paper)} - \\text{u(scissors,paper)} = -1-1 = -2\\)\nWe play Paper and opponent plays Paper \\(\\implies \\text{u(paper,paper)} = 0\\)\n\\(\\text{Regret(scissors)} = \\text{u(scissors,paper)} - \\text{u(paper,paper)} = 1-0 = 1\\)\n\\(\\text{Regret(paper)} = \\text{u(paper,paper)} - \\text{u(paper,paper)} = 0-0 = 0\\)$\n\\(\\text{Regret(rock)} = \\text{u(rock,paper)} - \\text{u(paper,paper)} = -1-0 = -1\\)\nTo generalize:\n\nThe action played always gets a regret of 0 since the “alternative” is really just that same action\nWhen we play a tying action, the alternative losing action gets a regret of -1 and the alternative winning action gets a regret of +1\nWhen we play a winning action, the alternative tying action gets a regret of -1 and the alternative losing action gets a regret of -2\nWhen we play a losing action, the alternative winning action gets a regret of +2 and the alternative tying action gets a regret of +1\n\nAfter each play, we accumulate regrets for each of the 3 actions.\n\n\n\nWe decide our strategy probability distribution using regret matching, which means playing a strategy that normalizes over the positive accumulated regrets, i.e. playing in proportion to the positive regrets.\nExample from Marc Lanctot’s CFR Tutorial:\n\nGame 1: Choose Rock and opponent chooses Paper\n\nLose 1\nRock: Regret 0\nPaper: Regret 1\nScissors: Regret 2\n\nNext Action: Proportional \\[\n\\begin{pmatrix}\n\\text{Rock} & 0/3 = 0 \\\\\n\\text{Paper} & 1/3 = 0.333 \\\\\n\\text{Scissors} & 2/3 = 0.667\n\\end{pmatrix}\n\\]\nGame 2: Choose Scissors (With probability \\(2/3\\)) and opponent chooses Rock\n\nLose 1\nRock: Regret 1\nPaper: Regret 2\nScissors: Regret 0\n\nCumulative regrets:\n\nRock: 1\nPaper: 3\nScissors: 2\n\nNext Action: Proportional \\[\n\\begin{pmatrix}\n\\text{Rock} & 1/6 = 0167 \\\\\n\\text{Paper} & 3/6 = 0.500 \\\\\n\\text{Scissors} & 2/6 = 0.333\n\\end{pmatrix}\n\\]\n\nRegret matching definitions:\n\n\\(a\\) is actions\n\\(\\sigma\\) is strategy\n\\(t\\) is time\n\\(i\\) is player\n\\(R\\) is cumulative regret\n\n\\[\n\\sigma_i^t(a) = \\begin{cases}\n\\frac{\\max(R_i^t(a), 0)}{\\sum_{a' \\in A} \\max(R_i^t(a'), 0)} & \\text{if } \\sum_{a' \\in A} \\max(R_i^t(a'), 0) &gt; 0 \\\\\n\\frac{1}{|A|} & \\text{otherwise}\n\\end{cases}\n\\]\nThis is showing that we take the cumulative regret for an action divided by the cumulative regrets for all actions (normalizing) and then play that strategy for this action on the next iteration.\nIf all cumulative regrets are \\(\\leq 0\\) then we use the uniform distribution.\nIf cumulative regrets are positive, but are are \\(&lt;0\\) for a specific action, then we use \\(0\\) for that action.\nIn code:\n    def get_strategy(self):\n  #First find the normalizing sum\n        normalizing_sum = 0\n        for a in range(NUM_ACTIONS):\n            if self.regret_sum[a] &gt; 0:\n                self.strategy[a] = self.regret_sum[a]\n            else:\n                self.strategy[a] = 0\n            normalizing_sum += self.strategy[a]\n\n    #Then normalize each action\n        for a in range(NUM_ACTIONS):\n            if normalizing_sum &gt; 0:\n                self.strategy[a] /= normalizing_sum\n            else:\n                self.strategy[a] = 1.0/NUM_ACTIONS\n            self.strategy_sum[a] += self.strategy[a]\n\n        return self.strategy\nAfter using regret matching and after many iterations, we can minimize expected regret by using the average strategy at the end, which is the strategy that converges to equilibrium.\nIf two players were training against each other using regret matching, they would converge to the Nash Equilibrium of \\(1/3\\) for each action using the average strategy in Rock Paper Scissors.\n\n\n\nHere we show that regret matching converges only using the average strategy over 10,000 iterations:\n\nThe bottom shows both players converging to \\(1/3\\), while the top shows Player 1’s volatile current strategies that are cycling around.\nSuppose that your opponent Player 2 is playing 40% Rock, 30% Paper, and 30% Scissors. Here is a regret matching 10,000 game experiment. It shows that it takes around 1,600 games before Player 1 plays only Paper (this will vary).\n\nWe see that if there is a fixed player, regret matching converges to the best strategy.\nBut what if your opponent is not using a fixed strategy? We’ll talk about that soon.\n\n\n\nThe core feature of the iterative algorithms is self-play by traversing the game tree over all infosets and tracking the strategies and regrets at each.\nFrom above, we know how to find the strategy and regret in the simple Rock Paper Scissors environment.\nIn poker:\n\nStrategies are determined the same as above, through regret matching from the previous regret values at the specific information set for each action\nCFR definitions:\n\n\\(a\\) is actions\n\\(I\\) is infoset\n\\(\\sigma\\) is strategy\n\\(t\\) is time\n\\(i\\) is player\n\\(R\\) is cumulative regret\n\\(z\\) is a terminal node\n\\(u\\) is utility (payoffs)\n\\(p\\) is the current player who plays at this node\n\\(-p\\) is the the opponent player and chance\n\\(v\\) is counterfactual value\n\nCounterfactual values are effectively the value of an information set. They are weighted by the probability of opponent and chance playing to this node (in other words, the probability of playing to this node if this player tried to do so).\n\nCounterfactual value: \\(v^\\sigma (I) = \\sum_{z\\in Z_I} \\pi^{\\sigma}_{-p}(z[I])\\pi^{\\sigma}(z[I] \\rightarrow z)u_p(z)\\)\n\\(\\sum_{z\\in Z_I}\\) is summing over all terminal histories reachable from this node\n\\(\\pi^{\\sigma}_{-p}(z[I])\\) is the probability of opponents and chance reaching this node\n\\(\\pi^{\\sigma}(z[I] \\rightarrow z)\\) is the probability of playing from this node to terminal history \\(z\\), i.e. the weight component of the expected value\n\\(u_p(z)\\) is the utility at terminal history \\(z\\), i.e. the value component of the expected value\n\nInstantaneous regrets are based on action values compared to infoset EV. Each action EV then adds to its regret counter:\n\n\\(r^t(I,a) = v^{\\sigma^t}(I,a) - v^{\\sigma^t}(I)\\)\n\nCumulative (counterfactual) regrets are the sum of the individual regrets:\n\n\\(R^T(I,a) = \\sum_{t=1}^T r^t(I,a)\\)\n\n\n\n\n\n\nCFR+ variation such that regrets can’t be \\(\\leq 0\\)\nLinear CFR such that regrets are weighted by their recency\nSampling\n\nExternal: Sample chance and opponent nodes\nChance: Sample chance only\nOutcome: Sample outcomes\n\n\nThe Counterfactual Regret Minimization (CFR) algorithm was first published in a 2007 paper from the University of Alberta by Martin Zinkevich et al. called “Regret Minimization in Games with Incomplete Information”. Counterfactual means “relating to or expressing what has not happened or is not the case”."
  },
  {
    "objectID": "aipcs24/cfr.html#the-algorithm",
    "href": "aipcs24/cfr.html#the-algorithm",
    "title": "Ultimate Guide to CFR",
    "section": "",
    "text": "Due to the constraints of solving imperfect information games with MCTS and the memory limits of solving games with linear programs, CFR was developed as a novel solution. CFR also benefits from being computationally cheap and doesn’t require parameter tuning. It is an iterative Nash equilibrium approximation method that works through the process of repeated self-play between two regret minimizing agents.\nCFR is an extension of regret minimization into sequential games, where players play a sequence of actions to reach a terminal game state. Instead of storing and minimizing regret for the exponential number of strategies, CFR stores and minimizes a regret for each information set and its actions, which can be used to form an upper bound on the regret for any deterministic strategy. This means that we must also consider the probabilities of reaching each information set given the players’ strategies, as well as passing forward game state information and probabilities of player actions, and passing backward utility information through the game information states. The algorithm stores a strategy and regret value for each action at each node, such that the space requirement is on the order O(|I|), where |I| is the number of information sets in the game.\nCFR is an offline self-play algorithm, as it learns to play by repeatedly playing against itself. It begins with a strategy that is completely uniformly random and adjusts the strategy each iteration using regret matching such that the strategy at each node is proportional to the regrets for each action. The regrets are, as explained previously, measures of how the current strategy would have performed compared to a fixed strategy of always taking one particular action. Positive regret means that we would have done better by taking that action more often and negative regret means that we would have done better by not taking that action at all. The average strategy is then shown to approach a Nash equilibrium in the long run.\nIn the vanilla CFR algorithm, each iteration involves passing through every node in the extensive form of the game. Each pass evaluates strategies for both players by using regret matching, based on the prior cumulative regrets at each player’s information sets. Before looking at the CFR equations, we will refresh some definitions that were given in previous sections here when they are relevant to the forthcoming equations.\nLet A denote the set of all game actions. We refer to a strategy profile that excludes player i’s strategy as sigma_(-i). A history h is a sequence of actions, including chance outcomes, starting from the root of the game. Let pi^(sigma)(h) be the reach probability of game history h with strategy profile sigma and pi^sigma(h,z) be the reach probability that begins at h and ends at z.\nLet Z denote the set of all terminal game histories and then we have h ⊏ z for z ∈ Z is a nonterminal game history. Let u_i(z) denote the utility to player i of terminal history z.\nWe can now define the counterfactual value at nonterminal history h as follows: v_i(sigma, h) = sum (z in Z),h ⊏ z of pi^sigma_(-i) * pi^sigma(h,z) * u_i(z)\nThis is the expected utility to player i of reaching nonterminal history h and taking action a under the counterfactual assumption that player i takes actions to do so, but otherwise player i and all other players follow the strategy profile sigma.\nThe counterfactual value takes a player’s strategy and history and returns a value that is the product of the reach probability of the opponent (and chance) to arrive to that history and the expected value of the player for all possible terminal histories from that point. This is counterfactual because we ignore the probabilities that factually came into player i’s play to reach position h, which means that he is not biasing his future strategy with his current strategy. This weights the regrets by how often nature (factors outside the player’s control, including chance and opponents) reach this information state. This is intuitive because states that are more frequently played to by opponents are more important to play profitably.\nAn information set is a group of histories that a player cannot distinguish between. Let I denote an information set and let A(I) denote the set of legal actions for information set I. Let sigma_(I–&gt;a) denote a profile equivalent to sigma, except that action a is always chosen at information set I. The counterfactual regret of not taking action a at history h is defined as:\nr(h,a) = v_i(sigma_(i–&gt;a),h) - v_i(sigma, h)\nThis is the difference between the value when always selecting action a at the history node and the value of the history node itself (which will be defined in more detail shortly).\nLet pi^sigma(I) be the probability of reaching information set I through all possible game histories in I. Therefore we have that pi^sigma(I) = sum h∈I pi^sigma(h). The counterfactual reach probability of information state I, p^sigma_(-i)(I), is the probability of reaching I with strategy profile sigma except that, we treat current player I actions to reach the state as having probability 1.\nThe counterfactual regret of not taking action a at information set I is: r(I,a) = sum h∈I r(h,a)\nThis calculation simply includes all histories in the information set.\nLet t and T denote time steps, where t is with respect to each fixed information set and is incremented with each visit to an information set. A strategy sigma^t_i for player i maps each player i information set I_i and legal player i action a∈A(I_i) to the probability that the player will choose a in I_i at time t. All player strategies together at time t form a strategy profile sigma^t, to be detailed shortly.\nIf we define r^t_i(I,a) as the regret when players use sigma_t of not taking action a at information set I belonging to player i, then we can define the cumulative counterfactual regret as follows, which is the summation over all time steps:\nR^T_i(I,a) = sum t=1 to T r^t_i(I,a)\nIn recent years, researchers have redefined the counterfactual value in terms of information sets. This formulation shows the counterfactual value for a particular information set and action, given a player and his strategy:\nv^sigma_i(I,a) = sum h∈I sum z∈Z: h⊏z u_i(z)pisigma_(-i)(z)pisigma:I–&gt;a _i(h,z)\nWe see that this is similar to the first equation for the counterfactual value, but has some differences. Because we are now calculating the value for an information set, we must sum over all of the relevant histories. The inner summation adds all possible leaf nodes that can be reached from the current history (same as the original one) and the outer summation adds all histories that are part of the current information set.\nFrom left to right, the three terms on the right hand side represent the main player’s utility at the leaf node z, the opponent and chance combined reach probability for the leaf node z, and the reach probability of the main player to go from the current history to the leaf node z, while always taking action a. The differences between this formulation and that of the original equation will be reconciled with the next equation.\nThe counterfactual regret of player i for action a at information set I can be written as follows:\nR^T_i(I,a) = sum t=1,T vsigmat _i(I,a) - sum t=1,T sum a’∈A vsigmaT _i (I,a’)sigma^t_i(a’\\|I)\nThis formulation combines the three equations, where one had introduced the cumulative summation, one added all histories in the information set, and one defined the counterfactual regret difference equation. The first part of the difference in the counterfactual regret equation computes this value for the given a value, while the second part computes the expected value of all other a value options at the information set.\nThe inner summation of this part of the equation is over all non-a strategies and the outer summation is over all times. The first term in the summations computes the counterfactual value for each non-a strategy and the second term multiplies the counterfactual value by the player’s probability of playing that particular strategy at the given information set.\nWe can show the regret-matching algorithm by first defining the nonnegative counterfactual regret as R^T,+ _i (I,a) = max(R^T _i(I,a),0). Now we can use the cumulative regrets to obtain the strategy decision for the next iteration using regret matching:\nCase 1 when sum a’∈A R^(t-1) _i (I,a’))^+ &gt; 0 then sigma^t _i(a\\|I) = (R^(t-1) _i (i,a))^+ / (sum a’∈A R^(t-1) _i (I,a’))^+)\nCase 2 otherwise then sigma^t _i(a\\|I) = 1/\\| A \\|\nThis regret matching formula calculates the action probabilities for each action at each information set in proportion to the positive cumulative regrets. First we check to see if the cumulative regrets at the previous time step are positive. If not, the strategy is set to be uniformly random, determined by the number of available actions. If it is, then the strategy is the ratio of the cumulative regret of the defined action over the sum of the cumulative regrets of all other actions.\nThe CFR algorithm works by taking these action probabilities and then producing the next state in the game and computing the utilities of each action recursively. Regrets are computed from the returned values and the value of playing to the current node is then computed and returned. Regrets are updated at the end of each iteration."
  },
  {
    "objectID": "aipcs24/cfr.html#regret-bounds-and-convergence-rates",
    "href": "aipcs24/cfr.html#regret-bounds-and-convergence-rates",
    "title": "Ultimate Guide to CFR",
    "section": "",
    "text": "CFR has been shown to eliminate all dominated strategies from its final average strategy solution.\nBy following regret matching, the following bound, showing that the counterfactual regret at each information set grows sublinearly with the number of iterations, is guaranteed, given that delta = maximum difference in leaf node utilities (|u_i(z) − u_i(z’)| ≤ delta for all i ∈ N and z,z’ ∈ Z), A = number of actions, T = iteration number.\nR^T _i_infoset(I,a) &lt;= delta * sqrt(\\| A \\| * T)\nWith a specific set of strategy profiles, we can define a player’s overall regret as: R^T i_overall = max sigma_i ∈ sum i (sum t=1 to T u_i(sigma_i, sigma^T -i)) - sum t=1 to T u_i(sigma)\nThis is the amount of extra utility that player i could have achieved in expectation if he had chosen the best fixed strategy in hindsight. Assuming perfect recall, this can be bounded by the per information set counterfactual regrets of CFR:\nR^T _i_overall &lt;= sum I∈I_i max a∈A R^T _i_infoset(I,a) &lt;= \\|I_i\\| * delta * sqrt(\\|A\\|*T)\nThe fact that minimizing regret at each information set results in minimizing overall regret is a key insight for why CFR works and since CFR indeed achieves sublinear regret, this means that it is a regret minimizing algorithm.\nIn a two-player zero-sum game with perfect recall, for R^t _i ≤ ε for all players, then the average strategy profile is known to be a 2ε Nash equilibrium. We can therefore use the regret minimizing properties of CFR to solve games like poker by computing average strategies as follows:\nsigmahat(a|I) = [sum t=1,T (sum h∈I pisigmat _i (h)) * sigma^t(a|I)] / [sum t=1,T (sum h∈I pisigmat _i (h)))]\nwhere sum t=1,T (sum h∈I pisigmat _i (h))) is each player’s contribution to the probability of reaching a history in information set I, and is therefore the weighting term on sigma^T _i.\nThe strategies are combined such that they select an action at an information set in proportion to that strategy’s probability of playing to reach that information set. We run the CFR algorithm for a sufficient number of iterations in order to reduce the ε sufficiently.\nIn the end, it is the average strategy profile that converges to Nash equilibrium. The best available guarantees for CFR require ~1/ε^2 iterations over the game tree to reach an ε-equilibrium, that is, strategies for players such that no player can be exploited by more than ε by any strategy.\nThe gradient-based algorithms, which match the optimal number of iterations needed, require only ~1/ε or ~log (1/ε) iterations. However, due to effective CFR sampling methods, quick approximate iterations can be used such that sampling CFR is still the preferred solution method.\n\n\n\n\nA good intuitive way to think about why at the end of running CFR, the average strategy is the Nash equilibrium rather than the final strategy being Nash equilibrium comes from looking at rock paper scissors.\nSuppose that our opponent is playing Rock too much, then CFR moves us towards playing 100% paper (moving towards the best response to their strategy, i.e. the goal of regret minimization). The current strategy can be mixed (and it starts off uniform random), but it gets updated to maximize exploiting opponents and tends to cycle between pure-ish strategies (assuming that we are playing against a real opponent and not using self-play).\nSo the algorithm moves us to 100% paper and then the opponent might move to 100% scissors and then we move to 100% rock, and so on! While the current strategy is making sharp bounces around the strategy space without stopping at equilibrium, the average strategy cycles in closer and closer to converging at equilibrium, which in rock paper scissors is playing each action a third of the time. Intuitively it makes sense that the average strategy would be more robust than just taking the final strategy, which could be at a strange point that clearly wouldn’t be an equilibrium.\nThat said, recent research teams have simply used the final strategy after many many iterations and have had good results, which saves a lot of memory and computation since all of the strategies throughout don’t need to be stored and averaged.\n\n\n\n\nVanilla CFR is the default CFR method where each pass is a full iteration of the game tree. This has been used less ever since the Monte Carlo sampling CFR methods began from Marc Lanctot’s research. Although sampling means a larger number of iterations are needed to reach near equilibrium levels, each iteration is generally much faster and overall calculation time is generally significantly reduced since regrets are updated very quickly, instead of only after every long iteration as is the case with vanilla CFR. Vanilla CFR works by passing forward probabilities and game states and passing back utility values and can solve games as large as about 10^12 game states.\n\n\n\nIn Monte Carlo CFR, some sampling method is used to sample to terminal state(s) of the game and then updates are applied only to the sample and not the full tree. Other than that, the updates occur the same way as they do in the standard algorithm.\nThe benefit of Monte Carlo CFR is that it uses sampling to make quicker updates to the game tree, even though these updates may be noisy, in practice they are much faster than the precise and slow updates of the vanilla implementation. Memory requirements remain the same as in vanilla CFR since the same information is being stored.\nIn practice, this means updating the counterfactual value equation with a 1/q(z) factor to define the probability of sampling a particular terminal history z. Also the summation is defined only for the information set when z is in the sample and passes through the information set I on the way to z. It can be shown that the sampled counterfactual values are equivalent in expectation to the vanilla version, meaning that MCCFR also will be guaranteed to converge to a Nash equilibrium.\nAn issue arises with sampling that when taking the average strategy, all nodes may not have been sampled equal times. This can be addressed in two ways, each of which has its own disadvantages: 1. Optimistic averaging: A counter is placed at each node and updates are weighted by the last time they were seen at this node, which requires\nextra memory, is only an approximation, and requires a final iteration\nsince some counters will be outdated.\n2. Stochastically weighted averaging: Increase the magnitude of each\nstrategy by sampling the strategy profile probability. This is unbiased, but results in large variance.\nWe touch on two of the most common sampling methods below, called external sampling and chance sampling. Outcome sampling is another method that is the most extreme possible sampling – it samples one action down the whole tree.\n\n\nExternal Sampling entails sampling the actions of the opponent and of chance only. This means that these samples are based on how likely the opponent’s plays are to occur, which is sensible, since then regret values corresponding to these plays are updated faster. We go into this in more detail in Section 4.4 regarding Deep CFR, but provide code here in Python for Kuhn Poker and Leduc Poker.\nKuhn External CFR code:\nimport numpy as np\nimport random\n\nclass Node:\n    def __init__(self, num_actions):\n        self.regret_sum = np.zeros(num_actions)\n        self.strategy = np.zeros(num_actions)\n        self.strategy_sum = np.zeros(num_actions)\n        self.num_actions = num_actions\n\n    def get_strategy(self):\n        normalizing_sum = 0\n        for a in range(self.num_actions):\n            if self.regret_sum[a] &gt; 0:\n                self.strategy[a] = self.regret_sum[a]\n            else:\n                self.strategy[a] = 0\n            normalizing_sum += self.strategy[a]\n\n        for a in range(self.num_actions):\n            if normalizing_sum &gt; 0:\n                self.strategy[a] /= normalizing_sum\n            else:\n                self.strategy[a] = 1.0/self.num_actions\n\n        return self.strategy\n\n    def get_average_strategy(self):\n        avg_strategy = np.zeros(self.num_actions)\n        normalizing_sum = 0\n        \n        for a in range(self.num_actions):\n            normalizing_sum += self.strategy_sum[a]\n        for a in range(self.num_actions):\n            if normalizing_sum &gt; 0:\n                avg_strategy[a] = self.strategy_sum[a] / normalizing_sum\n            else:\n                avg_strategy[a] = 1.0 / self.num_actions\n        \n        return avg_strategy\n\nclass KuhnCFR:\n    def __init__(self, iterations, decksize):\n        self.nbets = 2\n        self.iterations = iterations\n        self.decksize = decksize\n        self.cards = np.arange(decksize)\n        self.bet_options = 2\n        self.nodes = {}\n\n    def cfr_iterations_external(self):\n        util = np.zeros(2)\n        for t in range(1, self.iterations + 1): \n            for i in range(2):\n                random.shuffle(self.cards)\n                util[i] += self.external_cfr(self.cards[:2], [], 2, 0, i, t)\n                print(i, util[i])\n        print('Average game value: {}'.format(util[0]/(self.iterations)))\n        for i in sorted(self.nodes):\n            print(i, self.nodes[i].get_average_strategy())\n\n    def external_cfr(self, cards, history, pot, nodes_touched, traversing_player, t):\n        print('THIS IS ITERATION', t)\n        print(cards, history, pot)\n        plays = len(history)\n        acting_player = plays % 2\n        opponent_player = 1 - acting_player\n        if plays &gt;= 2:\n            if history[-1] == 0 and history[-2] == 1: #bet fold\n                if acting_player == traversing_player:\n                    return 1\n                else:\n                    return -1\n            if (history[-1] == 0 and history[-2] == 0) or (history[-1] == 1 and history[-2] == 1): #check check or bet call, go to showdown\n                if acting_player == traversing_player:\n                    if cards[acting_player] &gt; cards[opponent_player]:\n                        return pot/2 #profit\n                    else:\n                        return -pot/2\n                else:\n                    if cards[acting_player] &gt; cards[opponent_player]:\n                        return -pot/2\n                    else:\n                        return pot/2\n\n        infoset = str(cards[acting_player]) + str(history)\n        if infoset not in self.nodes:\n            self.nodes[infoset] = Node(self.bet_options)\n\n        nodes_touched += 1\n\n        if acting_player == traversing_player:\n            util = np.zeros(self.bet_options) #2 actions\n            node_util = 0\n            strategy = self.nodes[infoset].get_strategy()\n            for a in range(self.bet_options):\n                next_history = history + [a]\n                pot += a\n                util[a] = self.external_cfr(cards, next_history, pot, nodes_touched, traversing_player, t)\n                node_util += strategy[a] * util[a]\n\n            for a in range(self.bet_options):\n                regret = util[a] - node_util\n                self.nodes[infoset].regret_sum[a] += regret\n            return node_util\n\n        else: #acting_player != traversing_player\n            strategy = self.nodes[infoset].get_strategy()\n            util = 0\n            if random.random() &lt; strategy[0]:\n                next_history = history + [0]\n            else: \n                next_history = history + [1]\n                pot += 1\n            util = self.external_cfr(cards, next_history, pot, nodes_touched, traversing_player, t)\n            for a in range(self.bet_options):\n                self.nodes[infoset].strategy_sum[a] += strategy[a]\n            return util\n\nif __name__ == \"__main__\":\n    k = KuhnCFR(100000, 10)\n    k.cfr_iterations_external()\nThe Kuhn Poker External Sampling CFR code uses two main classes, the Node class to track information sets of the game and the KuhnCFR class to run the actual CFR function. The Node class stores the relevant variables (regret_sum, strategy, strategy_sum, and num_actions). The get_strategy function is called throughout CFR to compute the strategy using regret matching, while the get_average_strategy function is called only at the end of all of the iterations to produce the final Nash equilibrium strategy.\nIn the main KuhnCFR class, the cfr_iterations_external function runs the main parts of the algorithm, with a loop for the number of iterations and a loop for each player, after which the CFR function is called and the output is summed in a utility value for each player to compute the game value by the end of the game. The CFR function itself first determines who the acting player is, then does various checks to see whether the game state is terminal, and the value with respect to the traversing player is returned (passed up). The game is terminal either after bet fold (no showdown) or check check (showdown) or bet call (showdown).\nIn the case where the state was not terminal, we set the information set, which is defined as the acting player’s cards concatenated with the history. A new Node is created if the information set was not seen before.\nThe rest of the algorithm is split into two parts, depending on if the acting player is the traversing player or not. If it is the traversing player, then we compute the strategy using the get_strategy() function based on regret matching with the accumulated regrets at the node, then cycle through every possible bet option and recursively call the cfr function using the updated history and pot size based on the bet option taking place. This is then added to the accumulating node_util variable that computes the average value of the node (each action’s utility weighted by how often that action is played). Once again, we iterate through all of the bet options and now compute the regrets, which are derived as the utility of the action minus the node utility. These regrets are then added to the regret_sum of the node for that particular action. Finally, the node utility is returned.\nIf the acting player is not the traversing player, then we once again find the strategy using the get_strategy() function and now we sample a single action from that strategy. In Kuhn Poker this is simple because there are only two possible actions, so we just generate a random number from 0 to 1 and if it’s less than the probability of passing, then we take the pass action, otherwise we take the bet action and add 1 to the pot. We recursively call the CFR function using the updated next_history and pot and then update the strategy_sum for each strategy at this information set. Finally, we return the utility of the sampled action.\nLeduc External CFR code:\nimport numpy as np\nimport random\nfrom collections import defaultdict\n\nclass Node:\n    def __init__(self, bet_options):\n        self.num_actions = len(bet_options)\n        self.regret_sum = defaultdict(int)\n        self.strategy = defaultdict(int)\n        self.strategy_sum = defaultdict(int)\n        self.bet_options = bet_options\n\n    def get_strategy(self):\n        normalizing_sum = 0\n        for a in self.bet_options:\n            if self.regret_sum[a] &gt; 0:\n                self.strategy[a] = self.regret_sum[a]\n            else:\n                self.strategy[a] = 0\n            normalizing_sum += self.strategy[a]\n\n        for a in self.bet_options:\n            if normalizing_sum &gt; 0:\n                self.strategy[a] /= normalizing_sum\n            else:\n                self.strategy[a] = 1.0/self.num_actions\n\n        return self.strategy\n\n    def get_average_strategy(self):\n        avg_strategy = defaultdict(int)\n        normalizing_sum = 0\n        \n        for a in self.bet_options:\n            normalizing_sum += self.strategy_sum[a]\n        for a in self.bet_options:\n            if normalizing_sum &gt; 0:\n                avg_strategy[a] = self.strategy_sum[a] / normalizing_sum\n            else:\n                avg_strategy[a] = 1.0 / self.num_actions\n        \n        return avg_strategy\n\nclass LeducCFR:\n    def __init__(self, iterations, decksize, starting_stack):\n        #self.nbets = 2\n        self.iterations = iterations\n        self.decksize = decksize\n        self.bet_options = starting_stack\n        self.cards = sorted(np.concatenate((np.arange(decksize),np.arange(decksize))))\n        self.nodes = {}\n\n    def cfr_iterations_external(self):\n        util = np.zeros(2)\n        for t in range(1, self.iterations + 1): \n            for i in range(2):\n                    random.shuffle(self.cards)\n                    util[i] += self.external_cfr(self.cards[:3], [[], []], 0, 2, 0, i, t)\n        print('Average game value: {}'.format(util[0]/(self.iterations)))\n        \n        with open('leducnlstrat.txt', 'w+') as f:\n            for i in sorted(self.nodes):\n                f.write('{}, {}\\n'.format(i, self.nodes[i].get_average_strategy()))\n                print(i, self.nodes[i].get_average_strategy())\n\n    def winning_hand(self, cards):\n        if cards[0] == cards[2]:\n            return 0\n        elif cards[1] == cards[2]:\n            return 1\n        elif cards[0] &gt; cards[1]:\n            return 0\n        elif cards[1] &gt; cards[0]:\n            return 1\n        elif cards[1] == cards[0]:\n            return -1\n\n    def valid_bets(self, history, rd, acting_player):\n        if acting_player == 0:\n            acting_stack = int(19 - (np.sum(history[0][0::2]) + np.sum(history[1][0::2])))\n        elif acting_player == 1:\n            acting_stack = int(19 - (np.sum(history[0][1::2]) + np.sum(history[1][1::2])))\n\n\n        # print('VALID BETS CHECK HISTORY', history)\n        # print('VALID BETS CHECK ROUND', rd)\n        # print('VALID BETS CHECK ACTING STACK', acting_stack)\n        curr_history = history[rd]\n\n\n        if len(history[rd]) == 0:\n            # print('CASE LEN 0', [*np.arange(acting_stack+1)])\n            return [*np.arange(acting_stack+1)]\n\n        elif len(history[rd]) == 1:\n            min_raise = curr_history[0]*2\n            call_amount = curr_history[0]\n            if min_raise &gt; acting_stack:\n                if history[rd] == [acting_stack]:\n                    # print('CASE LEN 1', [0, acting_stack])\n                    return [0, acting_stack]\n                else:\n                    # print('CASE LEN 1', [0, call_amount, acting_stack])\n                    return [0, call_amount, acting_stack]\n            else:\n                if history[rd] == [0]:\n                    # print('CASE LEN 1', [*np.arange(min_raise, acting_stack+1)])\n                    return [*np.arange(min_raise, acting_stack+1)]\n                else:\n                    # print('CASE LEN 1', [0, call_amount, *np.arange(min_raise, acting_stack+1)])\n                    return [0, call_amount, *np.arange(min_raise, acting_stack+1)]\n\n        elif len(history[rd]) == 2:\n            min_raise = 2*(curr_history[1] - curr_history[0])\n            call_amount = curr_history[1] - curr_history[0]\n            if min_raise &gt; acting_stack:\n                if call_amount == acting_stack:\n                    # print('CASE LEN 2', [0, acting_stack])\n                    return [0, acting_stack]\n                else:\n                    # print('CASE LEN 2', [0, call_amount, acting_stack])\n                    return [0, call_amount, acting_stack]\n            else:\n                # print('CASE LEN 2', [0, call_amount, *np.arange(min_raise, acting_stack+1)])\n                return [0, call_amount, *np.arange(min_raise, acting_stack+1)]\n\n        elif len(history[rd]) == 3:\n            call_amount = np.abs(curr_history[1] - curr_history[2] - curr_history[0])\n            # print('CASE LEN 3', [0, call_amount])\n            return [0, call_amount] #final bet (4 maximum per rd)\n\n    def external_cfr(self, cards, history, rd, pot, nodes_touched, traversing_player, t):\n        if t % 1000 == 0 and t&gt;0:\n            print('THIS IS ITERATION', t)\n        plays = len(history[rd])\n        acting_player = plays % 2\n        # print('*************')\n        # print('HISTORY RD', history[rd])\n        # print('PLAYS', plays)\n\n        if plays &gt;= 2:\n            p0total = np.sum(history[rd][0::2])\n            p1total = np.sum(history[rd][1::2])\n            # print('P0 TOTAL', p0total)\n            # print('P1 TOTAL', p1total)\n            # print('ROUND BEG', rd)\n                \n            if p0total == p1total:\n                if rd == 0 and p0total != 19:\n                    rd = 1\n                    # print('ROUND TO 1')\n                else:\n                    # print('SHOWDOWN RETURN')\n                    winner = self.winning_hand(cards)\n                    if winner == -1:\n                        return 0\n                    elif traversing_player == winner:\n                        return pot/2\n                    elif traversing_player != winner:\n                        return -pot/2\n\n            elif history[rd][-1] == 0: #previous player folded\n                # print('FOLD RETURN')\n                if acting_player == 0 and acting_player == traversing_player:\n                    return p1total+1\n                elif acting_player == 0 and acting_player != traversing_player:\n                    return -(p1total +1)\n                elif acting_player == 1 and acting_player == traversing_player:\n                    return p0total+1\n                elif acting_player == 1 and acting_player != traversing_player:\n                    return -(p0total +1)\n        # print('ROUND AFTER', rd)\n        if rd == 0:\n            infoset = str(cards[acting_player]) + str(history)\n        elif rd == 1:\n            infoset = str(cards[acting_player]) + str(cards[2]) + str(history)\n\n        if acting_player == 0:\n            infoset_bets = self.valid_bets(history, rd, 0)\n        elif acting_player == 1:\n            infoset_bets = self.valid_bets(history, rd, 1)\n        # print('ROUND', rd)\n        # print('INFOSET BETS', infoset_bets)\n        if infoset not in self.nodes:\n            self.nodes[infoset] = Node(infoset_bets)\n\n        # print(self.nodes[infoset])\n        # print(infoset)\n\n        nodes_touched += 1\n\n        if acting_player == traversing_player:\n            util = defaultdict(int)\n            node_util = 0\n            strategy = self.nodes[infoset].get_strategy()\n            for a in infoset_bets:\n                if rd == 0:\n                    next_history = [history[0] + [a], history[1]]\n                elif rd == 1:\n                    next_history = [history[0], history[1] + [a]]\n                pot += a\n                util[a] = self.external_cfr(cards, next_history, rd, pot, nodes_touched, traversing_player, t)\n                node_util += strategy[a] * util[a]\n\n            for a in infoset_bets:\n                regret = util[a] - node_util\n                self.nodes[infoset].regret_sum[a] += regret\n            return node_util\n\n        else: #acting_player != traversing_player\n            strategy = self.nodes[infoset].get_strategy()\n            # print('STRATEGY', strategy)\n            dart = random.random()\n            # print('DART', dart)\n            strat_sum = 0\n            for a in strategy:\n                strat_sum += strategy[a]\n                if dart &lt; strat_sum:\n                    action = a\n                    break\n            # print('ACTION', action)\n            if rd == 0:\n                next_history = [history[0] + [action], history[1]]\n            elif rd == 1:\n                next_history = [history[0], history[1] + [action]]\n            pot += action\n            # if acting_player == 0:\n            #   p0stack -= action\n            # elif acting_player == 1:\n            #   p1stack -= action\n            # print('NEXT HISTORY2', next_history)\n            util = self.external_cfr(cards, next_history, rd, pot, nodes_touched, traversing_player, t)\n            for a in infoset_bets:\n                self.nodes[infoset].strategy_sum[a] += strategy[a]\n            return util\n\nif __name__ == \"__main__\":\n    k = LeducCFR(1000, 3, 20)\n    k.cfr_iterations_external()\n    # for i in range(20):\n    #   print(k.valid_bets([[i],[]], 0, 19))\n    #a = k.valid_bets([[4, 18],[]], 0, 15)\n    #print(a)\nThe External Sampling Leduc Poker CFR code works similarly to the Kuhn Poker, but has some additional complications resulting from Leduc Poker having a second betting round and in general having a slightly more complex hand structure whereby there are six cards in the deck and the first round is each player getting dealt a single card followed by a limit hold’em betting round capped at four bets, then potentially a second flop round where another community card is revealed and hand strengths can improve if the card is paired with the card on the board.\n\n\n\nThe Chance Sampling CFR variation selects a single chance node at the root of the tree. In poker, this is equivalent to selecting a specific dealing of the cards to both players. For example, in Kuhn poker where there are 3 cards and each player is dealt one of them, there are 6 combinations of possible dealings (KQ, KJ, QJ, QK, JK, JQ), each with equal probability. After this selection, CFR is run for all branches of the tree after this chance node. This is equivalent to using the non-sampled counterfactual values and ignoring chance in the counterfactual.\nHere are the steps for Chance Sampling: 1. Check to see if at a terminal node. If so, return the profit from the acting player’s perspective. 2. If not terminal, create or access an information set that is the card of the node’s acting player + the history up to this point. For example: qb. - Information set node call is set up with vectors for regret_sum, strategy, and strategy_sum 3. Get strategy vector of the acting player based on the normalized regret_sum at the node. We also pass int he reach probability of that player getting to this node so we can keep the strategy_sum vector (reach_prob * strategy[action]) 4. Iterate over the actions, update history, and make a recursive CFR call: - util[a] = -cfr(cards, next_history, p0 * strategy[a], p1) &lt;– Example for player 0 - Negative because the next node value will be in terms of the other player 5. Node utility is weighted sum of each strategy[a] * util[a] 6. Again iterate over each action to update regrets - Regret = util[a] - node_util - Update the regret_sum at the infoset node for the acting player to be the regret * the reach probability of the opponent (the counterfactual part of the regrets) 7. Return node_util\nNow we relate the steps to the algorithm below:\nBelow we show a figure of the MCCFR algorithm for Chance Sampling. The algorithm works by calling CFR for each player over T iterations (lines 32-37) given a single vector of cards of both players, history of plays, and each player’s reach probabilities. If the history h is terminal, then a utility value can be returned (lines 6-7). If this is the beginning of the game tree and a chance node, then a single outcome is sampled and CFR is recursively called again (lines 8- 10). If the node is neither a chance node or a terminal node, then for each action, CFR is recursively called with the new history and an updated reach probability (lines 15- 20). The weighted utilities of the actions is summed to find the node utility (line 21). On the iteration of the i player, regret and strategy sum values are stored for each action by adding the counterfactual regret (line 25) and the weighted strategy (line 26) to the previous values. The strategy values will be averaged at the end to find the Nash equilibrium strategy and the regret values are used with regret matching to find the next strategy (line 26).\nThe non-sampling Vanilla CFR would simply iterate over every chance outcome (every possible deal of the private cards) instead of sampling a single outcome on line 9.\nVanilla CFR has i iterations going through entire tree and Chance CFR has i iterations starting with a particular random deal of private cards. Each iteration updates nodes for both players. CFR returns utility of game state (initially called at root) from player 1’s perspective. The average of these over all the iterations from the root is the “game value”.\n\n\n\nChance Sampling Algorithm\n\n\nCode of the Vanilla version written in Java is available here.\nCode including a best response function for the Chance Sampling algorithm is available here in Java."
  },
  {
    "objectID": "aipcs24/cfr.html#going-through-an-iteration",
    "href": "aipcs24/cfr.html#going-through-an-iteration",
    "title": "Ultimate Guide to CFR",
    "section": "",
    "text": "Here we show two full iterations of Chance Sampled CFR where we assume that the chance node has selected P1 Queen and P2 King as the random draw and then iterates over the entire tree from there.\nFirst we show the initialization of the algorithm which has four information sets (the card + the history of actions). At each information set the regret sum is stored where the first number represents the accumulated regret for passing and the second number represents the accumulated regret for betting. The strategy column is the behavioral strategy at that information set node, based on using regret matching with the accumulated regrets. Finally, the strategy sum is what we average at the end to find the Nash equilibrium strategy.\n\n\n\nAlgorithm initialization\n\n\n\n\n\nIteration 1\n\n\nHere is the sequence of what the algorithm does in the first iteration:\nPlayer 1 plays p = 0.5 at node Q.\nPlayer 2 plays p = 0.5 at node Kp and gets utility of 1 for action p at node Kp.\nPlayer 2 plays b = 0.5 at node Kp.\nPlayer 1 plays p = 0.5 at node Qpb and gets utility of -1. Player 1 plays b = 0.5 at node Qpb and gets utility of -2. Node Qpb has overall utility of 0.5 * -1 + 0.5 * -2 = -1.5. Regret for playing p is -1 - (-1.5) = 0.5. Regret for playing b is -2 - (-1.5) = -0.5.\nRegret_sum updates are regret * p(opponent playing to node) so here we have regret_sum[p] += 0.5 * 0.5 = 0.25 and regret_sum[b] += -0.5 * 0.5 = -0.25.\nNode Qpb is valued at 1.5 for player 2 (opposite of what it was for player 1). Now from node Kp, player 2 had value 1 if playing p and value 1.5 if playing b, for a node_utility of 1.25. The regret for playing p is 1-1.25 = -0.25 and regret for playing b is 1.5-1.25 = 0.25.\nRegret_sum updates are regret_sum[p] += -0.25 * 0.5 = -0.125 and regret_sum[b] += 0.25 * 0.5 = 0.125.\nNode Kp is now valued at -1.25 for player 1 action p. Player 1 now takes action b = 0.5 from node Q. Then player 2 takes action p = 0.5 from node Kb and gets utility -1. Then player 2 takes action b = 0.5 from node Kb and gets utility 2. The node_util is 0.5. Regret for playing p is -1 - 0.5 = -1.5. Regret for playing b is 2 - 0.5 = 1.5.\nRegret_sum updates are regret_sum[p] += -1.5 * 0.5= -0.75 and regret_sum[b] += 1.5 * 0.5 = 0.75.\nNode Kb is now valued at -0.5 for player 1 action b. The node_util for node Q is 0.5 * -1.25 for action p and -0.5 * 0.5 for action b = -0.875. Regret for playing p is -1.25 - (-0.875) = -0.375 and regret for playing b is -0.5 - (-0.875) = 0.375. Regret_sum updates are regret_sum[p] += -0.375\nStrategy_sum updates are probabilities of the node player not including the opponent playing to that action. So after this iteration each node was updated to [0.5, 0.5] except for the bottom node Qpb, which is [0.25, 0.25] since reaching that node comes after playing p = 0.5 in node Q, so both are 0.5 * 0.5.\n\n\n\nAlgorithm before iteration 2\n\n\n\n\n\nIteration 2\n\n\nPlayer 1 plays p = 0 at node Q.\nPlayer 2 plays p = 0 at node Kp and gets utility of 1.\nPlayer 2 plays b = 0.5 at node Kp.\nPlayer 1 plays p = 0.5 at node Qpb and gets utility of -1. Player 1 plays b = 0.5 at node Qpb and gets utility of -2. Node Qpb has overall utility of 0.5 * -1 + 0.5 * -2 = -1.5. Regret for playing p is -1 - (-1.5) = 0.5. Regret for playing b is -2 - (-1.5) = -0.5.\nRegret_sum updates are regret * p(opponent playing to node) so here we have regret_sum[p] += 0.5 * 0.5 = 0.25 and regret_sum[b] += -0.5 * 0.5 = -0.25.\nNode Qpb is valued at 1.5 for player 2 (opposite of what it was for player 1). Now from node Kp, player 2 had value 1 if playing p and value 1.5 if playing b, for a node_utility of 1.25. The regret for playing p is 1-1.25 = -0.25 and regret for playing b is 1.5-1.25 = 0.25.\nRegret_sum updates are regret_sum[p] += -0.25 * 0.5 = -0.125 and regret_sum[b] += 0.25 * 0.5 = 0.125.\nNode Kp is now valued at -1.25 for player 1 action p. Player 1 now takes action b = 0.5 from node Q. Then player 2 takes action p = 0.5 from node Kb and gets utility -1. Then player 2 takes action b = 0.5 from node Kb and gets utility 2. The node_util is 0.5. Regret for playing p is -1 - 0.5 = -1.5. Regret for playing b is 2 - 0.5 = 1.5.\nRegret_sum updates are regret_sum[p] += -1.5 * 0.5= -0.75 and regret_sum[b] += 1.5 * 0.5 = 0.75.\nNode Kb is now valued at -0.5 for player 1 action b. The node_util for node Q is 0.5 * -1.25 for action p and -0.5 * 0.5 for action b = -0.875. Regret for playing p is -1.25 - (-0.875) = -0.375 and regret for playing b is -0.5 - (-0.875) = 0.375. Regret_sum updates are regret_sum[p] += -0.375\nStrategy_sum updates are probabilities of the node player not including the opponent playing to that action. So after this iteration each node was updated to [0.5, 0.5] except for the bottom node Qpb, which is [0.25, 0.25] since reaching that node comes after playing p = 0.5 in node Q, so both are 0.5 * 0.5.\n\n\nWe compared four CFR algorithms (Chance Sampling, External Sampling, Vanilla, and CFR+) in terms of exploitability vs. nodes touched (a measure of how long the algorithm has been running for) and then also look at two of those algorithms which are very similar, CFR and its recent update, CFR+, in terms of exploitability vs. time. Finally, we produce strategy charts that show a Nash equilibrium strategy for each player at all four stages of the game. CFR+ is explained more in the CFR Advances section, but in short resets regrets that have gone negative to 0 so they are more likely to have a chance to “rebound” in case they were actually good strategies that, for example, just got unlucky.\nThe simulations run for a set number of iterations and the regrets for all algorithms are updated after each iteration.\nAs the algorithms run, a best response function is called periodically, which iterates once through the game tree once for each player. The average of the best response values from each player is taken as the exploitability of the game at that point. All graphs show exploitability on the vertical axis on a log scale. CFR and CFR+ were run for 100,000 iterations and Chance and External Sampling were run for 10^9 iterations. Since the non-sampling algorithms require entire tree traversals for each iteration, they require far fewer iterations to reach the same number of nodes. The game value for all variants is -0.0566, as we have found in previous sections.\nWe examine nodes touched vs. exploitability for all four of our CFR algorithm types (Vanilla CFR vs. CFR+ vs. Chance Sampling vs. External Sampling) up to 4 * 10^9 nodes touched for each. Monte Carlo sampling methods require many more iterations than Vanilla CFR, while each iteration is relatively fast. Therefore, a nodes touched metric makes sense as a way of comparison.\n\n\n\nCFR Algs Compared\n\n\nWe can see that the sampled versions show a lower exploitability much faster than the Vanilla and CFR+ versions, although they are more erratic due to the sampling. While Chance Sampling is generally superior to External Sampling, they are quite close at the end of the experiment. Chance Sampling is the simplest algorithm, which may work in its favor since Kuhn Poker is also a very simple game. Vanilla CFR shows consistently lower exploitability than CFR+. Perhaps this is because CFR+ doesn’t allow regrets to become negative, it may then waste time on actions that would have gone negative.\nWe also experimented with time vs. exploitability. The algorithms and code for CFR+ and CFR are exactly the same except for how the regret is calculated and since this eliminates other sources of variability, we are able to reasonably compare CFR and CFR+ exploitability against time.\n\n\n\nCFR CFR+ Time vs. Exploitability\n\n\nThis graph shows that CFR+ takes significantly more time to complete its 100,000 iterations and yet is still at a higher exploitability. Since the only difference in the algorithms is that CFR+ does not allow regrets to become negative, this must be the cause of the additional calculation time needed."
  },
  {
    "objectID": "aipcs24/cfr.html#similarities-to-reinforcement-learning",
    "href": "aipcs24/cfr.html#similarities-to-reinforcement-learning",
    "title": "Ultimate Guide to CFR",
    "section": "",
    "text": "In reinforcement learning, agents learn what actions to take in an environment based on the rewards they’ve seen in the past. This is very similar to how regret updates work in CFR – we can think of the regrets as advantage functions, which is the value of a certain action compared to the value of a state, and in fact this terminology has been seen in recent papers like the Deep CFR paper. We can also compare this to having independent multiarm bandits at each decision point, learning from all of them simultaneously."
  },
  {
    "objectID": "aipcs24/bots.html",
    "href": "aipcs24/bots.html",
    "title": "Building and Running Bots",
    "section": "",
    "text": "Our game engine is run in Python 3.\n\nCheck that you have at least Python 3.12 installed with:\n\npython3 --version\n\nThe game engine uses the eval7 package, which is a Python Texas Hold’em hand evaluation library:\n\npip3 install eval7",
    "crumbs": [
      "About",
      "Configuration",
      "Building and Running Bots"
    ]
  },
  {
    "objectID": "aipcs24/bots.html#python-setup",
    "href": "aipcs24/bots.html#python-setup",
    "title": "Building and Running Bots",
    "section": "",
    "text": "Our game engine is run in Python 3.\n\nCheck that you have at least Python 3.12 installed with:\n\npython3 --version\n\nThe game engine uses the eval7 package, which is a Python Texas Hold’em hand evaluation library:\n\npip3 install eval7",
    "crumbs": [
      "About",
      "Configuration",
      "Building and Running Bots"
    ]
  },
  {
    "objectID": "aipcs24/bots.html#poker-camp-game-engine",
    "href": "aipcs24/bots.html#poker-camp-game-engine",
    "title": "Building and Running Bots",
    "section": "Poker Camp Game Engine",
    "text": "Poker Camp Game Engine\n7/18 Note: The game engine is under development and you might see changes, especially over the first couple of weeks of the AIPCS24.\n\nEngine\nThe engine is in engine.py. You can use python3 engine.py to test two agents playing against each other.\nTo run a 100 hand match with two bots that are named p1 and p2 and run the logic from players/random/ folder and output results to the p1p2test folder, do this:\npython3 engine.py -p1 'p1' players/random/ -p2 'p2' players/random/ -o p1p2test -n 100\nThe generic usage is:\npython3 engine.py -p1 {p1_name} {p1_file_path} -p2 {p2_name} {p2_file_path} -o {output_dir} -n {n_hands}\"\nThe output files are:\n\nscores.p1.p2.txt contains the raw scores (i.e. profits) of each player\nThe p1.p2 folder contains:\n\n\ngamelog.txt: A log of all hands played\nOther log files for each player\n\n\n\nConfig\nThe config.py file contains various parameters to control the game engine. You should not need to modify this in normal use.",
    "crumbs": [
      "About",
      "Configuration",
      "Building and Running Bots"
    ]
  },
  {
    "objectID": "aipcs24/bots.html#build-a-bot",
    "href": "aipcs24/bots.html#build-a-bot",
    "title": "Building and Running Bots",
    "section": "Build a Bot",
    "text": "Build a Bot\nThe player.py file is where you write your poker bot.\nNote that for Kuhn Poker, the cards are assigned as follows:\n\n\n\nCard\nEngine\n\n\n\n\nQ\n0\n\n\nK\n1\n\n\nA\n2\n\n\n\nThere are three preconfigured bots that you can see to get a sense of how they work:\n\nrandom: Every action is random. In Kuhn this means 50% ↑ actions and 50% ↓ actions.\nlinear: For Kuhn, every Q action is ↓, every K action is 50% ↑ and 50% ↓, and every A action is 100% ↑.\nfrom-weights: This is how the Kuhn Challenge works. Each infoset is assigned a specific weight and the bot always plays according to those strategy probabilities.\n\n\n\n\n\n\n\nOther files include that you should not need to modify\n\n\n\n\n\n\nactions.py: The actions a player can take\nbot.py: Defines the interface of the player.py functions\nrunner.py: The infrastructure for interacting with the engine\nstates.py: Encapsulates game and round state information for the player\n\n\n\n\n\nUsing player.py to Build a Bot\nplayer.py contains 3 functions:\n\nhandle_new_round(): Gets called when a new round (i.e. hand) starts\nhandle_round_over(): Gets called when a new round (i.e. hand) ends\nget_action(): The main function to implement, which is called any time the engine needs an action from your bot.\n\nYou should write these functions so that get_action() returns the actions that you want in the situations it faces.\n\nThe get_action() function\nThe arguments coming in to get_action() are:\n\ngame_state: the GameState object, which is the state of the entire match of hands. This was 100 in the above example. The game state gives:\nbankroll: Profits over the match\ngame_clock: Your time remaining to use during the match\nround_num: The round of betting, always 1 in Kuhn Poker\n\nHere’s an example GameState:\ngame state GameState(bankroll=0, game_clock=29.991, round_num=1)\n\nround_state: the RoundState object, which contains all information about the current hand.\n\nThis includes :\n\nturn: The number of actions that have taken place this game. (turn % 2 gives the player who will act next.)\nstreet: Current betting round (in Kuhn Poker, this will always be 0).\npips: How many chips each player has contributed to the pot on the current hand.\nstacks: How many chips each player has left (not contributed to the pot).\nhands: List of known hands to you, with None for unknown hands.\ndeck: This won’t be known to you, so it will probably always be None.\naction_history: History of actions, a list of UpAction() or DownAction(). (The type of this will definitely change as we work on it.)\nprevious_state: The previous state of the hand, as a RoundState.\n\nHere’s an example RoundState:\nRoundState(\n    turn=1,\n    street=0,\n    pips=[1, 1],\n    stacks=[1, 1],\n    hands=[None, 1],\n    deck=None,\n    action_history=[DownAction()],\n    previous_state=\n        RoundState(\n            turn=0,\n            street=0,\n            pips=[1, 1],\n            stacks=[1, 1],\n            hands=[None, 1],\n            deck=None,\n            action_history=[],\n            previous_state=None\n        )\n    )\n\nactive: your player’s index\n\nThe return is:\n\nYour action\n\n\n\n\nDebugging your broken bot\nBecause of the way engine.py captures the output of the bots it runs, you probably don’t get the printed output of your broken bot failing. If you comment out the line of stdout=subprocess.PIPE, stderr=subprocess.STDOUT, in this function call, you can (probably?) disable this behavior:\nproc = subprocess.Popen(\n    self.commands['run'] + [str(port)],\n    stdout=subprocess.PIPE, stderr=subprocess.STDOUT,\n    cwd=self.path,\n)",
    "crumbs": [
      "About",
      "Configuration",
      "Building and Running Bots"
    ]
  },
  {
    "objectID": "aipcs24/bots.html#sample-bots-for-challenge-1-kuhn-poker",
    "href": "aipcs24/bots.html#sample-bots-for-challenge-1-kuhn-poker",
    "title": "Building and Running Bots",
    "section": "Sample Bots (for Challenge 1 / Kuhn Poker)",
    "text": "Sample Bots (for Challenge 1 / Kuhn Poker)\n\nRandom\nThe most simple random agent doesn’t care about the GameState or RoundState and implements the simple action:\nreturn random.choice([UpAction(), DownAction()])\n\n\nLinear Agent\nThe linear agent also doesn’t use GameState or RoundState, but does change its actions depending on its own hand. It uses this code to match my_hand to the appropriate linear case.\nmatch my_hand:\n    case 0:\n        return DownAction()\n    case 1:\n        return random.choice([UpAction(), DownAction()])\n    case 2:\n        return UpAction()\n\n\nWeights (Probabilities) Agent\nThe from-weights agent does need to use the round_state to first see whose turn it is to act (round_state.turn) and then to match the hand to the appropriate infoset using match my_hand. From there, the strategy can be defined according to the strategy probabilities (weights) for that infoset.\nmatch round_state.turn:\n    case 0:\n        match my_hand:\n            case 0: # Q_\n                up_prob = self.strategy[\"Q_\"]\n            case 1: # K_\n                up_prob = self.strategy[\"K_\"]\n            case 2: # A_\n                up_prob = self.strategy[\"A_\"]\n    case 1:\n        match my_hand, round_state.action_history[0]:\n            case 0, DownAction(): #_Q↓\n                up_prob = self.strategy[\"_QD\"]\n...\n\n\n…and beyond?\nThe from-weights agent gives you the tools to implement any fixed strategy that you want. If you want to do better than Nash, though, you’ll have to do something that remembers what your opponent has played in previous rounds, and use it to do something differently in the future…\n\n\n\n\n\n\nBeta Note\n\n\n\nWe expect to add a handle_observed_action() function in the bot/runner framework, to make certain kinds of tracking easier. For now, you can do this by adding logging logic to the get_action() and/or handle_round_over().",
    "crumbs": [
      "About",
      "Configuration",
      "Building and Running Bots"
    ]
  },
  {
    "objectID": "aipcs24/order.html",
    "href": "aipcs24/order.html",
    "title": "Order in the AIPC",
    "section": "",
    "text": "Intro/Kuhn Poker\nRock Paper Scissors/Kuhn Poker vs. Fixed Opponent\nRock Paper Scissors/Kuhn Poker Equilibrium\nRock Paper Scissors Tournament (vs. equilibrium/adaptive/fixed opponents)\n100-Card Kuhn Poker Tournament (vs. equilibrium/adaptive/fixed opponents)\nLeduc Poker (or slightly more complicated)\nTexas Tac Toe\nAllin/Fold 10BB NLHE\nRock Poker Scissors\nRock Poker Scissors"
  },
  {
    "objectID": "aipcs24/order.html#ideas-for-order",
    "href": "aipcs24/order.html#ideas-for-order",
    "title": "Order in the AIPC",
    "section": "",
    "text": "Intro/Kuhn Poker\nRock Paper Scissors/Kuhn Poker vs. Fixed Opponent\nRock Paper Scissors/Kuhn Poker Equilibrium\nRock Paper Scissors Tournament (vs. equilibrium/adaptive/fixed opponents)\n100-Card Kuhn Poker Tournament (vs. equilibrium/adaptive/fixed opponents)\nLeduc Poker (or slightly more complicated)\nTexas Tac Toe\nAllin/Fold 10BB NLHE\nRock Poker Scissors\nRock Poker Scissors"
  },
  {
    "objectID": "aipcs24/sessions.html",
    "href": "aipcs24/sessions.html",
    "title": "Sessions and Challenges",
    "section": "",
    "text": "This is the schedule for AI Poker Camp Summer 2024. We plan to spend Mondays reviewing previous challenges and thinking about new challenges/adjacent problems and Thursdays working on the challenges, which will be due every Sunday.\n\nThe calendar below includes all dates, locations, and times. You can add the whole thing to your own calendar with the button in the bottom right.",
    "crumbs": [
      "About",
      "About the Course",
      "Sessions and Challenges"
    ]
  },
  {
    "objectID": "aipcs24/1kuhn_challenge_review.html",
    "href": "aipcs24/1kuhn_challenge_review.html",
    "title": "#1: Kuhn Poker | Challenge Review",
    "section": "",
    "text": "Explain differences here between the regret that wasn’t converging on the site and the one that is\nAverage strategy thing Regret min thing from cfr.pdf Failure mode spinning around the right solution, want to spin inwards/converge"
  },
  {
    "objectID": "aipcs24/1kuhn_challenge_review.html#review-of-challenge-1",
    "href": "aipcs24/1kuhn_challenge_review.html#review-of-challenge-1",
    "title": "#1: Kuhn Poker | Challenge Review",
    "section": "Review of Challenge 1",
    "text": "Review of Challenge 1\nGoal: Equilibrium agent, later opponent modeling\nWhat does best bot vs you look like? What is your exploitability? How do we evaluate agents?\n\nTournament Results\nYep, just a slight variation here where the dealer burns 26/52 cards and then you play against the house; make it something other than just off-the-shelf and make your MC solver have to do some real work.\n\n\nHow the Interactive Works\nOdds vs. probs Regret Full CFR details next session Show equations and graphs of how things correct when a single probability is thrown off\n\n\nOptimal Strategies\nSurprise! There are multiple Nash equilibria in Kuhn!\nValue of the game\nPosition thing\nPrinciple of 3 general types of hands and how it applies to regular poker\n\n\nPure vs. Mixed Strategies\nGradient Descent: Involves iteratively adjusting parameters to minimize a cost function. Each step moves the parameters in the direction of the negative gradient of the cost function, gradually converging to a local or global minimum. Aims to converge to the optimal parameters that minimize the cost function. With an appropriate learning rate and sufficient iterations, it can find the minimum. Utilizes feedback from the gradient of the cost function at each iteration to update the parameters. Focuses on a static objective function (cost function) and aims to find its minimum.\nRegret Minimization: In online learning and decision-making contexts, it involves iteratively updating strategies to minimize regret, which is the difference between the actual cumulative loss and the best possible cumulative loss in hindsight. Each step adjusts the strategy based on past performance to improve future decisions. Aims to minimize regret over time, which means the strategy becomes nearly as good as the best fixed strategy in hindsight. With enough iterations, the average regret per iteration tends to zero. Utilizes feedback from past performance (losses) to update the strategy, aiming to reduce future regret. Focuses on a dynamic objective (minimizing regret over time) in potentially changing environments, where the best action may vary over time.\nRegret Matching: Involves iteratively updating the probability distribution over actions based on past regrets. Actions with higher regrets (indicating they would have performed better in the past) are chosen more frequently in the future. Uses past regrets to update the probability distribution over actions. The probability of selecting each action increases proportionally to the regret of not having taken that action.\nPruning and compare to CFR"
  },
  {
    "objectID": "aipcs24/3rps_reading.html",
    "href": "aipcs24/3rps_reading.html",
    "title": "#3 Rock Paper Scissors: Reading 1",
    "section": "",
    "text": "Rock defeats scissors, scissors defeats paper, and paper defeats rock. You get 1 point for winning, -1 point for losing, and 0 points for ties.\nThe goal of this challenge is to focus on tracking opponent distributions and how to respond to them.",
    "crumbs": [
      "About",
      "Challenges",
      "#3: Rock Paper Scissors",
      "Class Materials"
    ]
  },
  {
    "objectID": "aipcs24/3rps_reading.html#counterfactual-regret-minimization",
    "href": "aipcs24/3rps_reading.html#counterfactual-regret-minimization",
    "title": "#3 Rock Paper Scissors: Reading 1",
    "section": "Counterfactual Regret Minimization",
    "text": "Counterfactual Regret Minimization\nAs we think about solving larger games, we start to look at iterative algorithms.\nThe most popular method for iteratively solving poker games is the Counterfactual Regret Minimization (CFR) algorithm. CFR is an iterative algorithm developed in 2007 at the University of Alberta that converges to Nash equilibrium in two player zero-sum games.\n(Note: The handout solver does not exactly use CFR. You can make updates to the solver however you would like, including modifying it to become CFR.)\nWhat is a counterfactual? Here’s an example:\nActual event: I didn’t bring an umbrella, and I got wet in the rain\nCounterfactual event: If I had brought an umbrella, I wouldn’t have gotten wet\n\nRegret and Strategies\nA strategy at an infoset is a probability distribution over each possible action.\nRegret is a measure of how much each strategy at an infoset is preferred and is used as a way to update strategies.\nFor a given P1 strategy and P2 strategy, a player has regret when they take an action at an infoset that was not the highest-EV action at that infoset.\n\n\n\n\n\n\nRegret Exercise\n\n\n\n\nWhat is the regret for each action?\n\n\n\nAction\nRegret\n\n\n\n\nA\n\n\n\nB\n\n\n\nC\n\n\n\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\n\nAction\nRegret\n\n\n\n\nA\n4\n\n\nB\n2\n\n\nC\n0\n\n\n\n\n\n\n\n\n\n\n\n\nExpected Value Exercise\n\n\n\n\nIf taking a uniform strategy at this node (i.e. \\(\\frac{1}{3}\\) for each action), then what is the expected value of the node?\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\\(\\mathbb{E} = \\frac{1}{3}*1 + \\frac{1}{3}*3+\\frac{1}{3}*5 = 0.33+1+1.67 = 3\\)\n\n\n\n\n\n\n\n\n\nPoker Regret Exercise\n\n\n\n\nIn poker games, the regret for each action is defined as the value for that action minus the expected value of the node. Give the regret values for each action under this definition.\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\n\nAction\nValue\nPoker Regret\n\n\n\n\nA\n1\n-2\n\n\nB\n3\n0\n\n\nC\n5\n2\n\n\n\nAt a node in a poker game, the player prefers actions with higher regrets by this definition.\n\n\n\nEach infoset maintains a strategy and regret tabular counter for each action. These accumulate the sum of all strategies and the sum of all regrets.\nIn a game like Rock Paper Scissors, there is effectively only one infoset, so only one table for strategy over each action (Rock, Paper, Scissors) and one table for regret over each action (Rock, Paper, Scissors).\nRegrets are linked to strategies through a policy called regret matching.\n\n\nRegret Matching\n\n\n\n\n\n\nRPS Regret Details\n\n\n\n\n\nIn general, we define regret as:\n\\(\\text{Regret} = u(\\text{Alternative Strategy}) − u(\\text{Current Strategy})\\)\nWe prefer alternative actions with high regret and wish to minimize our overall regret.\nWe play Rock and opponent plays Paper \\(\\implies \\text{u(rock,paper)} = -1\\)\n\\(\\text{Regret(scissors)} = \\text{u(scissors,paper)} - \\text{u(rock,paper)} = 1-(-1) = 2\\)\n\\(\\text{Regret(paper)} = \\text{u(paper,paper)} - \\text{u(rock,paper)} = 0-(-1) = 1\\)\n\\(\\text{Regret(rock)} = \\text{u(rock,paper)} - \\text{u(rock,paper)} = -1-(-1) = 0\\)\nWe play Scissors and opponent plays Paper \\(\\implies \\text{u(scissors,paper)} = 1\\)\n\\(\\text{Regret(scissors)} = \\text{u(scissors,paper)} - \\text{u(scissors,paper)} = 1-1 = 0\\)\n\\(\\text{Regret(paper)} = \\text{u(paper,paper)} - \\text{u(scissors,paper)} = 0-1 = -1\\)\n\\(\\text{Regret(rock)} = \\text{u(rock,paper)} - \\text{u(scissors,paper)} = -1-1 = -2\\)\nWe play Paper and opponent plays Paper \\(\\implies \\text{u(paper,paper)} = 0\\)\n\\(\\text{Regret(scissors)} = \\text{u(scissors,paper)} - \\text{u(paper,paper)} = 1-0 = 1\\)\n\\(\\text{Regret(paper)} = \\text{u(paper,paper)} - \\text{u(paper,paper)} = 0-0 = 0\\)$\n\\(\\text{Regret(rock)} = \\text{u(rock,paper)} - \\text{u(paper,paper)} = -1-0 = -1\\)\nTo generalize:\n\nThe action played always gets a regret of 0 since the “alternative” is really just that same action\nWhen we play a tying action, the alternative losing action gets a regret of -1 and the alternative winning action gets a regret of +1\nWhen we play a winning action, the alternative tying action gets a regret of -1 and the alternative losing action gets a regret of -2\nWhen we play a losing action, the alternative winning action gets a regret of +2 and the alternative tying action gets a regret of +1\n\nAfter each play, we accumulate regrets for each of the 3 actions.\n\n\n\nWe decide our strategy probability distribution using regret matching, which means playing a strategy that normalizes over the positive accumulated regrets, i.e. playing in proportion to the positive regrets.\nExample from Marc Lanctot’s CFR Tutorial:\n\nGame 1: Choose Rock and opponent chooses Paper\n\nLose 1\nRock: Regret 0\nPaper: Regret 1\nScissors: Regret 2\n\nNext Action: Proportional \\[\n\\begin{pmatrix}\n\\text{Rock} & 0/3 = 0 \\\\\n\\text{Paper} & 1/3 = 0.333 \\\\\n\\text{Scissors} & 2/3 = 0.667\n\\end{pmatrix}\n\\]\nGame 2: Choose Scissors (With probability \\(2/3\\)) and opponent chooses Rock\n\nLose 1\nRock: Regret 1\nPaper: Regret 2\nScissors: Regret 0\n\nCumulative regrets:\n\nRock: 1\nPaper: 3\nScissors: 2\n\nNext Action: Proportional \\[\n\\begin{pmatrix}\n\\text{Rock} & 1/6 = 0167 \\\\\n\\text{Paper} & 3/6 = 0.500 \\\\\n\\text{Scissors} & 2/6 = 0.333\n\\end{pmatrix}\n\\]\n\nRegret matching definitions:\n\n\\(a\\) is actions\n\\(\\sigma\\) is strategy\n\\(t\\) is time\n\\(i\\) is player\n\\(R\\) is cumulative regret\n\n\\[\n\\sigma_i^t(a) = \\begin{cases}\n\\frac{\\max(R_i^t(a), 0)}{\\sum_{a' \\in A} \\max(R_i^t(a'), 0)} & \\text{if } \\sum_{a' \\in A} \\max(R_i^t(a'), 0) &gt; 0 \\\\\n\\frac{1}{|A|} & \\text{otherwise}\n\\end{cases}\n\\]\nThis is showing that we take the cumulative regret for an action divided by the cumulative regrets for all actions (normalizing) and then play that strategy for this action on the next iteration.\nIf all cumulative regrets are \\(\\leq 0\\) then we use the uniform distribution.\nIf cumulative regrets are positive, but are are \\(&lt;0\\) for a specific action, then we use \\(0\\) for that action.\nIn code:\n    def get_strategy(self):\n  #First find the normalizing sum\n        normalizing_sum = 0\n        for a in range(NUM_ACTIONS):\n            if self.regret_sum[a] &gt; 0:\n                self.strategy[a] = self.regret_sum[a]\n            else:\n                self.strategy[a] = 0\n            normalizing_sum += self.strategy[a]\n\n    #Then normalize each action\n        for a in range(NUM_ACTIONS):\n            if normalizing_sum &gt; 0:\n                self.strategy[a] /= normalizing_sum\n            else:\n                self.strategy[a] = 1.0/NUM_ACTIONS\n            self.strategy_sum[a] += self.strategy[a]\n\n        return self.strategy\nAfter using regret matching and after many iterations, we can minimize expected regret by using the average strategy at the end, which is the strategy that converges to equilibrium.\nIf two players were training against each other using regret matching, they would converge to the Nash Equilibrium of \\(1/3\\) for each action using the average strategy in Rock Paper Scissors.\n\n\nRPS Regret Matching Experiment\nHere we show that regret matching converges only using the average strategy over 10,000 iterations:\n\nThe bottom shows both players converging to \\(1/3\\), while the top shows Player 1’s volatile current strategies that are cycling around.\nSuppose that your opponent Player 2 is playing 40% Rock, 30% Paper, and 30% Scissors. Here is a regret matching 10,000 game experiment. It shows that it takes around 1,600 games before Player 1 plays only Paper (this will vary).\n\nWe see that if there is a fixed player, regret matching converges to the best strategy.\nBut what if your opponent is not using a fixed strategy? We’ll talk about that soon.\n\n\nIterating through the Tree\nThe core feature of the iterative algorithms is self-play by traversing the game tree over all infosets and tracking the strategies and regrets at each.\nFrom above, we know how to find the strategy and regret in the simple Rock Paper Scissors environment.\nIn poker:\n\nStrategies are determined the same as above, through regret matching from the previous regret values at the specific information set for each action\nCFR definitions:\n\n\\(a\\) is actions\n\\(I\\) is infoset\n\\(\\sigma\\) is strategy\n\\(t\\) is time\n\\(i\\) is player\n\\(R\\) is cumulative regret\n\\(z\\) is a terminal node\n\\(u\\) is utility (payoffs)\n\\(p\\) is the current player who plays at this node\n\\(-p\\) is the the opponent player and chance\n\\(v\\) is counterfactual value\n\nCounterfactual values are effectively the value of an information set. They are weighted by the probability of opponent and chance playing to this node (in other words, the probability of playing to this node if this player tried to do so).\n\nCounterfactual value: \\(v^\\sigma (I) = \\sum_{z\\in Z_I} \\pi^{\\sigma}_{-p}(z[I])\\pi^{\\sigma}(z[I] \\rightarrow z)u_p(z)\\)\n\\(\\sum_{z\\in Z_I}\\) is summing over all terminal histories reachable from this node\n\\(\\pi^{\\sigma}_{-p}(z[I])\\) is the probability of opponents and chance reaching this node\n\\(\\pi^{\\sigma}(z[I] \\rightarrow z)\\) is the probability of playing from this node to terminal history \\(z\\), i.e. the weight component of the expected value\n\\(u_p(z)\\) is the utility at terminal history \\(z\\), i.e. the value component of the expected value\n\nInstantaneous regrets are based on action values compared to infoset EV. Each action EV then adds to its regret counter:\n\n\\(r^t(I,a) = v^{\\sigma^t}(I,a) - v^{\\sigma^t}(I)\\)\n\nCumulative (counterfactual) regrets are the sum of the individual regrets:\n\n\\(R^T(I,a) = \\sum_{t=1}^T r^t(I,a)\\)\n\n\n\n\nAlternatives/Updates to Original Algorithm\n\nCFR+ variation such that regrets can’t be \\(\\leq 0\\)\nLinear CFR such that regrets are weighted by their recency\nSampling\n\nExternal: Sample chance and opponent nodes\nChance: Sample chance only\nOutcome: Sample outcomes",
    "crumbs": [
      "About",
      "Challenges",
      "#3: Rock Paper Scissors",
      "Class Materials"
    ]
  },
  {
    "objectID": "aipcs24/3rps_reading.html#data-talk-paper-scissors",
    "href": "aipcs24/3rps_reading.html#data-talk-paper-scissors",
    "title": "#3 Rock Paper Scissors: Reading 1",
    "section": "Data: Talk Paper Scissors",
    "text": "Data: Talk Paper Scissors\neieio games made a Rock Paper Scissors over voice game in which players call a phone number and get matched up with another player for a 3 game RPS match.\nThey published their 40,000 round data on X:\n Overall: R 37.2%, P 35.4%, S 27.4%\n Round 1: R 39.7%, P 37.6%, S 22.7%\n Round 2: R 34.0%, 33.4%, 32.6%\n Round 3: R 37.2%, 34.7%, 28.1%\n\n\n\n\n\n\nExpected Value Against TPS Player\n\n\n\nWhat is the best strategy per round against the average TPS player? What is your expected value per round and overall?\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThe best strategy is to always play Paper.\n\\(\\mathbb{E}(\\text{Round 1}) = 0.397*1 + 0.376*0 + 0.227*-1 = 0.17\\)\n\\(\\mathbb{E}(\\text{Round 2}) = 0.34*1 + 0.334*0 + 0.326*-1 = 0.014\\)\n\\(\\mathbb{E}(\\text{Round 3}) = 0.372*1 + 0.347*0 + 0.281*-1 = 0.091\\)\n\\(\\mathbb{E}(\\text{Round 4}) = 0.17 + 0.014 + 0.091 = 0.275\\)",
    "crumbs": [
      "About",
      "Challenges",
      "#3: Rock Paper Scissors",
      "Class Materials"
    ]
  },
  {
    "objectID": "aipcs24/3rps_reading.html#more-exercises",
    "href": "aipcs24/3rps_reading.html#more-exercises",
    "title": "#3 Rock Paper Scissors: Reading 1",
    "section": "More Exercises",
    "text": "More Exercises\n\n\n\n\n\n\nMaximize Against non-Nash Fixed Opponent\n\n\n\nHow would you maximize in RPS knowing the opponent plays a fixed non-Nash strategy that you don’t know?\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nOne option is to play the equilibrium strategy until you get a significant sample on your opponent and then to exploit their strategy going forward.\n\n\n\n\n\n\n\n\n\nStrategy Against No-Rock Opponent\n\n\n\nWhat is the optimal play if your opponent can’t play Rock?\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\n\nPlayer 1/2\nPaper\nScissors\n\n\n\n\nRock\n(-1, 1)\n(1, -1)\n\n\nPaper\n(0, 0)\n(-1, 1)\n\n\nScissors\n(1, -1)\n(0, 0)\n\n\n\nWe can see that Player 1 playing Paper is dominated by Scissors, so Player 1 should never play Paper.\n\n\n\nPlayer 1/2\nPaper\nScissors\n\n\n\n\nRock\n(-1, 1)\n(1, -1)\n\n\nScissors\n(1, -1)\n(0, 0)\n\n\n\nIn the reduced game, we see that if Player 2 plays Paper with probability \\(p\\) and Scissors with probability \\(s\\), then:\n\\(\\mathbb{E}(\\text{P1 R}) = -1*p + 1*s = -p + s\\) \\(\\mathbb{E}(\\text{P1 S}) = 1*p + 0*s = p\\)\nSetting these equal, \\(-p + s = p \\Rightarrow s = 2p\\).\nWe also know that \\(s + p = 1\\).\nTherefore \\(s = 1 - p\\) and \\(1 - p = 2p \\Rightarrow 1 = 3p \\Rightarrow p = 1/3\\).\nTherefore, \\(s = 1 - 1/3 = 2/3\\).\nFor Player 2, we have \\(s = 2/3\\) and \\(p = 1/3\\).\nFor Player 1, we can solve similarly:\n\\(\\mathbb{E}(\\text{P2 P}) = 1*r - 1*s = r - s\\) \\(\\mathbb{E}(\\text{P2 S}) = -1*r + 0*s = -r\\)\n\\(r - s = -r \\Rightarrow 2r = s\\)\nWe also know that \\(r + s = 1\\).\nTherefore \\(s = 1 - r\\) and \\(1 - r = 2r \\Rightarrow 1 = 3r \\Rightarrow r = 1/3\\).\nTherefore, \\(s = 1 - 1/3 = 2/3\\).\nFor Player 2, we have \\(s = 2/3\\) and \\(p = 1/3\\).\nInserting these probabilities, we have:\n\n\n\nPlayer 1/2\nPaper (1/3)\nScissors (2/3)\n\n\n\n\nRock (1/3)\n(-1, 1) (1/9)\n(1, -1) (2/9)\n\n\nScissors (2/3)\n(1, -1) (2/9)\n(0, 0) (4/9)\n\n\n\nTherefore Player 1 has payoffs of: \\(1/9 * -1 + 2/9 * 1 + 2/9 * 1 + 4/9 * 0 = 3/9 = 1/3\\). Therefore the player that can still play Rock has an advantage of \\(1/3\\) at equilibrium.\n\n\n\n\n\n\n\n\n\nMaximize Against Adapting Rock Opponent\n\n\n\n\nSuppose that your opponent is forced to play Rock exactly\n\nSuppose that your opponent gets a card with probability \\(X\\) such that \\(X\\%\\) of the time they are forced to play Rock What if the opponent is adapting to you, but 10% of the time they are forced to play Rock?\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nSoon\n\n\n\n\n\n\n\n\n\nSkewed Rock Payoff\n\n\n\nWhat is the equilibrium strategy if the payoff for Rock over Scissors is 2 (others stay the same)?\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nSoon",
    "crumbs": [
      "About",
      "Challenges",
      "#3: Rock Paper Scissors",
      "Class Materials"
    ]
  },
  {
    "objectID": "aipcs24/2leduc_challenge.html",
    "href": "aipcs24/2leduc_challenge.html",
    "title": "#2: Leduc Poker | Challenge",
    "section": "",
    "text": "This week, you will submit two bots, one to play 100-Card Kuhn Poker, and one to play Leduc Poker.\nSkip ahead to the challenge specification.",
    "crumbs": [
      "About",
      "Challenges",
      "#2: Leduc Poker",
      "Challenge"
    ]
  },
  {
    "objectID": "aipcs24/2leduc_challenge.html#kuhn-poker-100-cards",
    "href": "aipcs24/2leduc_challenge.html#kuhn-poker-100-cards",
    "title": "#2: Leduc Poker | Challenge",
    "section": "Kuhn Poker 100 Cards",
    "text": "Kuhn Poker 100 Cards\nKuhn Poker with 100 cards plays the same as Kuhn Poker with 3 cards, but the cards are numbered from 1 to 100 (or 0 to 99).\n\n\n\n\n\n\n100-Card Kuhn Infosets\n\n\n\nKuhn Poker with 3 cards has 6 infosets per player, 12 total.\nHow many infosets are in 100-card Kuhn Poker?\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThese scale linearly and so with 100 cards there are 400 infosets since each card has 4 infosets:\n\nP1 acting first\nP2 facing an Up action\nP2 facing a Down action\nP1 after a Down-Up sequence\n\n\n\n\n\n\n\n\n\n\nGame states for 100-Card Kuhn infosets\n\n\n\nEach infoset in Kuhn Poker has two possible game states that correspond to it.\nHow many game states correspond to each infoset in 100-card Kuhn Poker?\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nEach infoset corresponds to 99 possible game states, one for each card the opponent could have.",
    "crumbs": [
      "About",
      "Challenges",
      "#2: Leduc Poker",
      "Challenge"
    ]
  },
  {
    "objectID": "aipcs24/2leduc_challenge.html#leduc-poker",
    "href": "aipcs24/2leduc_challenge.html#leduc-poker",
    "title": "#2: Leduc Poker | Challenge",
    "section": "Leduc Poker",
    "text": "Leduc Poker\nLeduc Poker is a simple toy poker game invented at the University of Alberta.\n\n\n\nAlberta’s nearby Edmonton Airport is in the city of Leduc\n\n\nHere is the setup:\n\n2 players.\n6 card deck: 2 Queens, 2 Kings, 2 Aces (in ascending order, so Ace is highest).\nEach player antes 1 chip.\nDeal 1 card to each player.\nBetting round 1 (the preflop):\n\nThere is a fixed bet size of 2 chips.\nIf the opponent has made no bet in this round, a player can Check (bet nothing) or Raise (bet one standard bet size).\nIf the opponent has bet, then a player can Fold (bet nothing and lose), Call (add chips to match opponent’s bet), or Raise (call the bet and add one standard bet size).\nThis round ends when one player Folds (and loses; neither cards are shown), or when one player Calls (and the game continues to the next step below). A special case is when both players Check, which proceeds the same as a Call with no added chips.\n\nDeal a face up community card (shown to both players).\n\nA pair (your card matches the community card) is the best hand, then an unpaired Ace, then an unpaired King, finally an unpaired Queen.\n\nBetting round 2 (the flop):\n\nThere is a fixed bet size of 4 chips.\nPlayers can Check, Fold, Call, and Raise the same as in the first round.\nThis round also ends when one player Folds (and loses), or one player Calls (and proceeds to showdown where the higher hand wins).\n\n\n\n\n\n\n\n\nMaximum bets per round rule in original version\n\n\n\n\n\nThe original version of Leduc Poker has a rule where the maximum bets per betting round is 2 (i.e. a bet and a raise), but we are not using that rule. Instead, each player has a maximum of 50 chips for each game. If a player does not have enough chips to make a full Raise, their Raise is for all their remaining chips.\n\n\n\n\nSample Leduc Hand and Leduc Math\nHere is a Leduc game situation in which:\n\nBoth players ante 1 each.\n\nPot = 2\n\nPreflop: P1 bets 2 and P2 calls.\n\nPot = 6\n\nFlop: Community card K revealed. Player 1 bets 4. Player 2 to act.\n\n\n\n\n\n\n\n\nLeduc Strategy\n\n\n\nWhat should Player 2 do here?\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nRaise! Player 2 has the best possible hand because they have a pair.\n\n\n\n\n\n\n\n\n\nLeduc Infoset\n\n\n\nWhat is Player 2’s infoset?\nWhat will Player 1’s infoset be after Player 2 acts?\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nWe could write Player 2’s infoset as: (P2)[_, K, K][Bet 2, Call 2][Bet 4]\nOr the default solver in pokercamp/aipc-challenges/challenge-2-leduc will write it at: (P2){'community': [2]}[None, 2][Raise, Call, Raise].\n\n\n\n\n\n\n\n\n\nLeduc Ties\n\n\n\nHow often will you and your opponent be dealt the same card?\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nFirst we find the total combinations of cards:\n\\({6\\choose2} = \\frac{6!}{4!*2!} = \\frac{6*5}{2} = 15\\)\nThen we count that there is exactly \\(1\\) way to make Q/Q, \\(1\\) way to make K/K, and \\(1\\) way to make A/A. Therefore the probability of having the same hand as your opponent is \\(\\frac{3}{15} = 0.2\\).\n\n\n\n\n\n\n\n\n\nLeduc Pairs\n\n\n\nSuppose that you are dealt a K. How often will you hit a pair on the flop given that you see it?\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThere are \\(5\\) unknown cards to you and \\(1\\) of them matches yours for a pair, so the \\(\\Pr(\\text{Pair} \\mid \\text{See Flop}) = \\frac{1}{5} = 0.2\\)",
    "crumbs": [
      "About",
      "Challenges",
      "#2: Leduc Poker",
      "Challenge"
    ]
  },
  {
    "objectID": "aipcs24/2leduc_challenge.html#hints",
    "href": "aipcs24/2leduc_challenge.html#hints",
    "title": "#2: Leduc Poker | Challenge",
    "section": "Hints",
    "text": "Hints\n\n\n\n\n\n\nCard abstractions in 100-Card Kuhn\n\n\n\n\n\nIt might be more efficient to solve 100-Card Kuhn Poker if you shrink the number of cards to a more manageable size by bucketing a group of nearby numbers together for strategy purposes. For example, you could treat cards 1-10 as the same, 11-20, and so on. Or is there a better way to bucket than uniformly?\n\n\n\n\n\n\n\n\n\nSampling Policy in Leduc\n\n\n\n\n\nThe set of possible action histories in Leduc Poker with 50-chip stacks is relatively large, and you don’t really care about most of it. Consider whether you can do something more efficient than always expanding every node.\n\n\n\n\n\n\n\n\n\nAveraging intermediate strategy probabilities\n\n\n\n\n\nAs you may have seen with the Kuhn solver site, counterfactual regret minimization tends to cycle around the equilibrium instead of descending into it. If you use some kind of average over recent strategy probs as your final probabilities, you may get much closer to Nash than just using the final values.\n\n\n\n\n\n\n\n\n\nKuhn Game Value Bonus Challenge\n\n\n\nFind an equilibrium strategy for 100 card and 3 card Kuhn Poker and compare the P2 advantage in 100 card Kuhn Poker to 3 card Kuhn Poker.\nExtra bonus: Compare the P2 advantage in 100 card Kuhn Poker to a uniform 10-bucket abstraction, and if possible, a better 10-bucket abstraction.\n\n\n\n\n\n\n\n\nLeduc Game Value Bonus Challenge\n\n\n\nFind an equilibrium strategy for Leduc Poker and compare the P2 advantage to 3-Card Kuhn and 100-Card Kuhn.",
    "crumbs": [
      "About",
      "Challenges",
      "#2: Leduc Poker",
      "Challenge"
    ]
  },
  {
    "objectID": "aipcs24/3rps_leaderboard.html#paper---maybe-scissors---maybe-rock",
    "href": "aipcs24/3rps_leaderboard.html#paper---maybe-scissors---maybe-rock",
    "title": "#3: Rock Paper Scissors | Challenge Leaderboard",
    "section": "Paper - Maybe Scissors - Maybe Rock",
    "text": "Paper - Maybe Scissors - Maybe Rock",
    "crumbs": [
      "About",
      "Challenges",
      "#3: Rock Paper Scissors",
      "Leaderboard"
    ]
  },
  {
    "objectID": "aipcs24/solvers.html",
    "href": "aipcs24/solvers.html",
    "title": "Implementing a Basic Solver",
    "section": "",
    "text": "We’ve provided a basic game-solver with the challenges code. You can find it at aipcs-challenges/solvers/default/ and run it with python solver.py --iter N.\nIn its base form, it will perform a simple version of the Counterfactual Regret Minimization Algorithm, computing expected values of each action and updating action probabilities towards the actions with higher EV (much like the Kuhn automatic-solver page).\nYou’re also welcome to write whatever else you want; we just thought this might be helpful for getting started.",
    "crumbs": [
      "About",
      "Configuration",
      "Building and Running Solvers"
    ]
  },
  {
    "objectID": "aipcs24/solvers.html#solver-framework",
    "href": "aipcs24/solvers.html#solver-framework",
    "title": "Implementing a Basic Solver",
    "section": "",
    "text": "We’ve provided a basic game-solver with the challenges code. You can find it at aipcs-challenges/solvers/default/ and run it with python solver.py --iter N.\nIn its base form, it will perform a simple version of the Counterfactual Regret Minimization Algorithm, computing expected values of each action and updating action probabilities towards the actions with higher EV (much like the Kuhn automatic-solver page).\nYou’re also welcome to write whatever else you want; we just thought this might be helpful for getting started.",
    "crumbs": [
      "About",
      "Configuration",
      "Building and Running Solvers"
    ]
  },
  {
    "objectID": "aipcs24/solvers.html#the-algorithm",
    "href": "aipcs24/solvers.html#the-algorithm",
    "title": "Implementing a Basic Solver",
    "section": "The algorithm",
    "text": "The algorithm\n\nBegin at the first turn of the game, with Chance actions determined randomly.\nAt each infoset, we will either use the sampling policy sample or expand_all (determined by get_sampling_policy()) to get an estimate of the expected value of this node.\n\nIf sample, we pick an action randomly based on get_training_strategy_probabilities() for the current infoset, and use the value of the state that takes us to as the value of this state.\nIf expand_all, then get the values of each possible successor state, and use the weighted average by get_training_strategy_probabilities() to get the expected value of this state.\n\nEach time we do expand_all (including during a recursive drill-down step), update the strategy probabiliites for this infoset based on which actions did better (in this state) than this state’s overall expected value (weighted over all actions).\n\nIn particular, we keep a running sum of the amount each action beat the EV by (floored at zero), and use the ratio of the sums as our training_strategy_probabilities.\n\nYou probably want to do a different transformation of training_strategy_probabilities to get your final strategy, but for now we just return the same thing.\n\nSome easy things to change are: - get_sampling_policy() to return \"sample\" at some infosets and \"expand_all\" at others. - determine_infoset() to coalesce different observable states into the same infoset. - get_training_strategy_probabilities() to have different behavior for default and updated strategy probs.",
    "crumbs": [
      "About",
      "Configuration",
      "Building and Running Solvers"
    ]
  },
  {
    "objectID": "aipcs24/solvers.html#making-changes",
    "href": "aipcs24/solvers.html#making-changes",
    "title": "Implementing a Basic Solver",
    "section": "Making changes",
    "text": "Making changes\nIn general, you can change the behavior of the solver significantly by editing the functions defined in solver.py:\n\nhandle_new_iteration() - called with the iteration number when a new iteration is about to begin.\nhandle_iteration_over() - called with the iteration number when an iteration is over.\nget_root() - called witht the iteration number to get a RoundState object to begin the new iteration at. (Can be used to define how states should be explored.)\ndetermine_infoset() - called to get the canonical name of the infoset for a given visible game state. (Can be used to coalesce infosets.)\nsample_actions() - relevant if action types have variable parameters (like bet sizes), called to go from a list of legal action types to a set of action-instances to investigate.\nget_sampling_policy() - currently supports \"sample\" and \"expand_all\", called with the iteration number to determine whether to use a random sample to approximate this state’s EV, or to call all possible actions and compute a weighted sum. (Can vary policy by iteration by using the iteration number.)\nhandle_new_samples() - called with the sampling policy, and the resulting samples, from visiting a state node. (Can be used to update probabilities based on EVs.)\nget_training_strategy_probabilities() - used by both sampling policies, called to get the current strategy’s probabilities of taking each action at a given infoset.\nget_final_strategy_probabilities() - relevant if you want to use a different transformation of the data to get final probabilities than the intermediate probabilities used in training steps.",
    "crumbs": [
      "About",
      "Configuration",
      "Building and Running Solvers"
    ]
  },
  {
    "objectID": "aipcs24/solvers.html#more",
    "href": "aipcs24/solvers.html#more",
    "title": "Implementing a Basic Solver",
    "section": "More",
    "text": "More\nIf you have positive or negative feedback about the solver, feel free to share it in the #aipcs24-technical-feedback channel of the discord.",
    "crumbs": [
      "About",
      "Configuration",
      "Building and Running Solvers"
    ]
  },
  {
    "objectID": "aipcs24/2leduc_challenge copy.html",
    "href": "aipcs24/2leduc_challenge copy.html",
    "title": "#2: Leduc Poker | Challenge",
    "section": "",
    "text": "Leduc Poker is a simple toy poker game invented at the University of Alberta.\nHere is the setup:\n\n6 card deck: 2 Queens, 2 Kings, 2 Aces (in ascending order, so Ace is highest)\nLeduc Poker is played with 2 players. We’ll again use  and .\nEach player antes 1 chip\nDeal 1 card to each player\nBetting round 1:\n\nThere is a fixed bet size of 2 chips\nThere is a maximum of 2 bets per round (i.e. a bet and a raise)\n\nDeal a face up community card\n\nPlayers make the best 2 card hand combining their card and the community card, meaning a pair is the best possible hand\n\nBetting round 2:\n\nThere is a fixed bet size of 4 chips\nThere is a maximum of 2 bets per round (i.e. a bet and a raise)\n\nNotes:\n\nPlayer 1 acts first, rotate who is Player 1 each hand\nPlayers can win/lose a maximum of 13 chips per hand\n\nInfosets and payoffs\n\n - ↑Up (putting a chip into the pot)\n\n↓Down (not putting a chip into the pot)\n\n\n\n\n\n\n\n↑Up and ↓Down in traditional poker terms\n\n\n\n\n↑Up actions indicate a Bet or Call.\n↓Down actions indicate a Check or Fold.\n\n\n\n\n\n\n\n\n\nExample game\n\n\n\n\nPlayers ante and cards are dealt.\n sees a A and plays ↑Up (1 more chip into pot).\n sees a K and plays ↑Up (1 more chip into pot).\nBoth cards are revealed in a 2-chip showdown.  has an A and  has a K.\n has the better hand and wins +2 chips (+1 from the ante, +1 from ’s ↑Up).\n\n\n\nThe betting (and the game) can go in three ways:\n\nOne player plays ↑Up, then the other player plays ↓Down. The player who played ↓Down folds. The winner wins the loser’s ante (and gets their own chip back). The players’ cards are not revealed. Note that this happens if the action is :\n\n\n↑Up, ↓Down, or\n↓Down, ↑Up, ↓Down.\n\n\nBoth players play ↓Down. They go to a showdown and the winner wins the one chip that the loser anted (and their own back).\nA player plays ↑Up, then the other player plays ↑Up. They go to a showdown and the winner wins the two chips the loser has put in the pot (and gets their own chips back). Note that the game will proceed to a 2-chip showdown if the action is:\n\n\n↑Up, ↑Up or - ↓Down, ↑Up , ↑Up.\n\nHere is a list of all possible betting sequences:\n\n\n\n\n\n\n\n\n\n\n\n\nWinner\n\n\n\n\n↑Up\n↓Down\n\n (+1)\n\n\n↑Up\n↑Up\n\nHigher Card (+2)\n\n\n↓Down\n↓Down\n\nHigher Card (+1)\n\n\n↓Down\n↑Up\n↓Down\n (+1)\n\n\n↓Down\n↑Up\n↑Up\nHigher Card (+2)\n\n\n\n\n\n\n\n\n\nPartner Exercise: Get started with Kuhn Poker\n\n\n\n\nGet cards, chips, paper, pen\nPlay 3 hands of Kuhn Poker and get used to how the game runs\nUse the pen and paper to start writing down all deterministic situations in the game (situations where there is clearly a correct move that you should take 100% of the time)\nPlay more hands as you think helpful\nOnce you have what you think is a full list of the deterministic states, you can stop and review the optional reading"
  },
  {
    "objectID": "aipcs24/2leduc_challenge copy.html#leduc-poker-rules",
    "href": "aipcs24/2leduc_challenge copy.html#leduc-poker-rules",
    "title": "#2: Leduc Poker | Challenge",
    "section": "",
    "text": "Leduc Poker is a simple toy poker game invented at the University of Alberta.\nHere is the setup:\n\n6 card deck: 2 Queens, 2 Kings, 2 Aces (in ascending order, so Ace is highest)\nLeduc Poker is played with 2 players. We’ll again use  and .\nEach player antes 1 chip\nDeal 1 card to each player\nBetting round 1:\n\nThere is a fixed bet size of 2 chips\nThere is a maximum of 2 bets per round (i.e. a bet and a raise)\n\nDeal a face up community card\n\nPlayers make the best 2 card hand combining their card and the community card, meaning a pair is the best possible hand\n\nBetting round 2:\n\nThere is a fixed bet size of 4 chips\nThere is a maximum of 2 bets per round (i.e. a bet and a raise)\n\nNotes:\n\nPlayer 1 acts first, rotate who is Player 1 each hand\nPlayers can win/lose a maximum of 13 chips per hand\n\nInfosets and payoffs\n\n - ↑Up (putting a chip into the pot)\n\n↓Down (not putting a chip into the pot)\n\n\n\n\n\n\n\n↑Up and ↓Down in traditional poker terms\n\n\n\n\n↑Up actions indicate a Bet or Call.\n↓Down actions indicate a Check or Fold.\n\n\n\n\n\n\n\n\n\nExample game\n\n\n\n\nPlayers ante and cards are dealt.\n sees a A and plays ↑Up (1 more chip into pot).\n sees a K and plays ↑Up (1 more chip into pot).\nBoth cards are revealed in a 2-chip showdown.  has an A and  has a K.\n has the better hand and wins +2 chips (+1 from the ante, +1 from ’s ↑Up).\n\n\n\nThe betting (and the game) can go in three ways:\n\nOne player plays ↑Up, then the other player plays ↓Down. The player who played ↓Down folds. The winner wins the loser’s ante (and gets their own chip back). The players’ cards are not revealed. Note that this happens if the action is :\n\n\n↑Up, ↓Down, or\n↓Down, ↑Up, ↓Down.\n\n\nBoth players play ↓Down. They go to a showdown and the winner wins the one chip that the loser anted (and their own back).\nA player plays ↑Up, then the other player plays ↑Up. They go to a showdown and the winner wins the two chips the loser has put in the pot (and gets their own chips back). Note that the game will proceed to a 2-chip showdown if the action is:\n\n\n↑Up, ↑Up or - ↓Down, ↑Up , ↑Up.\n\nHere is a list of all possible betting sequences:\n\n\n\n\n\n\n\n\n\n\n\n\nWinner\n\n\n\n\n↑Up\n↓Down\n\n (+1)\n\n\n↑Up\n↑Up\n\nHigher Card (+2)\n\n\n↓Down\n↓Down\n\nHigher Card (+1)\n\n\n↓Down\n↑Up\n↓Down\n (+1)\n\n\n↓Down\n↑Up\n↑Up\nHigher Card (+2)\n\n\n\n\n\n\n\n\n\nPartner Exercise: Get started with Kuhn Poker\n\n\n\n\nGet cards, chips, paper, pen\nPlay 3 hands of Kuhn Poker and get used to how the game runs\nUse the pen and paper to start writing down all deterministic situations in the game (situations where there is clearly a correct move that you should take 100% of the time)\nPlay more hands as you think helpful\nOnce you have what you think is a full list of the deterministic states, you can stop and review the optional reading"
  },
  {
    "objectID": "aipcs24/2leduc_challenge copy.html#types-of-games",
    "href": "aipcs24/2leduc_challenge copy.html#types-of-games",
    "title": "#2: Leduc Poker | Challenge",
    "section": "Types of Games",
    "text": "Types of Games\n\n\n\n\n\n\nExercise\n\n\n\nFill in the table below with games you know about. Thanks to Eliezer for getting us started.\n\n\n\n\n\n\n\n\n\n\nGame/Opponent\nFixed/Probabilistic Opponent\nAdversarial Opponent\n\n\n\n\nImperfect Info, No Player Agency/Decisions\n\n\n\n\nPerfect Info, Player Actions Always Perfect Info\n\n\n\n\nImperfect Info, Imperfect from Randomness\n\n\n\n\nImperfect Info, Imperfect from Randomness AND Game States\n\n\n\n\n\n\n\n\n\n\n\nPre-Filled Table\n\n\n\n\n\n\n\n\n\n\n\n\n\nGame/Opponent\nFixed/Probabilistic Opponent\nAdversarial Opponent\n\n\n\n\nImperfect Info, No Player Agency/Decisions\nCandy Land, War, Dreidel, Bingo, Chutes and Ladders, Slot machine\n\n\n\nPerfect Info, Player Actions Always Perfect Info\nPuzzles\nTictactoe, Checkers, Chess, Arimaa, Go\n\n\nImperfect Info, Imperfect from Randomness\nBlackjack\nBackgammon\n\n\nImperfect Info, Imperfect from Randomness AND Game States\nPartial Info Multi-armed Bandit\nPoker, Rock Paper Scissors, Liar’s Dice, Figgie\n\n\n\n\n\n\nWhat about solitaire? With Blackjack? What about a lottery? Mahjong? Tennis value of states in RL\nPure strategies in perfect info games vs. mixed in imperfect info\nDeterministic Nature: Because all players can see the entire game state and know all possible moves, strategies can be deterministic. Players can calculate and choose the optimal move based on this complete information. Pure Strategies: A pure strategy is a complete plan of action for every possible situation in a game. In perfect information games, players can follow a pure strategy because they know exactly what will happen as a result of each possible move.\nUncertainty and Hidden Information: Because players cannot see the entire game state, they must account for uncertainty and the hidden information of their opponents. This makes the game more about probabilities and expectations rather than certainties. Mixed Strategies: A mixed strategy involves randomizing over possible moves to prevent opponents from exploiting predictable patterns. By using mixed strategies, players can become less predictable and make it more difficult for opponents to formulate a counter-strategy.\nStrategic Randomization: In games like poker, where bluffing and deception play significant roles, mixed strategies are essential. For example, a player might choose to bluff (make a bet with a weak hand) with a certain probability to keep opponents guessing and to avoid being exploited by always playing in a predictable manner.\n(Ross note: The difference between “Chance” and “Imperfect Info” is that in Chance, the unknown [thing] doesn’t affect anything about the world until it becomes known, and then it’s not unknown any more. In Imperfect Info, the information has some effect on the world at time T1, then you need to make a decision at time T2, then the information will matter at some later point T3.)\n\n\n\n\n\n\nExercise\n\n\n\nWhat makes poker and other games in the bottom right of the table interesting?"
  },
  {
    "objectID": "aipcs24/2leduc_challenge copy.html#simulator",
    "href": "aipcs24/2leduc_challenge copy.html#simulator",
    "title": "#2: Leduc Poker | Challenge",
    "section": "Simulator",
    "text": "Simulator\nRandom, reward avg, etc.\nDescribe the exploration vs. exploitation dilemma. Introduce basic strategies: epsilon-greedy, UCB (Upper Confidence Bound), and Thompson Sampling.\nRecord the results and display them in real-time (either through the program or manually on a board). Strategy Discussion:\nAfter a few rounds, pause and discuss the strategies teams are using. Introduce the different algorithms and how they would approach the problem.\nAlgorithm Implementation:\nAllow teams to adopt one of the introduced algorithms for the next rounds. Compare the performance of different algorithms in terms of accumulated rewards.\n\n\n\n\n\nArm\n\n\nAverage Reward\n\n\nPulls\n\n\nActions\n\n\n\n\n\n\n\n\n\nReset"
  },
  {
    "objectID": "aipcs24/2leduc_challenge copy.html#solving-poker-games",
    "href": "aipcs24/2leduc_challenge copy.html#solving-poker-games",
    "title": "#2: Leduc Poker | Challenge",
    "section": "Solving Poker Games",
    "text": "Solving Poker Games\n\nKuhn Normal Form\nKuhn Game Tree"
  },
  {
    "objectID": "aipcs24/2leduc_challenge copy.html#cfr",
    "href": "aipcs24/2leduc_challenge copy.html#cfr",
    "title": "#2: Leduc Poker | Challenge",
    "section": "CFR",
    "text": "CFR\nSlides, what to include?"
  },
  {
    "objectID": "signup/index.html",
    "href": "signup/index.html",
    "title": "AI Poker Camp (2024 Summer Beta SF)",
    "section": "",
    "text": "Sign up now!\n\n\n\nSignup Form\n\nAfter signing up, we’ll be in touch with all details\nThe beta program is free!"
  },
  {
    "objectID": "signup/index.html#when",
    "href": "signup/index.html#when",
    "title": "AI Poker Camp (2024 Summer Beta SF)",
    "section": "When?",
    "text": "When?\n6pm-8pm Mondays and Thursdays, July 15 through August 15."
  },
  {
    "objectID": "signup/index.html#what",
    "href": "signup/index.html#what",
    "title": "AI Poker Camp (2024 Summer Beta SF)",
    "section": "What?",
    "text": "What?\nA five-week, twice-weekly course on applied game theory through you writing AIs to play games. By the end, you should be able to write an AI to play poker.\nThis is a beta test of a course we’re planning to run online in the fall, so it’ll be small. We’ll cap signups somewhere between 16 and 24 students."
  },
  {
    "objectID": "signup/index.html#where",
    "href": "signup/index.html#where",
    "title": "AI Poker Camp (2024 Summer Beta SF)",
    "section": "Where?",
    "text": "Where?\nStrictly in-person in San Francisco. Location to be announced.\n\nWait, really? I can’t make that!\nWe’re also planning to run an online version of the course starting in late September. You can join our mailing list to learn more when it’s announced."
  },
  {
    "objectID": "signup/index.html#who",
    "href": "signup/index.html#who",
    "title": "AI Poker Camp (2024 Summer Beta SF)",
    "section": "Who?",
    "text": "Who?\n\nWe recommend students be able to program in Python and perform a Bayesian update (though it’s fine to lean on an LLM for help on either).\nKnowledge of the game of poker is not necessary. (We are offering a 2 hour Poker Basics workshop on Sun Jul 7.)"
  },
  {
    "objectID": "signup/index.html#whats-the-curriculum",
    "href": "signup/index.html#whats-the-curriculum",
    "title": "AI Poker Camp (2024 Summer Beta SF)",
    "section": "What’s the curriculum?",
    "text": "What’s the curriculum?\nThe course is built around six or seven practical challenges – think “kaggle competition for game-playing programs”. These will cover:\n\nTutorial: One-card / Kuhn Poker\n\nTopic: Algorithms for solving incomplete-information games.\n\nLarger one-card poker formats and other simple games\n\nTopic: Scaling up algorithms to larger game trees.\n\nRock-Paper-Scissors against imperfect opponents\n\nTopic: Techniques for modeling empirical opponent behavior.\n\nHidden-information version of Probabilistic Tic-Tac-Toe\n\nTopic: Modeling hidden information from opponent actions.\n\nTexas Holdem with simplified betting\n\nTopic: Putting it together!\n\n\nWe’re intending for the Summer Beta to have about the intensity of one (1) college course in applied CS. You should expect to make at least 9 out of 10 class sessions."
  },
  {
    "objectID": "signup/index.html#whos-teaching",
    "href": "signup/index.html#whos-teaching",
    "title": "AI Poker Camp (2024 Summer Beta SF)",
    "section": "Who’s teaching?",
    "text": "Who’s teaching?\nRoss Rheingans-Yoo wrote the advanced trading simulations for the Jane Street trading internship program from 2018 to 2021.\nMax Chiswick is a former poker pro who has played more than 10 million hands of online poker, and created AI Poker Tutorial.\nRicki Heicklen is a curriculum advisor (but will not be teaching in the SF Beta)."
  },
  {
    "objectID": "signup/index.html#how",
    "href": "signup/index.html#how",
    "title": "AI Poker Camp (2024 Summer Beta SF)",
    "section": "How?",
    "text": "How?\nSignup Form\n\nAfter signing up, we’ll be in touch with all details\nThe beta program is free!"
  },
  {
    "objectID": "signup/index.html#something-else",
    "href": "signup/index.html#something-else",
    "title": "AI Poker Camp (2024 Summer Beta SF)",
    "section": "Something else?",
    "text": "Something else?\nGet in touch!"
  },
  {
    "objectID": "rps/index.html",
    "href": "rps/index.html",
    "title": "Rock Paper Scissors Hackathon",
    "section": "",
    "text": "Rock Paper Scissors Hackathon by Poker Camp\nSign up! (link soon) $20 entry fee to cover lunch and snacks"
  },
  {
    "objectID": "rps/index.html#event-details",
    "href": "rps/index.html#event-details",
    "title": "Rock Paper Scissors Hackathon",
    "section": "Event Details",
    "text": "Event Details\nWhat: ​8 rounds of Rock Paper Scissors bot competitions. All entrants play 1000 games against each other in each round. We’ll have our own bots that start on easy mode, and smarter ones will be added as the rounds go on.\nEach round you’ll get a score vs. our bots, vs. other participants, and overall.\nWhen: Sunday Sep 8, 2024 11am-5pm (can arrive starting at 10:30am)\nWhy:\n\nIn sequential decision-making, agent evaluation has largely been restricted to few interactions against experts, with the aim to reach some desired level of performance (e.g. beating a human professional player). We propose a benchmark for multiagent learning based on repeated play of the simple game Rock, Paper, Scissors.(DeepMind 2023)\n\nWhere: Fractal Tech Hub: 111 Conselyea St, Brooklyn NY"
  },
  {
    "objectID": "rps/index.html#how-it-works",
    "href": "rps/index.html#how-it-works",
    "title": "Rock Paper Scissors Hackathon",
    "section": "How it Works",
    "text": "How it Works\nCompetition structure:\n\nRock defeats scissors, scissors defeats paper, and paper defeats rock\nYou get 1 point for a win, -1 for a loss, and 0 for ties\nEach round, you’ll play 1000 game matches against each hackathon participant and each of our bots\nWe’ll put in 1-3 bots during each round and they’ll stay in for all rounds going forward\n\nHow do I write my own bot?\n\nWe’ll give you starter code in Python\nStarter code will show how to make a bot that plays any fixed percentage\nStarter code will show how to make a bot that plays some strategy according to the history of actions\n\nHow do I submit?\n\nFor each competition, you can submit up to the end of that round\nEach submission during the round will receive a test result based on how that bot would have done in the previous round\n\nDoing better in the previous round is a sign of progress, but may not actually result in a better score in the current round, which will have updated hackathon bots and newly inserted bots of our own\n\nAt the end of each round, we’ll run the tournament and show the results ~5 minutes later\n\nWhat’s the point of this?:\n\nFun\nThinking about strategy in a repeated game against a variety of opponents\n\nAre there prizes?\nThere will be small on-theme prizes\n\nSchedule\n10:30am: Doors open\n11:00am: Arrival and setup\n11:30am: Begin Competition 1\n12:00pm: Competition 1 results, begin Competition 2\n12:30pm: Competition 2 results, pizza lunch break\n1:00pm: Begin Competition 3\n1:30pm: Competition 3 results, begin Competition 4\n2:00pm: Competition 4 results, begin Competition 5\n2:30pm: Competition 5 results, begin Competition 6\n3:00pm: Competition 6 results, begin Competition 7\n3:30pm: Competition 7 results, begin Competition 8\n4:00pm: Final results and bot explanations\n4:30pm: Hangout\n5:00pm: End"
  },
  {
    "objectID": "rps/index.html#strategy",
    "href": "rps/index.html#strategy",
    "title": "Rock Paper Scissors Hackathon",
    "section": "Strategy",
    "text": "Strategy\nEach round you’ll be facing off against a selection of our bots and other hackathon participants.\n\n\nGame Theory Equilibrium\nRPS is a zero-sum game and the payouts are symmetrical as follows:\n\n\n\nPlayer 1/2\nRock\nPaper\nScissors\n\n\n\n\nRock\n(0, 0)\n(-1, 1)\n(1, -1)\n\n\nPaper\n(1, -1)\n(0, 0)\n(-1, 1)\n\n\nScissors\n(-1, 1)\n(1, -1)\n(0, 0)\n\n\n\nThe Nash Equilibrium strategy is to play each action \\(r = p = s = 1/3\\) of the time.\n\n\n\n\n\n\nNash Equilibrium Strategy for RPS\n\n\n\n\n\nIf Player 1 plays Rock with probability \\(r\\), Paper with probability \\(p\\), and Scissors with probability \\(s\\), we have the following expected value equations for Player 2:\n\\(\\mathbb{E}(\\text{R}) = -1*p + 1*s\\)\n\\(\\mathbb{E}(\\text{P}) = 1*r - 1*s\\)\n\\(\\mathbb{E}(\\text{S}) = -1*r + 1*p\\)\nSince no action dominates, we know that the EV of every strategic action should be equal (since if a certain strategy was best, we’d want to always play that strategy).\nTo solve for \\(r\\), \\(p\\), and \\(s\\), we can start by setting these EVs equal:\n\\(\\mathbb{E}(\\text{R}) = \\mathbb{E}(\\text{P})\\)\n\\(-1*p + 1*s = 1*r - 1*s\\)\n\\(2*s = p + r\\)\nThen setting these equal:\n\\(\\mathbb{E}(\\text{R}) = \\mathbb{E}(\\text{S})\\)\n\\(-1*p + 1*s = -1*r + 1*p\\)\n\\(s + r = 2*p\\)\nAnd finally setting these equal:\n\\(\\mathbb{E}(\\text{P}) = \\mathbb{E}(\\text{S})\\)\n\\(1*r - 1*s = -1*r + 1*p\\)\n\\(2*r = s + p\\)\nNow we have these equations:\n\\[\n\\begin{cases}\n2s = p + r \\\\\ns + r = 2p \\\\\n2r = s + p\n\\end{cases}\n\\]\nWe can rewrite the 1st:\n\\(r = 2*s - p\\)\nAnd combine with the 2nd:\n\\(s + (2*s - p) = 2*p\\)\n\\(3*s = 3*p\\)\nResulting in:\n\\(s = p\\)\nNow we can go back to the 2nd equation:\n\\(s + r = 2*p\\)\nAnd insert \\(s\\) = \\(p\\):\n\\(s + r = 2*s\\)\nAnd arrive at:\n\\(r = s\\)\nWe now see that all are equal:\n\\(s = p = r\\)\nWe also know that they must all sum to \\(1\\):\n\\(r + p + s = 1\\)\nSince they’re all equal and sum to \\(1\\), we can substitute \\(p\\) and \\(s\\) with \\(r\\):\n\\(3*r = 1\\)\n\\(r = 1/3\\)\nSo all actions are taken with probability \\(1/3\\):\n\\(r = p = s = 1/3 \\quad \\blacksquare\\)\n\n\n\nPlaying this strategy means that whatever your opponent does, you will breakeven! For example, think about an opponent that always plays Rock.\n\\[\n\\begin{equation}\n\\begin{split}\n\\mathbb{E}(\\text{Equilibrium vs. Rock}) &= r*0 + p*1 + s*-1 \\\\\n&= 1/3*0 + 1/3*1 + 1/3*-1 \\\\\n&= 0\n\\end{split}\n\\end{equation}\n\\]\nYou can see this interactively below:\n\n\n\n\nAdapting to Opponents\n\n\nPlay more like Lisa\nSuppose that your opponent always plays Rock. What’s the best counter-strategy? To always play Paper!\nInstead of breaking even with the equilibrium strategy, you’d now be gaining \\(1\\) every game by always winning.\nThe goal of the hackathon is to figure out how to adapt to your opponents! You can try playing a few games against the bot on this site, which as of writing has won/tied/lost 45%/27%/28%."
  },
  {
    "objectID": "aipcs24/2leduc_class.html",
    "href": "aipcs24/2leduc_class.html",
    "title": "#2: Leduc Poker | Class Materials",
    "section": "",
    "text": "Typically solving a 2-player poker game is defined as finding a Nash equilibrium strategy for both players. This is straightforward to define and has been the target of much poker research, but means finding a fixed strategy that doesn’t adapt to opponents and might not be the most profitable strategy in a field of a variety of opponents.\nSmall poker games can be solved through linear programming given a matrix of strategies at each information set and a matrix of payoffs (see here for more details).\nAs we prepare to solve larger games, we start to look at iterative algorithms.\nThe core feature of the iterative algorithms is self-play by traversing the game tree over all infosets and tracking the strategies and regrets at each.\n\n\nRegret is a measure of how much each strategy at an infoset is preferred and is used as a way to update strategies.\nFor a given P1 strategy and P2 strategy, a player has regret when they take an action at an infoset that was not the highest-EV action at that infoset.\n\n\n\n\n\n\nRegret Exercise\n\n\n\n\nWhat is the regret for each action?\n\n\n\nAction\nRegret\n\n\n\n\nA\n\n\n\nB\n\n\n\nC\n\n\n\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\n\nAction\nRegret\n\n\n\n\nA\n4\n\n\nB\n2\n\n\nC\n0\n\n\n\n\n\n\n\n\n\n\n\n\nExpected Value Exercise\n\n\n\n\nIf taking a uniform strategy at this node (i.e. \\(\\frac{1}{3}\\) for each action), then what is the expected value of the node?\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\\(\\mathbb{E} = \\frac{1}{3}*1 + \\frac{1}{3}*3+\\frac{1}{3}*5 = 0.33+1+1.67 = 3\\)\n\n\n\n\n\n\n\n\n\nPoker Regret Exercise\n\n\n\n\nIn poker games, the regret for each action is defined as the value for that action minus the expected value of the node. Give the regret values for each action under this definition.\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\n\nAction\nValue\nPoker Regret\n\n\n\n\nA\n1\n-2\n\n\nB\n3\n0\n\n\nC\n5\n2\n\n\n\nAt a node in a poker game, the player prefers actions with higher regrets by this definition.\n\n\n\n\n\n\nThe most popular method for iteratively solving poker games is the Counterfactual Regret Minimization (CFR) algorithm. A good resource on CFR is this 2015 paper from the University of Alberta.\nThe handout solver does not exactly use CFR. You can make updates to the solver however you would like, including modifying it to become CFR.\nWhat is a counterfactual?\nActual event: I didn’t bring an umbrella, and I got wet in the rain\nCounterfactual event: If I had brought an umbrella, I wouldn’t have gotten wet\n\n\n\nTabular storing strategies and regrets at each infoset\nRegrets based on action values compared to node EV, which is based on counterfactual values\nRegret minimization, usually regret matching, to get new strategies\nAverage strategy converges to Nash equilibrium\nCFR+ variation such that regrets can’t be &lt;0\nLinear CFR such that regrets are weighted by their recency\nSampling methods\n\nExternal: Sample chance and opponent nodes\nChance: Sample chance only\nOutcome: Sample outcomes",
    "crumbs": [
      "About",
      "Challenges",
      "#2: Leduc Poker",
      "Class Materials"
    ]
  },
  {
    "objectID": "aipcs24/2leduc_class.html#solving-poker-games",
    "href": "aipcs24/2leduc_class.html#solving-poker-games",
    "title": "#2: Leduc Poker | Class Materials",
    "section": "",
    "text": "Typically solving a 2-player poker game is defined as finding a Nash equilibrium strategy for both players. This is straightforward to define and has been the target of much poker research, but means finding a fixed strategy that doesn’t adapt to opponents and might not be the most profitable strategy in a field of a variety of opponents.\nSmall poker games can be solved through linear programming given a matrix of strategies at each information set and a matrix of payoffs (see here for more details).\nAs we prepare to solve larger games, we start to look at iterative algorithms.\nThe core feature of the iterative algorithms is self-play by traversing the game tree over all infosets and tracking the strategies and regrets at each.\n\n\nRegret is a measure of how much each strategy at an infoset is preferred and is used as a way to update strategies.\nFor a given P1 strategy and P2 strategy, a player has regret when they take an action at an infoset that was not the highest-EV action at that infoset.\n\n\n\n\n\n\nRegret Exercise\n\n\n\n\nWhat is the regret for each action?\n\n\n\nAction\nRegret\n\n\n\n\nA\n\n\n\nB\n\n\n\nC\n\n\n\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\n\nAction\nRegret\n\n\n\n\nA\n4\n\n\nB\n2\n\n\nC\n0\n\n\n\n\n\n\n\n\n\n\n\n\nExpected Value Exercise\n\n\n\n\nIf taking a uniform strategy at this node (i.e. \\(\\frac{1}{3}\\) for each action), then what is the expected value of the node?\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\\(\\mathbb{E} = \\frac{1}{3}*1 + \\frac{1}{3}*3+\\frac{1}{3}*5 = 0.33+1+1.67 = 3\\)\n\n\n\n\n\n\n\n\n\nPoker Regret Exercise\n\n\n\n\nIn poker games, the regret for each action is defined as the value for that action minus the expected value of the node. Give the regret values for each action under this definition.\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\n\nAction\nValue\nPoker Regret\n\n\n\n\nA\n1\n-2\n\n\nB\n3\n0\n\n\nC\n5\n2\n\n\n\nAt a node in a poker game, the player prefers actions with higher regrets by this definition.\n\n\n\n\n\n\nThe most popular method for iteratively solving poker games is the Counterfactual Regret Minimization (CFR) algorithm. A good resource on CFR is this 2015 paper from the University of Alberta.\nThe handout solver does not exactly use CFR. You can make updates to the solver however you would like, including modifying it to become CFR.\nWhat is a counterfactual?\nActual event: I didn’t bring an umbrella, and I got wet in the rain\nCounterfactual event: If I had brought an umbrella, I wouldn’t have gotten wet\n\n\n\nTabular storing strategies and regrets at each infoset\nRegrets based on action values compared to node EV, which is based on counterfactual values\nRegret minimization, usually regret matching, to get new strategies\nAverage strategy converges to Nash equilibrium\nCFR+ variation such that regrets can’t be &lt;0\nLinear CFR such that regrets are weighted by their recency\nSampling methods\n\nExternal: Sample chance and opponent nodes\nChance: Sample chance only\nOutcome: Sample outcomes",
    "crumbs": [
      "About",
      "Challenges",
      "#2: Leduc Poker",
      "Class Materials"
    ]
  },
  {
    "objectID": "aipcs24/2leduc_reading.html",
    "href": "aipcs24/2leduc_reading.html",
    "title": "#2: Leduc Poker | Class Materials",
    "section": "",
    "text": "Typically solving a 2-player poker game is defined as finding a Nash equilibrium strategy for both players. This is straightforward to define and has been the target of much poker research, but means finding a fixed strategy that doesn’t adapt to opponents and might not be the most profitable strategy in a field of a variety of opponents.\nSmall poker games can be solved through linear programming given a matrix of strategies at each information set and a matrix of payoffs (see here for more details).\nAs we prepare to solve larger games, we start to look at iterative algorithms.\nThe core feature of the iterative algorithms is self-play by traversing the game tree over all infosets and tracking the strategies and regrets at each.\n\n\nRegret is a measure of how much each strategy at an infoset is preferred and is used as a way to update strategies.\nFor a given P1 strategy and P2 strategy, a player has regret when they take an action at an infoset that was not the highest-EV action at that infoset.\n\n\n\n\n\n\nRegret Exercise\n\n\n\n\nWhat is the regret for each action?\n\n\n\nAction\nRegret\n\n\n\n\nA\n\n\n\nB\n\n\n\nC\n\n\n\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\n\nAction\nRegret\n\n\n\n\nA\n4\n\n\nB\n2\n\n\nC\n0\n\n\n\n\n\n\n\n\n\n\n\n\nExpected Value Exercise\n\n\n\n\nIf taking a uniform strategy at this node (i.e. \\(\\frac{1}{3}\\) for each action), then what is the expected value of the node?\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\\(\\mathbb{E} = \\frac{1}{3}*1 + \\frac{1}{3}*3+\\frac{1}{3}*5 = 0.33+1+1.67 = 3\\)\n\n\n\n\n\n\n\n\n\nPoker Regret Exercise\n\n\n\n\nIn poker games, the regret for each action is defined as the value for that action minus the expected value of the node. Give the regret values for each action under this definition.\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\n\nAction\nValue\nPoker Regret\n\n\n\n\nA\n1\n-2\n\n\nB\n3\n0\n\n\nC\n5\n2\n\n\n\nAt a node in a poker game, the player prefers actions with higher regrets by this definition.\n\n\n\n\n\n\nThe most popular method for iteratively solving poker games is the Counterfactual Regret Minimization (CFR) algorithm.\nThe handout solver does not exactly use CFR. You can make updates to the solver however you would like, including modifying it to become CFR.\nWhat is a counterfactual?\nActual event: I didn’t bring an umbrella, and I got wet in the rain\nCounterfactual event: If I had brought an umbrella, I wouldn’t have gotten wet\n\n\n\ngeneral algorithm\n\nstore: strategy, regret\nregret matching\naverage strategy at end\nCFR+, Linear CFR\nsampling (external, sampling, chance, etc. )"
  },
  {
    "objectID": "aipcs24/2leduc_reading.html#solving-poker-games",
    "href": "aipcs24/2leduc_reading.html#solving-poker-games",
    "title": "#2: Leduc Poker | Class Materials",
    "section": "",
    "text": "Typically solving a 2-player poker game is defined as finding a Nash equilibrium strategy for both players. This is straightforward to define and has been the target of much poker research, but means finding a fixed strategy that doesn’t adapt to opponents and might not be the most profitable strategy in a field of a variety of opponents.\nSmall poker games can be solved through linear programming given a matrix of strategies at each information set and a matrix of payoffs (see here for more details).\nAs we prepare to solve larger games, we start to look at iterative algorithms.\nThe core feature of the iterative algorithms is self-play by traversing the game tree over all infosets and tracking the strategies and regrets at each.\n\n\nRegret is a measure of how much each strategy at an infoset is preferred and is used as a way to update strategies.\nFor a given P1 strategy and P2 strategy, a player has regret when they take an action at an infoset that was not the highest-EV action at that infoset.\n\n\n\n\n\n\nRegret Exercise\n\n\n\n\nWhat is the regret for each action?\n\n\n\nAction\nRegret\n\n\n\n\nA\n\n\n\nB\n\n\n\nC\n\n\n\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\n\nAction\nRegret\n\n\n\n\nA\n4\n\n\nB\n2\n\n\nC\n0\n\n\n\n\n\n\n\n\n\n\n\n\nExpected Value Exercise\n\n\n\n\nIf taking a uniform strategy at this node (i.e. \\(\\frac{1}{3}\\) for each action), then what is the expected value of the node?\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\\(\\mathbb{E} = \\frac{1}{3}*1 + \\frac{1}{3}*3+\\frac{1}{3}*5 = 0.33+1+1.67 = 3\\)\n\n\n\n\n\n\n\n\n\nPoker Regret Exercise\n\n\n\n\nIn poker games, the regret for each action is defined as the value for that action minus the expected value of the node. Give the regret values for each action under this definition.\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\n\nAction\nValue\nPoker Regret\n\n\n\n\nA\n1\n-2\n\n\nB\n3\n0\n\n\nC\n5\n2\n\n\n\nAt a node in a poker game, the player prefers actions with higher regrets by this definition.\n\n\n\n\n\n\nThe most popular method for iteratively solving poker games is the Counterfactual Regret Minimization (CFR) algorithm.\nThe handout solver does not exactly use CFR. You can make updates to the solver however you would like, including modifying it to become CFR.\nWhat is a counterfactual?\nActual event: I didn’t bring an umbrella, and I got wet in the rain\nCounterfactual event: If I had brought an umbrella, I wouldn’t have gotten wet\n\n\n\ngeneral algorithm\n\nstore: strategy, regret\nregret matching\naverage strategy at end\nCFR+, Linear CFR\nsampling (external, sampling, chance, etc. )"
  },
  {
    "objectID": "aipcs24/5nlhe_reading.html#deep-cfr",
    "href": "aipcs24/5nlhe_reading.html#deep-cfr",
    "title": "#5: No Limit Hold’em | Reading",
    "section": "Deep CFR",
    "text": "Deep CFR\nA more modern way to make a large game tractable is through deep learning."
  },
  {
    "objectID": "aipcs24/5nlhe_reading.html#evaluating-agents",
    "href": "aipcs24/5nlhe_reading.html#evaluating-agents",
    "title": "#5: No Limit Hold’em | Reading",
    "section": "Evaluating Agents",
    "text": "Evaluating Agents\nThere are four main ways to evaluate poker agents:\n\nPlay against humans https://en.wikipedia.org/wiki/Libratus https://en.wikipedia.org/wiki/Pluribus_(poker_bot)\nPlay against other agents ACPC AAAI conference\nPlay against a Nash equilibrium agent\nPlay against a best response agent"
  },
  {
    "objectID": "aipcs24/1kuhn_challenge2.html",
    "href": "aipcs24/1kuhn_challenge2.html",
    "title": "#1: Kuhn Poker | Challenge (Part 2)",
    "section": "",
    "text": "This section is locked until you complete part 1.\n\n\n\n\n\n\n\n\nChallenge part 2: submission and leaderboard\n\n\n\n\nThe strategy that you used to unlock this stage has been submitted to the leaderboard; you can see it as name [undefined] below. You can resubmit to replace it with another strategy as many times as you like using the “Submit” button below. The preceding sentences will be true when the challenge goes live, expect Tuesday or Wednesday.\n\nNext, use the automatic solver tool below to refine your strategies for each player in terms of the fixed action probabilities at each player’s infosets. For this challenge, we recommend that you submit strategies that form a Nash equilibrium (or equivalently, a pair of strategies such that neither player has regret).\nOnce you have a strategy that you believe improves on your current submission, you can re-submit and wait for the results to be re-run (which may take some minutes).\n\n\n\n\n\n\n\n\n\n\n\nSolver controls\n\n\n\n\n\n\nThe site should save your progress if you navigate away or refresh, though might lose the last few edits, depending.\nIt doesn’t have any help for sharing solutions between teammates, sorry.\nAt each iteration, the solver will update all 12 nodes (in some arbitrary order), using a rule modified by the update parameters:\n\na magnitude multiplier\nhow to scale the update based on EV (currently supports: no effect or linearly)\nhow to scale the update based on the infoset’s visit probability (currently supports: no effect or linearly)\nwhether to use a learning rate to decay the magnitude over time (currently supports: no decay, or linear in the sum of updates made to this infoset since reset)\nwhether to update probability or odds\n\nYou can set it to run for a number of iterations. (If you accidentally set it to too many, you can stop the solver by pressing “Stop” or by reloading the page.)\nSpeed is hopefully self-explanatory.\nTolerance controls the difference between action EVs that is too small to update on.\n\n\n\n\n\n\nUpdate:  100% 10% 1% 0.1% 0.01% 0.001%   ×1 ×EVdiff   ×1 ×p(visit)   ×1 /Σupdates   probability odds \n\n\n\nRun Solver\n\n\nIterations:  1 10 100 1,000 10,000 100,000 \n\n\nSpeed:  Max Fast 10/sec 3/sec 1/sec \n\n\nTolerance:  0.30 chip 0.10 chip 0.03 chip 0.01 chip 0.003 chip \n\n\n\n\n\n\n\n\n\n\n\n\n\nSolver history\n\n\n\n\n\n\nLog:  (this slows the solver down somewhat)\n\n\n\n\n📋\n\n\n\n\nStrategy\n\n\nA_\n\n\nK_\n\n\nQ_\n\n\n_A↑\n\n\n_A↓\n\n\n_K↑\n\n\n_K↓\n\n\n_Q↑\n\n\n_Q↓\n\n\nA_↓↑\n\n\nK_↓↑\n\n\nQ_↓↑\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEV\n\n\nA_\n\n\nK_\n\n\nQ_\n\n\n_A↑\n\n\n_A↓\n\n\n_K↑\n\n\n_K↓\n\n\n_Q↑\n\n\n_Q↓\n\n\nA_↓↑\n\n\nK_↓↑\n\n\nQ_↓↑\n\n\n\n\n\n\n\n\n\n\n\nLog all:  (this slows the solver down significantly)\n\n\n\n\n\n\n\nOther logs\n\n\n\n\n\n\n\n\n\n\n\n\n\nEV(action=↑Up)\n\n\nA_\n\n\nK_\n\n\nQ_\n\n\n_A↑\n\n\n_A↓\n\n\n_K↑\n\n\n_K↓\n\n\n_Q↑\n\n\n_Q↓\n\n\nA_↓↑\n\n\nK_↓↑\n\n\nQ_↓↑\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEV(action=↓Down)\n\n\nA_\n\n\nK_\n\n\nQ_\n\n\n_A↑\n\n\n_A↓\n\n\n_K↑\n\n\n_K↓\n\n\n_Q↑\n\n\n_Q↓\n\n\nA_↓↑\n\n\nK_↓↑\n\n\nQ_↓↑\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nVisit Prob\n\n\nA_\n\n\nK_\n\n\nQ_\n\n\n_A↑\n\n\n_A↓\n\n\n_K↑\n\n\n_K↓\n\n\n_Q↑\n\n\n_Q↓\n\n\nA_↓↑\n\n\nK_↓↑\n\n\nQ_↓↑\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTotal Updates\n\n\nA_\n\n\nK_\n\n\nQ_\n\n\n_A↑\n\n\n_A↓\n\n\n_K↑\n\n\n_K↓\n\n\n_Q↑\n\n\n_Q↓\n\n\nA_↓↑\n\n\nK_↓↑\n\n\nQ_↓↑\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPonder…\n\n\n\nThe solver’s algorithm is a reinforcement-learning approach that you will use some version of for the rest of the course. Unfortunately, the vanilla version that this page defaults to doesn’t converge.\nThe solver controls will let you tweak how the algorithm determines the size of the updates, which is critical to having the convergence behavior you want. Try to find a setting of the controls that converges to a good solution.\nFor this task you will almost certainly want to look at the history of updates represented in the “Solver history” box above.\nWhile Kuhn Poker is small enough to solve by hand or by manual trial-and-error, having an efficient and effective (and converging!) algorithm for learning better strategies is going to be key in later weeks.\n\n\n\n\n\n\n\n\nBeta Note\n\n\n\nThis week’s challenge (submit a strategy) has drifted apart somewhat from the lesson we tried to build up to (exploring the nuances of reinforcement-update algorithms). Our current thinking is that this would be better if the challenge were actually to submit 100-card Kuhn Poker, which would do a better job of applying the answer to “how does a good solver update?”\nUnfortunately, we ran out of time to implement the 100-card Kuhn Poker train / test tournament.\n\n\n\n\n\n\n\n\n\n\nBeta Note\n\n\n\nRe-running the tournament currently takes between one and three minutes, and there’s no indication that it’s done except the board changing. In some cases, you may have to refresh the page to see changes. Working on improvements.\n\n\n\n\nUpdate Strategy Submission\n\n\n\n\n\n\n\n\n\n\n\n\nMore →\n\n\n\n\n\nIf you’ve finished all of the above, we’d like to hear about it (and any questions you still have—which we expect you do. Then you can do any of:\n\nWait for next week’s material next week.\nHelp other students with their confusions and stuck points (and let us know how we could have improved!).\nGet a start on the next segment of the course by writing a bot that can learn from your opponent’s moves and do better than Nash against them. (For this, see the instructors for info on setting up the games environment on your own computer.)"
  },
  {
    "objectID": "aipcs24/1kuhn_challenge2.html#challenge-part-2-automatic-solver",
    "href": "aipcs24/1kuhn_challenge2.html#challenge-part-2-automatic-solver",
    "title": "#1: Kuhn Poker | Challenge (Part 2)",
    "section": "",
    "text": "This section is locked until you complete part 1.\n\n\n\n\n\n\n\n\nChallenge part 2: submission and leaderboard\n\n\n\n\nThe strategy that you used to unlock this stage has been submitted to the leaderboard; you can see it as name [undefined] below. You can resubmit to replace it with another strategy as many times as you like using the “Submit” button below. The preceding sentences will be true when the challenge goes live, expect Tuesday or Wednesday.\n\nNext, use the automatic solver tool below to refine your strategies for each player in terms of the fixed action probabilities at each player’s infosets. For this challenge, we recommend that you submit strategies that form a Nash equilibrium (or equivalently, a pair of strategies such that neither player has regret).\nOnce you have a strategy that you believe improves on your current submission, you can re-submit and wait for the results to be re-run (which may take some minutes).\n\n\n\n\n\n\n\n\n\n\n\nSolver controls\n\n\n\n\n\n\nThe site should save your progress if you navigate away or refresh, though might lose the last few edits, depending.\nIt doesn’t have any help for sharing solutions between teammates, sorry.\nAt each iteration, the solver will update all 12 nodes (in some arbitrary order), using a rule modified by the update parameters:\n\na magnitude multiplier\nhow to scale the update based on EV (currently supports: no effect or linearly)\nhow to scale the update based on the infoset’s visit probability (currently supports: no effect or linearly)\nwhether to use a learning rate to decay the magnitude over time (currently supports: no decay, or linear in the sum of updates made to this infoset since reset)\nwhether to update probability or odds\n\nYou can set it to run for a number of iterations. (If you accidentally set it to too many, you can stop the solver by pressing “Stop” or by reloading the page.)\nSpeed is hopefully self-explanatory.\nTolerance controls the difference between action EVs that is too small to update on.\n\n\n\n\n\n\nUpdate:  100% 10% 1% 0.1% 0.01% 0.001%   ×1 ×EVdiff   ×1 ×p(visit)   ×1 /Σupdates   probability odds \n\n\n\nRun Solver\n\n\nIterations:  1 10 100 1,000 10,000 100,000 \n\n\nSpeed:  Max Fast 10/sec 3/sec 1/sec \n\n\nTolerance:  0.30 chip 0.10 chip 0.03 chip 0.01 chip 0.003 chip \n\n\n\n\n\n\n\n\n\n\n\n\n\nSolver history\n\n\n\n\n\n\nLog:  (this slows the solver down somewhat)\n\n\n\n\n📋\n\n\n\n\nStrategy\n\n\nA_\n\n\nK_\n\n\nQ_\n\n\n_A↑\n\n\n_A↓\n\n\n_K↑\n\n\n_K↓\n\n\n_Q↑\n\n\n_Q↓\n\n\nA_↓↑\n\n\nK_↓↑\n\n\nQ_↓↑\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEV\n\n\nA_\n\n\nK_\n\n\nQ_\n\n\n_A↑\n\n\n_A↓\n\n\n_K↑\n\n\n_K↓\n\n\n_Q↑\n\n\n_Q↓\n\n\nA_↓↑\n\n\nK_↓↑\n\n\nQ_↓↑\n\n\n\n\n\n\n\n\n\n\n\nLog all:  (this slows the solver down significantly)\n\n\n\n\n\n\n\nOther logs\n\n\n\n\n\n\n\n\n\n\n\n\n\nEV(action=↑Up)\n\n\nA_\n\n\nK_\n\n\nQ_\n\n\n_A↑\n\n\n_A↓\n\n\n_K↑\n\n\n_K↓\n\n\n_Q↑\n\n\n_Q↓\n\n\nA_↓↑\n\n\nK_↓↑\n\n\nQ_↓↑\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEV(action=↓Down)\n\n\nA_\n\n\nK_\n\n\nQ_\n\n\n_A↑\n\n\n_A↓\n\n\n_K↑\n\n\n_K↓\n\n\n_Q↑\n\n\n_Q↓\n\n\nA_↓↑\n\n\nK_↓↑\n\n\nQ_↓↑\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nVisit Prob\n\n\nA_\n\n\nK_\n\n\nQ_\n\n\n_A↑\n\n\n_A↓\n\n\n_K↑\n\n\n_K↓\n\n\n_Q↑\n\n\n_Q↓\n\n\nA_↓↑\n\n\nK_↓↑\n\n\nQ_↓↑\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTotal Updates\n\n\nA_\n\n\nK_\n\n\nQ_\n\n\n_A↑\n\n\n_A↓\n\n\n_K↑\n\n\n_K↓\n\n\n_Q↑\n\n\n_Q↓\n\n\nA_↓↑\n\n\nK_↓↑\n\n\nQ_↓↑\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPonder…\n\n\n\nThe solver’s algorithm is a reinforcement-learning approach that you will use some version of for the rest of the course. Unfortunately, the vanilla version that this page defaults to doesn’t converge.\nThe solver controls will let you tweak how the algorithm determines the size of the updates, which is critical to having the convergence behavior you want. Try to find a setting of the controls that converges to a good solution.\nFor this task you will almost certainly want to look at the history of updates represented in the “Solver history” box above.\nWhile Kuhn Poker is small enough to solve by hand or by manual trial-and-error, having an efficient and effective (and converging!) algorithm for learning better strategies is going to be key in later weeks.\n\n\n\n\n\n\n\n\nBeta Note\n\n\n\nThis week’s challenge (submit a strategy) has drifted apart somewhat from the lesson we tried to build up to (exploring the nuances of reinforcement-update algorithms). Our current thinking is that this would be better if the challenge were actually to submit 100-card Kuhn Poker, which would do a better job of applying the answer to “how does a good solver update?”\nUnfortunately, we ran out of time to implement the 100-card Kuhn Poker train / test tournament.\n\n\n\n\n\n\n\n\n\n\nBeta Note\n\n\n\nRe-running the tournament currently takes between one and three minutes, and there’s no indication that it’s done except the board changing. In some cases, you may have to refresh the page to see changes. Working on improvements.\n\n\n\n\nUpdate Strategy Submission\n\n\n\n\n\n\n\n\n\n\n\n\nMore →\n\n\n\n\n\nIf you’ve finished all of the above, we’d like to hear about it (and any questions you still have—which we expect you do. Then you can do any of:\n\nWait for next week’s material next week.\nHelp other students with their confusions and stuck points (and let us know how we could have improved!).\nGet a start on the next segment of the course by writing a bot that can learn from your opponent’s moves and do better than Nash against them. (For this, see the instructors for info on setting up the games environment on your own computer.)"
  },
  {
    "objectID": "aipcs24/3rps_cfr.html",
    "href": "aipcs24/3rps_cfr.html",
    "title": "#3 Rock Paper Scissors: CFR",
    "section": "",
    "text": "As we think about solving larger games, we start to look at iterative algorithms.\nThe most popular method for iteratively solving poker games is the Counterfactual Regret Minimization (CFR) algorithm. CFR is an iterative algorithm developed in 2007 at the University of Alberta that converges to Nash equilibrium in two player zero-sum games.\nThe handout solver does not exactly use CFR. You can make updates to the solver however you would like, including modifying it to become CFR.\nWhat is a counterfactual?\nActual event: I didn’t bring an umbrella, and I got wet in the rain\nCounterfactual event: If I had brought an umbrella, I wouldn’t have gotten wet"
  },
  {
    "objectID": "aipcs24/3rps_cfr.html#counterfactual-regret-minimization",
    "href": "aipcs24/3rps_cfr.html#counterfactual-regret-minimization",
    "title": "#3 Rock Paper Scissors: CFR",
    "section": "",
    "text": "As we think about solving larger games, we start to look at iterative algorithms.\nThe most popular method for iteratively solving poker games is the Counterfactual Regret Minimization (CFR) algorithm. CFR is an iterative algorithm developed in 2007 at the University of Alberta that converges to Nash equilibrium in two player zero-sum games.\nThe handout solver does not exactly use CFR. You can make updates to the solver however you would like, including modifying it to become CFR.\nWhat is a counterfactual?\nActual event: I didn’t bring an umbrella, and I got wet in the rain\nCounterfactual event: If I had brought an umbrella, I wouldn’t have gotten wet"
  },
  {
    "objectID": "aipcs24/3rps_cfr.html#cfr-algorithm-parts",
    "href": "aipcs24/3rps_cfr.html#cfr-algorithm-parts",
    "title": "#3 Rock Paper Scissors: CFR",
    "section": "CFR Algorithm Parts",
    "text": "CFR Algorithm Parts\n\nRegret and Strategies\nA strategy at an infoset is a probability distribution over each possible action.\nRegret is a measure of how much each strategy at an infoset is preferred and is used as a way to update strategies.\nFor a given P1 strategy and P2 strategy, a player has regret when they take an action at an infoset that was not the highest-EV action at that infoset.\n\n\n\n\n\n\nRegret Exercise\n\n\n\n\nWhat is the regret for each action?\n\n\n\nAction\nRegret\n\n\n\n\nA\n\n\n\nB\n\n\n\nC\n\n\n\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\n\nAction\nRegret\n\n\n\n\nA\n4\n\n\nB\n2\n\n\nC\n0\n\n\n\n\n\n\n\n\n\n\n\n\nExpected Value Exercise\n\n\n\n\nIf taking a uniform strategy at this node (i.e. \\(\\frac{1}{3}\\) for each action), then what is the expected value of the node?\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\\(\\mathbb{E} = \\frac{1}{3}*1 + \\frac{1}{3}*3+\\frac{1}{3}*5 = 0.33+1+1.67 = 3\\)\n\n\n\n\n\n\n\n\n\nPoker Regret Exercise\n\n\n\n\nIn poker games, the regret for each action is defined as the value for that action minus the expected value of the node. Give the regret values for each action under this definition.\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\n\nAction\nValue\nPoker Regret\n\n\n\n\nA\n1\n-2\n\n\nB\n3\n0\n\n\nC\n5\n2\n\n\n\nAt a node in a poker game, the player prefers actions with higher regrets by this definition.\n\n\n\nEach infoset maintains a strategy and regret tabular counter for each action. These accumulate the sum of all strategies and the sum of all regrets.\nIn a game like Rock Paper Scissors, there is effectively only one infoset, so only one table for strategy over each action (Rock, Paper, Scissors) and one table for regret over each action (Rock, Paper, Scissors).\nRegrets are linked to strategies through a policy called regret matching.\n\nRegret matching\n\nAverage strategy at end\n\n\n\nIterating through the Tree\nThe core feature of the iterative algorithms is self-play by traversing the game tree over all infosets and tracking the strategies and regrets at each.\ncounterfactual values\n\n\nAlternatives to Original Algorithm\n\nCFR+\nLinear CFR\nSampling\nExternal\nChance\nOutcome\nTabular storing strategies and regrets at each infoset\nRegrets based on action values compared to node EV, which is based on counterfactual values\nRegret minimization, usually regret matching, to get new strategies\nAverage strategy converges to Nash equilibrium\nCFR+ variation such that regrets can’t be &lt;0\nLinear CFR such that regrets are weighted by their recency\nSampling methods\n\nExternal: Sample chance and opponent nodes\nChance: Sample chance only\nOutcome: Sample outcomes"
  },
  {
    "objectID": "aipcs24/index.html",
    "href": "aipcs24/index.html",
    "title": "AI Poker Camp Summer 2024",
    "section": "",
    "text": "Welcome to AI Poker Camp Summer 2024 Beta in San Francisco!\n\nThanks for signing up. This is the first time we’re running AI Poker Camp and it’s very much in beta. The idea to run this course has existed for a few years, but this curriculum is very new and came about from a discussion at Manifest.\nWe’re glad to have you along for the ride and welcome any feedback about all aspects of the course including things like whether the pace is too fast or too slow, too easy or too hard, etc.\nWe plan to spend Mondays reviewing previous challenges and thinking about new challenges/adjacent problems and Thursdays working on the challenges, which will be due every Sunday.\nOur goal is to develop your intuitions around decision making and problem solving by engaging with challenges in fun and non-idealized scenarios."
  },
  {
    "objectID": "aipcs24/1kuhn_challenge.html",
    "href": "aipcs24/1kuhn_challenge.html",
    "title": "#1: Kuhn Poker | Challenge (Part 1)",
    "section": "",
    "text": "Enter Part 2 (unlocked) →",
    "crumbs": [
      "About",
      "Challenges",
      "#1: Kuhn Poker",
      "Challenge"
    ]
  },
  {
    "objectID": "aipcs24/1kuhn_challenge.html#pre-challenge-visualizing-a-game-tree",
    "href": "aipcs24/1kuhn_challenge.html#pre-challenge-visualizing-a-game-tree",
    "title": "#1: Kuhn Poker | Challenge (Part 1)",
    "section": "Pre-Challenge: Visualizing a game tree",
    "text": "Pre-Challenge: Visualizing a game tree\nTo analyze Kuhn Poker further, we’ll represent the game in terms of a visual game tree.\nFirst, deal one card to  and one card to . Each possible deal of the cards forms a separate “game node”. For example, AK means that  has A and  has K:\n\n\n\n will act first. He knows what his card is, but not what card  has, so he can be in one of 3 information sets, or infosets. An infoset is a set of nodes that are indistinguishable to the player, meaning that they don’t know which of the states they are in and will act with the same strategy at all of them. They are identified by the player card and the previous actions and are the labels in bold in the game tree.\nWe’ll name ’s infosets A_, K_, and Q_ based on what card he holds:\n\n\n\nAt each infoset,  can choose ↑Up or ↓Down. Note that the probaility with which he chooses ↑Up (versus ↓Down) from a given infoset will have to be the same for both nodes, since they are indistinguishable to .\n\n\n\nEach ↑Up or ↓Down action will each take us to a distinct game node, based on a unique set of cards and action history:\n\n\n\nNext,  will act. He can observe his card and ’s first action, but not ’s card, so those pieces of information characterize his infoset. We’ll give ’s 6 infosets names like _K↓ and _Q↑ based on his card and the action:\n\n\n\nAt each of these infosets,  can choose ↑Up or ↓Down:\n\n\n\nIf the actions were ↓Down, ↑Up (“check–bet”), then  will have to act again. Otherwise, the game is now over (with a payoff determined by the cards and the action sequence). Recall the 5 sequences of betting, which end either in one player folding (after an ↑Up then ↓Down, a “bet–fold”) or the higher card winning at showdown, which results from either two ↓Downs (“check–check”) or two ↑Ups (“bet–call”).\nWe’ll write the payoffs from ’s perspective, and remember that ’s payoffs will be the inverse:\n\n\n\nIf  still has to act, he’ll be in one of just three infosets, A_↓↑, K_↓↑, or Q_↓↑:\n\n\n\n…and can choose ↑Up or ↓Down…\n\n\n\n…but whatever he chooses, the game will end after that move. We now have the entire game tree for Kuhn Poker:\n\n\n\n\n\n\n\n\n\nExercise: Deterministic Places on Tree\n\n\n\n\nEarlier you found situations where a player should take one action 100% of the time. Find these places on the tree.\nPut a light X on parts of the tree that are blocked off (never played) because of these.\n\n\n\n\n\n\n\n\n\nExercise: Following the Tree\n\n\n\n\nPlay a hand of Kuhn Poker with the cards up and follow along with the tree. Point out where you are with your partner.\nPlay a hand of Kuhn Poker with the cards down. Each individual should track where they could be in the tree. Notice where these possibilities overlap.",
    "crumbs": [
      "About",
      "Challenges",
      "#1: Kuhn Poker",
      "Challenge"
    ]
  },
  {
    "objectID": "aipcs24/1kuhn_challenge.html#kuhn-poker-strategies",
    "href": "aipcs24/1kuhn_challenge.html#kuhn-poker-strategies",
    "title": "#1: Kuhn Poker | Challenge (Part 1)",
    "section": "Kuhn Poker strategies",
    "text": "Kuhn Poker strategies\nEach player’s strategy can be completely described in terms of their action probabilities at each infoset (6 of them per player).\n\nExample 1: Determining a local best response\n\n\n\n\n\n\nBeta Note\n\n\n\nAll of the strategy boxes on the whole page are linked together, which is a bad design on our part. This means that if you go to the next example or the final section and change things, the values in this section will no longer make sense. You can get back to a good state by setting all strategy probs to 50%, or opening this page in an incognito window, or by clearing cache including local storage (which will also clear your unlock progress).\n\n\nLet’s start by considering ’s decision at the infoset K_↓↑. When  is at K_↓↑, he doesn’t know whether the true state of the world is KA↓↑ or KQ↓↑. This is what the game looks like from ’s perspective:\n\nAs we said earlier, whatever action (or randomized mix of actions) ’s strategy says to make, he will be doing so in all of the situations where he arrives at K_↓↑, without the ability to do different things at KA↓↑ vs KQ↓↑.\n\n\n\n\n\n\nIf we start by assuming that both players play 50-50 randomly (50% ↑Up at each infoset), then  would arrive at K_↓↑ via KA↓↑ equally often as via KQ↓↑. In this case, his expected payoff for playing ↑Up is a 50%-50% weighted sum of the payoffs KA↓↑↑ (-2, calling with K when opponent has A and losing at showdown) and KQ↓↑↑ (+2, calling with K when opponent has Q and winning at showdown), for an expected value of 0. The expected value, or EV, of a situation is the value of each outcome weighted by the probability of that outcome.\nSimilarly, his expected payoff for ↓Down is a 50%-50% weighted sum of the payoffs for KA↓↑↓ and KQ↓↑↓, though in this case they’re both -1 and the EV doesn’t depend on the composition weights of the infoset (since he folds and loses exactly one chip either way).\nWith these expected values, ’s at this infoset (holding everything else about both strategies constant) is to play ↑Up 100% of the time.\n\n\n\n\n\n\nQuestion\n\n\n\nIf we hold everything else about both strategies constant at 50-50, what should  do at K_↓↑? (We’ll call this his local best response.)\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nHolding the rest of both strategies constant,  should always play ↑Up (“call”).\n\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nHow much does ’s local best response improve his expected value from the scenario K_↓↑, versus playing 50-50?\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nSwitching from 50% ↑Up to 100% ↑Up improves ’s expected value at K_↓↑ by +0.5 (from -0.5 to 0).\n\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nHolding both strategies at 50-50 everywhere else, how much does ’s local best response improve his expected value of playing the game (versus playing 50-50)?\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nSwitching from 50% ↑Up to 100% ↑Up improves ’s expected value of the whole game by +1/24, or about +0.0417. This combines the previous answer (+0.5) with the probability that any game ends up at K_↓↑ (which is 1/12).\n\n\n\nBut what if the probabilities of reaching K_↓↑ via KA↓↑ versus KQ↓↑ aren’t equal? If the players don’t play 50-50 randomly, then we’ll have to separately calculate the probability of reaching KA↓↑ (including the probability of the initial KA deal) and the probability of reaching KQ↓↑ (likewise); the composition of the infoset will be proportional to the the probabilities of reaching these states—as we’ll see in our next example. (In larger games, we might approximate these reach-state probabilities by sampling games with simulated play instead of calculating them analytically.)\n\n\nExample 2: Changing local best responses\n\n\n\n\n\n\nBeta Note\n\n\n\nThis section also assumes that all other strategy probabilities are set to 50%, including _A↓, though this isn’t clear in the text. Like in the previous section, you may need to reset the probabilities for the other strategy probs if you changed them in the final section.\n\n\nConsider ’s actions at _Q↓ (“having a Q facing a check”). If  plays ↓Down (action sequence: “check–check”), he will go to showdown, always have the worse card, and get a payoff of -1.0 (+1.0). If he plays ↑Up (“bets”), then it will be ’s turn to act.\nIf  were to play randomly at K_↓↑, then ’s ↑Up would get a payoff of -0.5 (+0.5), since half the time  will play ↓Down (“fold”, +1) and half the time he will ↑Up (“call”, showdown, -2).\nNote that ’s Q is always the worse hand and an ↑Up action is a bluff, but if  plays ↓Down (“folds”) often enough, then  can win enough +1s to do better than always taking a 1-chip showdown.\nBut recall that in the previous section, we thought  should always play ↑Up at K_↓↑. In that case, ’s payoff for playing ↑Up at _Q↓ becomes -1.25 (+1.25), worse than -1 for playing ↓Down, and so he should play ↓Down instead of ↑Up.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise: Entangled strategies\n\n\n\nReset the probabilities, then set the K_↓↑ strategy probability to ’s local best response. Then set the _Q↓ strategy prob to ’s local best response. What has happened to ’s local best response?\n\n\n\n\n\n\n\n\nExercise: No-regret mixed strategies\n\n\n\nSet the K_↓↑ and _Q↓ strategy probabilities to a pair of values such that neither player has regret. A player has regret when they take an action at an infoset that was not the highest-EV action at that infoset. The amount of regret is the difference between the highest-EV action and the selected action. So a player will have no regret if they always take the higher-EV action, or if they mix between actions with equally-highest EV.\n\n\nThis example shows the core difference between solving perfect-information games and imperfect-information games. In Kuhn Poker, when a random  switches to only playing ↓Down at _Q↓, ’s payoff EVs at K_↓↑ change (because now that infoset is only composed of KA↓↑ and no KQ↓↑). With the resulting EVs,  has a payoff of -2 for playing ↑Up, and should play ↓Down to get -1 instead. (But now  should…)\nIn a perfect information game, we can find each player’s best action in each game situation inductively, by passing up the tree from the end-states and determining each situation in terms of known solutions to its successor nodes. And whatever the best thing to do at the downtree node was, it’ll still be the best thing to do, regardless of how we get there.\nBut in an imperfect information game, changing an uptree strategy parameter can change what the best response is at downtree infosets, so we can’t solve a game in a single pass from the endgames to the beginning. Nearly every game-solving technique for imperfect information games, then, is based on taking a strategy for each player and iteratively improving on both based on local improvements until they resolve on something like a Nash equilibrium.\nThere are two types of uncertainty in an imperfect-information game:\n\nUncertainty about which node we are actually in (depending on strategy and chance probabilities upwards in the game tree).\nUncertainty about what happens after an action (depending on the strategy and chance probabilites downwards in the game tree).",
    "crumbs": [
      "About",
      "Challenges",
      "#1: Kuhn Poker",
      "Challenge"
    ]
  },
  {
    "objectID": "aipcs24/1kuhn_challenge.html#challenge",
    "href": "aipcs24/1kuhn_challenge.html#challenge",
    "title": "#1: Kuhn Poker | Challenge (Part 1)",
    "section": "Challenge",
    "text": "Challenge\nThe goal of this challenge is to find optimal strategies for Kuhn Poker in terms of the fixed action probabilities at each player’s 6 infosets. For this challenge, we recommend that you submit strategies that form a Nash equilibrium (or equivalently, a pair of strategies such that neither player has regret). You will submit strategies for both players (12 probabilities total) and be matched against many opponents (but not yourself); your average score across all matchups will be the basis of your leaderboard rank. For each opponent, you will play an equal number of times as  and as .\n\nPart 1: Manual Solutions\n\n\n\n\n\n\nChallenge part 1: Kuhn Poker (manual solutions)\n\n\n\nUse the boxes below to find a pair of no-regret strategies in the boxes below for  and .\n\n\nWhen you get a pair of strategies such that each infoset is at most 0.1 chips of EV away from regret-free, a link to the next stage will appear at the bottom of the page. You will be able to fine-tune your strategy in the next stage before submitting it to the leaderboard.\n\n\n\n\n\nRun Solver\n\n\nUpdate mode:  100% CFR 10% CFR 1% CFR 0.1% CFR 0.0001% CFR 0.00001% CFR \n\n\nIterations:  1 10 100 1,000 10,000 100,000 \n\n\nSpeed:  Max Fast 10/sec 1/sec \n\n\nTolerance:  0.01 0.03 0.1 0.3 \n\n\n\n\n\n\n\n\nPart 2: Automatic Solver\n\n\n&lt;This section is locked until you complete part 1.&gt;\n\n\n\n\nEnter Part 2 (unlocked) →",
    "crumbs": [
      "About",
      "Challenges",
      "#1: Kuhn Poker",
      "Challenge"
    ]
  },
  {
    "objectID": "aipcs24/1kuhn_reading.html",
    "href": "aipcs24/1kuhn_reading.html",
    "title": "#1: Kuhn Poker | Reading",
    "section": "",
    "text": "(This game is modified from Lisy et al. 2015.)\n\n\n\nBugs and Fudd\n\n\nWe developed this game to show concepts in games with imperfect information that are applicable to our first challenge. Imperfect information games involve hidden information, which are things like opponent cards in poker or enemy location in the Hunting Wabbits game. In perfect information games like chess, all information is available to both players.\nThis game has one chance player (the Author), one maximizing player (Fudd) and one minimizing player (Bugs). Fudd and Bugs are in a long-term, all-out war, and so any energy that Fudd wastes or saves is a gain or loss for Bugs; our game is zero-sum.\nFirst, the Author will choose whether to write an episode where Bugs is at the Opera (50%) or in the Forest (50%). Fudd cannot tell what the Author has chosen.\nNext, Fudd will decide whether to Hunt_Near or Hunt_Far. If he chooses Hunt_Far, he takes -1 value for the extra effort.\nBugs knows whether the Author wrote him into the Opera or Forest, but he does not know Fudd’s hunting location. If the Author gives him the Opera, Bugs has no choices to make and the game ends after Fudd’s action. If Forest, Bugs will decide whether to Play_Near or Play_Far. If he plays in the same location as Fudd is hunting, Fudd gets +3 value for a successful hunt (for a total of +2 in the Hunt_Far action due to the -1 value for the extra effort). If they are in different locations (or if Bugs is at the Opera), Fudd will get 0 value for an unsuccessful hunt (-1 for the Hunt_Far misses).\nPutting it all together, the payoff structure of this game is:\n\n\n\nAuthor\nBugs/Fudd\nHunt_Near\nHunt_Far\n\n\n\n\nOpera\n\n0, 0\n+1, -1\n\n\nForest\nPlay_Near\n-3, +3\n+1, -1\n\n\nForest\nPlay_Far\n0, 0\n-2, +2\n\n\n\nThe tree structure is as follows with the payoffs written from the perspective of Fudd:\n\nIf Fudd knows he’s at the Opera, then he must prefer to Hunt_Near to get a value of \\(0\\) instead of \\(-1\\) for Hunt_Far, but since he doesn’t know his location, he must take both scenarios into account.\nNote that Bugs’s optimal actions depend on Fudd’s Opera strategy even though that outcome cannot be reached once Bugs is playing since Bugs only plays in the Forest! For example if we kept the same game tree except Fudd had a \\(+100\\) Opera Hunt_Far payoff, then he would always Hunt_Far. Bugs would see this and it would affect how Bugs plays in the Forest scenario.\n\n\n\n\n\n\nExercise\n\n\n\nHow would Bugs play in the Forest scenario knowing that Fudd is always playing Hunt_Far?\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nBugs would always Play_Near because then his payoff in the only scenario he can control would be \\(+1\\), whereas Play_Far would get him a payout of \\(-2\\). (The payoffs are all written from the perspective of Fudd, so Bugs’s are opposite.)",
    "crumbs": [
      "About",
      "Challenges",
      "#1: Kuhn Poker",
      "Optional Reading"
    ]
  },
  {
    "objectID": "aipcs24/1kuhn_reading.html#hunting-wabbits-game",
    "href": "aipcs24/1kuhn_reading.html#hunting-wabbits-game",
    "title": "#1: Kuhn Poker | Reading",
    "section": "",
    "text": "(This game is modified from Lisy et al. 2015.)\n\n\n\nBugs and Fudd\n\n\nWe developed this game to show concepts in games with imperfect information that are applicable to our first challenge. Imperfect information games involve hidden information, which are things like opponent cards in poker or enemy location in the Hunting Wabbits game. In perfect information games like chess, all information is available to both players.\nThis game has one chance player (the Author), one maximizing player (Fudd) and one minimizing player (Bugs). Fudd and Bugs are in a long-term, all-out war, and so any energy that Fudd wastes or saves is a gain or loss for Bugs; our game is zero-sum.\nFirst, the Author will choose whether to write an episode where Bugs is at the Opera (50%) or in the Forest (50%). Fudd cannot tell what the Author has chosen.\nNext, Fudd will decide whether to Hunt_Near or Hunt_Far. If he chooses Hunt_Far, he takes -1 value for the extra effort.\nBugs knows whether the Author wrote him into the Opera or Forest, but he does not know Fudd’s hunting location. If the Author gives him the Opera, Bugs has no choices to make and the game ends after Fudd’s action. If Forest, Bugs will decide whether to Play_Near or Play_Far. If he plays in the same location as Fudd is hunting, Fudd gets +3 value for a successful hunt (for a total of +2 in the Hunt_Far action due to the -1 value for the extra effort). If they are in different locations (or if Bugs is at the Opera), Fudd will get 0 value for an unsuccessful hunt (-1 for the Hunt_Far misses).\nPutting it all together, the payoff structure of this game is:\n\n\n\nAuthor\nBugs/Fudd\nHunt_Near\nHunt_Far\n\n\n\n\nOpera\n\n0, 0\n+1, -1\n\n\nForest\nPlay_Near\n-3, +3\n+1, -1\n\n\nForest\nPlay_Far\n0, 0\n-2, +2\n\n\n\nThe tree structure is as follows with the payoffs written from the perspective of Fudd:\n\nIf Fudd knows he’s at the Opera, then he must prefer to Hunt_Near to get a value of \\(0\\) instead of \\(-1\\) for Hunt_Far, but since he doesn’t know his location, he must take both scenarios into account.\nNote that Bugs’s optimal actions depend on Fudd’s Opera strategy even though that outcome cannot be reached once Bugs is playing since Bugs only plays in the Forest! For example if we kept the same game tree except Fudd had a \\(+100\\) Opera Hunt_Far payoff, then he would always Hunt_Far. Bugs would see this and it would affect how Bugs plays in the Forest scenario.\n\n\n\n\n\n\nExercise\n\n\n\nHow would Bugs play in the Forest scenario knowing that Fudd is always playing Hunt_Far?\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nBugs would always Play_Near because then his payoff in the only scenario he can control would be \\(+1\\), whereas Play_Far would get him a payout of \\(-2\\). (The payoffs are all written from the perspective of Fudd, so Bugs’s are opposite.)",
    "crumbs": [
      "About",
      "Challenges",
      "#1: Kuhn Poker",
      "Optional Reading"
    ]
  },
  {
    "objectID": "aipcs24/1kuhn_reading.html#concept-information-set",
    "href": "aipcs24/1kuhn_reading.html#concept-information-set",
    "title": "#1: Kuhn Poker | Reading",
    "section": "Concept: Information Set",
    "text": "Concept: Information Set\nIn a perfect information game, we can draw a tree of the possible states the game can be in. Every time a player is called to take an action, they will know what node they are at, and what node they will go to with each legal action.\nIn an imperfect information game, we can still draw that tree, but now a player might be called to take an action without knowing what node they are actually at. Instead, there will be a set of one or more nodes that are indistinguishable to that player based on what they have seen so far, and they will have to take an action knowing only that they are in that set. Such a set of nodes is an information set or an infoset.\nThe infosets contain information about the player and what actions have been seen so far. For example, [Fudd] is an infoset that contains the nodes [Fudd, Forest] and [Fudd, Opera]. (Just [Fudd] because no other actions/information have been revealed to Fudd at this point.)\nA player strategy is a rule that says, for every information set that player will face, what action or (random choice of actions) that player will take. For a game like Hunting Wabbits or Kuhn Poker (the Challenge 1 game), we can list every information set and its probabilities. For a more complicated game, we might write our strategy as a computation that will output probabilities based on inputs and an algorithm.",
    "crumbs": [
      "About",
      "Challenges",
      "#1: Kuhn Poker",
      "Optional Reading"
    ]
  },
  {
    "objectID": "aipcs24/1kuhn_reading.html#concept-expected-value",
    "href": "aipcs24/1kuhn_reading.html#concept-expected-value",
    "title": "#1: Kuhn Poker | Reading",
    "section": "Concept: Expected Value",
    "text": "Concept: Expected Value\nOnce we’ve assigned definite values to the ultimate outcomes, the expected value, or EV, of a situation is the value of each outcome weighted by the probability of that outcome.\n\n\n\n\n\n\nExercise\n\n\n\nSuppose that Bugs plays uniform \\(0.5\\) Play_Near and \\(0.5\\) Play_Far.\n\nWhat is the value of each Bugs node and what should Fudd do if he knew Bugs’s actions? (Recall that the payoff values are from the perspective of Fudd and Bugs uses opposite values.)\nWhat is Fudd’s expected value in this case?\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nBugs’s node values are:\n\n\\[\n\\begin{align*}\n\\mathbb{E}(\\text{Left Node}) &= 0.5*-3 + 0.5*0 = -1.5 \\\\\n\\mathbb{E}(\\text{Right Node}) &= 0.5*1 + 0.5*-2 = -0.5\n\\end{align*}\n\\]\n\nThe values for Fudd are inverse, so Fudd prefers the Left Node, which has a value of \\(1.5\\). This means that if Fudd is in the Forest, he prefers to Hunt_Near.\n\n\nWe computed that Fudd prefers Hunt_Near in the Forest scenario and can see on the game tree that he also prefers Hunt_Near in the Opera scenario, so can already know that he will always choose Hunt_Near. Fudd will then have the following expected values:\n\n\\[\n\\begin{equation}\n\\begin{split}\n\\mathbb{E}(\\text{Hunt\\_Near}) &= \\Pr(\\text{Opera})*u(\\text{Hunt\\_Near}) + \\Pr(\\text{Forest})*u(\\text{Hunt\\_Near}) \\\\\n  &= 0.5*0 + 0.5*1.5 \\\\\n  &= 0.75\n\\end{split}\n\\end{equation}\n\\]\n\n\n\nIn imperfect information games, we consider probabilities over two different sources of uncertainty, after assuming a particular P1 strategy and P2 strategy:\n\nUncertainty about which node we are actually in, given that we know that we’re in one of multiple nodes that we can’t tell apart. The probabilities of being in node 1, node 2, … of an information set can be calculated by the probabilities of strategies upwards in the game tree (and the probabilites of chance events upwards in the game that have already happened). For example, Fudd doesn’t know if he’s in the Forest or Opera at the beginning.\nUncertainty about what will happen after we go to a node downwards in the game tree, coming from chance events or strategy probabilities in the players’ following actions. For example, after Fudd selects Hunt_Near, there is uncertainty about the outcome since it depends on Bugs’s actions.\n\n\n\nWe will focus on zero-sum two-player games, so the value to one player is simply the negative of the value to the other. Therefore, we can represent value in the game as a single number that the maximizing player wishes to make positive and the minimizing player wishes to make negative.\nWe will focus on maximizing (or minimizing) expected value as our goal for all of session 1. One thing that makes it natural to care about expected value is that it’s usually the best way to predict what your score will be after a very large number of games, whether they are the same game or different from each other.",
    "crumbs": [
      "About",
      "Challenges",
      "#1: Kuhn Poker",
      "Optional Reading"
    ]
  },
  {
    "objectID": "aipcs24/1kuhn_reading.html#concept-regret",
    "href": "aipcs24/1kuhn_reading.html#concept-regret",
    "title": "#1: Kuhn Poker | Reading",
    "section": "Concept: Regret",
    "text": "Concept: Regret\nFor a given P1 strategy and P2 strategy, a player has regret when they take an action at an infoset that was not the highest-EV action at that infoset. The amount of regret is the difference between the highest-EV action and the selected action.\n\n\n\n\n\n\nExercise\n\n\n\n\nWhat is the regret for each action?\n\n\n\nAction\nRegret\n\n\n\n\nA\n\n\n\nB\n\n\n\nC\n\n\n\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\n\nAction\nRegret\n\n\n\n\nA\n4\n\n\nB\n2\n\n\nC\n0\n\n\n\n\n\n\nThere are other things that “regret” can mean in English, that are separate from this technical concept:\n\nBased on what chance events later happened, I wish I had taken a different action instead.\nI was wrong about what strategy my opponent was playing, and I wish I had taken a different action instead.\n\nHowever, we will use “regret” as a technical concept to mean how much worse actions that are not-highest-EV perform compared to highest-EV actions given a particular P1 strategy and P2 strategy.",
    "crumbs": [
      "About",
      "Challenges",
      "#1: Kuhn Poker",
      "Optional Reading"
    ]
  },
  {
    "objectID": "aipcs24/1kuhn_reading.html#exercises",
    "href": "aipcs24/1kuhn_reading.html#exercises",
    "title": "#1: Kuhn Poker | Reading",
    "section": "Exercises",
    "text": "Exercises\n\n\n\n\n\n\nInformation Set\n\n\n\n\nWhat are Fudd’s information set(s)?\nWhat are Bugs’s information set(s)?\n\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nBoth Fudd nodes are a single information set because when Fudd is making the Hunt_Near or Hunt_Far decision, he doesn’t know whether he’s in the Opera or the Forest, so his information is the same in both nodes. We can label these simply [Fudd].\nBoth Bugs nodes are also a single information set because although Bugs knows that he’s in the Forest, he doesn’t know which action Fudd has taken, so his information is the same in both nodes. We can label these [Bugs, Forest].\n\n\n\n\n\n\n\n\n\n\nExpected Value\n\n\n\nSay that Fudd chooses to Hunt_Near with probability \\(p\\).\n\n\nAt the Bugs infoset where Bugs knows he’s in the Forest, what is the expected value of choosing to Play_Near?\nWhat is the expected value of choosing to Play_Far?\n\n(Reminder: The payoffs are from the perspective of Fudd and are the opposite for Bugs.)\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\\(\\mathbb{E}(\\text{Play\\_Near}) = -3*p + 1*(1-p) = -4*p + 1\\)\n\\(\\mathbb{E}(\\text{Play\\_Far}) = 0*p + -2*(1-p) = 2*p - 2\\)\n\n\n\n\n\n\n\n\n\n\nExpected Value 2\n\n\n\nSay that Bugs chooses to Play_Near with probability \\(q\\).\n\n\nWhat is Fudd’s expected value of choosing Hunt_Near at his infoset?\nWhat is Fudd’s EV of choosing Hunt_Far?\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\\(\\mathbb{E}(\\text{Hunt\\_Near}) = 0.5*0 +0.5*[3*q + 0*(1-q)] = 1.5*q\\)\n\\(\\mathbb{E}(\\text{Hunt\\_Far}) = 0.5*(-1) + 0.5*[-1*q + 2*(1-q)] = -0.5 - 0.5*q + 1 - q = 0.5 - 1.5*q\\)\n\n\n\n\n\n\n\n\n\n\nRegret\n\n\n\nFind a \\(p\\) and a \\(q\\) such that both:\n\nBugs never chooses an action with regret\nFudd never chooses an action with regret\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nIn order to have no regret, the expected value of both actions should be equal\n\n\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nFor Bugs to never choose an action with regret, EV of Play_Near and EV of Play_Far should be equal.\n\n\\(\\mathbb{E}(\\text{Bugs Play\\_Near}) = -4*p + 1\\)\n\\(\\mathbb{E}(\\text{Bugs Play\\_Far}) = 2*p - 2\\)\nSetting equal, we have \\(-4*p + 1 = 2*p - 2 \\Rightarrow 3 = 6*p \\Rightarrow p = \\frac{1}{2} = 0.5\\)\n\\(\\Pr(\\text{Fudd Hunt\\_Near}) = p = 0.5\\)\n\\(\\Pr(\\text{Fudd Hunt\\_Far}) = 1 - p = 0.5\\)\n\nFor Fudd to never choose an action with regret, EV of Hunt_Near and EV of Hunt_Far should be equal.\n\n\\(\\mathbb{E}(\\text{Fudd Hunt\\_Near}) = 1.5*q\\)\n\\(\\mathbb{E}(\\text{Fudd Hunt\\_Far}) = 0.5 - 1.5*q\\)\nSetting equal, we have \\(1.5*q = 0.5 - 1.5*q \\Rightarrow 3*q = 0.5 \\Rightarrow q = \\frac{1}{6} = 0.167\\)\n\\(\\Pr(\\text{Bugs Play\\_Near}) = q = 0.167\\)\n\\(\\Pr(\\text{Bugs Play\\_Far}) = 1 - q = 0.833\\)\nNotice that each player is inducing the other player to have no regret (to be indifferent to both actions) by playing actions at these probabilities, which then result in an equilibrium as shown below:\n\nWe will go deeper into indifference in the next reading.\n\n\n\n\n\n\n\n\n\nGame Value\n\n\n\nWhat is the value of the game (i.e. the expected value as Fudd over the entire game) at the equilibrium strategies found in the previous question?\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nSince the game is zero-sum, we can find Fudd’s expected value and it is equivalent to the game value:\n\\[\\begin{align}\n\\mathbb{E} &= \\Pr(\\text{Opera}) * \\Pr(\\text{Hunt\\_Near}) * 0 \\\\\n              & + \\Pr(\\text{Opera}) * \\Pr(\\text{Hunt\\_Far}) * -1 \\\\\n              & + \\Pr(\\text{Forest}) * \\Pr(\\text{Hunt\\_Near}) * \\Pr(\\text{Play\\_Near}) * 3 \\\\\n              & + \\Pr(\\text{Forest}) * \\Pr(\\text{Hunt\\_Near}) * \\Pr(\\text{Play\\_Far}) * 0 \\\\\n              & + \\Pr(\\text{Forest}) * \\Pr(\\text{Hunt\\_Far}) * \\Pr(\\text{Play\\_Near}) * -1 \\\\\n              & + \\Pr(\\text{Forest}) * \\Pr(\\text{Hunt\\_Far}) * \\Pr(\\text{Play\\_Far}) * 2 \\\\ \\\\\n    &= 0.5 * 0.5 * 0 \\\\\n    &+ 0.5 * 0.5 * -1 \\\\\n    &+ 0.5 * 0.5 * 0.17 * 3 \\\\\n    &+ 0.5 * 0.5 * 0.83 * 0 \\\\\n    &+ 0.5 * 0.5 * 0.17 * -1 \\\\\n    &+ 0.5 * 0.5 * 0.83 * 2 \\\\ \\\\\n    &= 0 \\\\\n    &+ -0.25 \\\\\n    &+ 0.125 \\\\\n    &+ 0 \\\\\n    &+ -0.042 \\\\\n    &+ 0.42 \\\\ \\\\\n    &= 0.25 \\\\\n\\end{align}\\]\nThis means that if they play repeatedly, we expect Fudd to average a payoff of \\(+0.25\\), while Bugs has a payoff of \\(-0.25\\).",
    "crumbs": [
      "About",
      "Challenges",
      "#1: Kuhn Poker",
      "Optional Reading"
    ]
  },
  {
    "objectID": "aipcs24/2othergames_reading.html",
    "href": "aipcs24/2othergames_reading.html",
    "title": "#2: Other Games | Reading",
    "section": "",
    "text": "Want to highlight different classes of games and elements of solving them that will be valuable later in imperfect info games, things like: Keeping table of values Regret Node values What a policy/strategy is Monte Carlo methods"
  },
  {
    "objectID": "aipcs24/2othergames_reading.html#types-of-games",
    "href": "aipcs24/2othergames_reading.html#types-of-games",
    "title": "#2: Other Games | Reading",
    "section": "Types of Games",
    "text": "Types of Games\nThis course is primarily about poker and poker-adjacent games, but let’s take a step back and look at different classes of games and where poker fits.\nThe goal of this reading is to introduce various concepts that will be useful when building agents for future challenges."
  },
  {
    "objectID": "aipcs24/2othergames_reading.html#reinforcement-learning-rl",
    "href": "aipcs24/2othergames_reading.html#reinforcement-learning-rl",
    "title": "#2: Other Games | Reading",
    "section": "Reinforcement Learning (RL)",
    "text": "Reinforcement Learning (RL)\nMany ideas in this section on RL come from Sutton and Barto’s Reinforcement Learning book.\nThe main idea of reinforcement learning is that an agent learns by taking actions in its environment. The environment then gives feedback in the form of rewards and a new state.\n\n\n\nDiagram from Sutton Barton Reinforcement Learning\n\n\nWe have for each time \\(t\\): - State: \\(S_t\\) - Action: \\(A_t\\) - Reward: \\(R_t\\)\nThe goal of an agent is generally to maximize the expected value of the rewards received.\nReward values can be designed in a way to optimally train the agent. In chess for example, we could give a reward of \\(+1\\) for winning a game, \\(-1\\) for losing, and \\(0\\) for all other states including draws. Defining intermediate rewards for actions like taking an opponent piece risks the agent prioritizing that goal over winning the game.\nReward hacking is when agents find a way to obtain rewards in a way that isn’t aligned with the intended goal. (The agents will take the reward structure very literally and this is all they have to go on!)\nA Markov Decision Process (MDP) is a simplified model of the reinforcement learning problem in that the probability of future states and rewards depends only on the previous state and actions.\nThis can be written as:\n\\[\np(s', r \\mid s, a) = \\Pr\\{S_t=s', R_t=r \\mid S_{t-1}=s, A_{t-1}=a\\}\n\\]\nValue functions estimate the value of being in a certain state in terms of expected future rewards (which can be discounted).\n\nIf you’re about to flip a coin and want it to be heads (\\(+1\\)) and not tails (\\(-1\\)), then the value of that state is \\(0\\).\nIf you’re in a chess game where winning is \\(+1\\) and losing is \\(-1\\) and everything else is \\(0\\) and you’re in a state where it’s your turn and you can checkmate, then that state has a value of \\(+1\\).\n\nA policy is a mapping from states to probabilties of selecting each action and is written as \\(\\pi(a\\mid s)\\).\nSimple MDP example, like buy carrots not anything else\nPolicy, value function, optimal policy, optimal value function"
  },
  {
    "objectID": "aipcs24/2othergames_reading.html#war",
    "href": "aipcs24/2othergames_reading.html#war",
    "title": "#2: Other Games | Reading",
    "section": "War",
    "text": "War\nIf you thought that Kuhn Poker was a simple card game, meet War.\nThere are two players, each gets half the deck, 26 cards. Each player turns over their top card and faces it against the opponent’s top card. The better card wins, with Aces high. This repeats until one player has all the cards.\nWhen the cards match, the players go to “War”. When this happens, put the next card face down, and then the card after that face up, and then these up-cards face off against each other. The winner takes all six cards. If there’s another tie, then repeat and the winner takes 10 cards, etc.\n\n\n\n\n\n\nOptional Coding Exercise\n\n\n\nCode and run 10,000 simulations of War and determine how many turns the average game takes and how many War situations occur on average in each simulation.\n\n\nYou can see a Dreidel game simulator written by Ben Blatt in Slate from 2014 at this link."
  },
  {
    "objectID": "aipcs24/2othergames_reading.html#tictactoe",
    "href": "aipcs24/2othergames_reading.html#tictactoe",
    "title": "#2: Other Games | Reading",
    "section": "Tictactoe",
    "text": "Tictactoe\n\nEvolutionary algorithms for poker and tic-tac-toe\nExercise: Look at this game tree with payouts at the bottom written in terms of Player 1. Start from the bottom of the tree and figure out the actions of each player at each node. Then figure out the value of the game for Player 1 and for Player 2. This procedure is called backpropagation.\nQuestion: Why can’t we use this procedure in Kuhn Poker and imperfect info games? Tree imperfect info vs perfect info issues, show trees Compare this to the Wabbits game and problem from paper\n\nMinimax\nhttps://www.neverstopbuilding.com/blog/minimax\n\n\nValue Function\nMinimax assumes opponent playing best plays too temporal-difference\n\n\nRL: Planning and Learning\n\n\nRL: Monte Carlo Tree Search\nhttps://starai.cs.ucla.edu/papers/VdBBNAIC09.pdf What about just using imperfect info version of MCTS?"
  },
  {
    "objectID": "aipcs24/2othergames_reading.html#multi-armed-bandits",
    "href": "aipcs24/2othergames_reading.html#multi-armed-bandits",
    "title": "#2: Other Games | Reading",
    "section": "Multi-Armed Bandits",
    "text": "Multi-Armed Bandits\n\nBandits are a set of problems with repeated decisions and a fixed number of actions possible coming from a single state.\nThe agent updates its strategy based on what it learns from the feedback from the environment. You could think of this in real-world settings like picking which dish to eat at a restaurant.\nConsider a 10-armed bandit setup like in the image below:\n\n\n\nSutton Barto Bandits\n\n\nEach time the player pulls an arm, they get some reward, which could be positive or negative.\nA basic setting initializes each of 10 arms with \\(q*(\\text{arm}) = \\mathcal{N}(0,1)\\) so each is initialized with a center point around a Gaussian distribution. Each pull of an arm returns a reward of \\(R = \\mathcal{N}(q*(\\text{arm}_i), 1)\\).\nIn other words, each arm is initialized with a value centered around \\(0\\) but with some variance, so each will be a bit different. Then from that point, the actual pull of an arm is centered around that new point with some variance as seen in this figure above.\nThe agent can sample actions and estimate their values based on experience and then use some algorithm for deciding which action to use next to maximize rewards. The estimated value of action \\(a\\) at timestep \\(t\\) is defined as \\(Q_t(a)\\).\nThe agent is then faced with two competing goals:\n\nGet as accurate an estimate \\(Q_t(a)\\) as possible\nSelect actions with the highest rewards as much as possible\n\nExploring refers to figuring out the values of the arms, or in the case of a restaurant, figuring out how good each dish is.\nExploiting refers to using current knowledge to choose the highest value estimated action.\n\n\n\n\n\n\nExercise\n\n\n\nA naive approach is that a player could sample each action once and then always use the action that gave the best reward going forward.\nWhat is a problem with this strategy? Can you suggest a better one?\n\n\n\\(Q_t(a)\\) can simply be estimated by averaging the rewards received each time a specific arm has been tried. The so-called greedy action rule is to then take the largest \\(Q_t(a)\\) action, \\(A_t = \\argmax_{a} Q_t(a)\\)\n\nRegret\nHow would you define regret in this bandit setting? How is minimizing regret related to maximizing reward? explore exploit\n\n\nRL: Action-Value Methods\nHidden imperfect info, understand in distribution over possible states of the world Depending on state will want to make decisions differently Bayesian explore exploit optimizer What is my opponent range exercise in Bayesian update Explore vs. exploit Bayesian updating https://aipokertutorial.com/game-theory-foundation/#regret https://www.reddit.com/r/statistics/comments/1949met/how_are_multi_armed_bandits_related_to_bayesian/ https://tor-lattimore.com/downloads/book/book.pdf https://lcalem.github.io/blog/2018/09/22/sutton-chap02-bandits#26-optimistic-initial-values\nBut you could use the MCTS that does work by changing the game setup"
  },
  {
    "objectID": "aipcs24/2othergames_reading.html#blackjack",
    "href": "aipcs24/2othergames_reading.html#blackjack",
    "title": "#2: Other Games | Reading",
    "section": "Blackjack",
    "text": "Blackjack\nSetup Solving\n\nRL: Dynamic Programming\nMy blog post\n\n\nRL: Monte Carlo Methods"
  },
  {
    "objectID": "aipcs24/2othergames_reading.html#gridworld",
    "href": "aipcs24/2othergames_reading.html#gridworld",
    "title": "#2: Other Games | Reading",
    "section": "Gridworld",
    "text": "Gridworld\npg 60\n\nRL: Q-Learning"
  },
  {
    "objectID": "aipcs24/2othergames_reading.html#poker-games",
    "href": "aipcs24/2othergames_reading.html#poker-games",
    "title": "#2: Other Games | Reading",
    "section": "Poker Games",
    "text": "Poker Games\nWhy are we mostly interested in poker games? We think that\nimperfect adversarial widely played well researched, but only nash equilibrium"
  },
  {
    "objectID": "aipcs24/1kuhn_k.html",
    "href": "aipcs24/1kuhn_k.html",
    "title": "#1: Kuhn Poker | WTK",
    "section": "",
    "text": "During the first AI Poker Camp session on Monday 7/15/24, we asked participants to list the deterministic situations in the Kuhn Poker game. In other words, points of the game where it is strictly best to play a certain action.\nWe started with the full game tree:\n\nWe came up with a list of 5 of these:\n\n\n\n\n\n\n\n\n#\nScenario\nAction\n\n\n\n\n1\n_Q↑: Q as Player 2 facing a bet\nShould always play ↓ fold\n\n\n2\n_A↑: A as Player 2 facing a bet\nShould always play ↑ call\n\n\n3\n_A↓: A as Player 2 facing a check\nShould always play ↑ bet\n\n\n4\nQ_↓↑: Q as Player 1 checking and then facing a bet\nShould always play ↓ fold\n\n\n5\nA_↓↑: A as Player 2 checking and then facing a bet\nShould always play ↑ bet\n\n\n\nThe game tree is then reduced to:\n\nThere are 2 others that we knew were slightly different, but we still believed that they belonged in this category:\n\n\n\n\n\n\n\n\n#\nScenario\nAction\n\n\n\n\n6\nK_↓: K as Player 2 facing a check\n\n\n\n7\nK_: K as Player 1 acting first\n\n\n\n\nMy assumption was that 6 and 7 were trickier because they relied on the opponent playing under the conditions of 1-5 above. Things turned around and I became the confused one when a couple of participants pointed out that at K_ it is not always clearly best to play ↓!\nBetting with a K knowing that your opponent will always play perfectly (fold with Q and call with A) does not seem intuitively very profitable, but this exercise is asking for deterministic situations where it is definitely best to play one action.\nGoing forward on this page, we are assuming that players play their mandatory +EV decisions correctly (i.e., numbers 1-5 above)."
  },
  {
    "objectID": "aipcs24/1kuhn_k.html#what-the-k",
    "href": "aipcs24/1kuhn_k.html#what-the-k",
    "title": "#1: Kuhn Poker | WTK",
    "section": "",
    "text": "During the first AI Poker Camp session on Monday 7/15/24, we asked participants to list the deterministic situations in the Kuhn Poker game. In other words, points of the game where it is strictly best to play a certain action.\nWe started with the full game tree:\n\nWe came up with a list of 5 of these:\n\n\n\n\n\n\n\n\n#\nScenario\nAction\n\n\n\n\n1\n_Q↑: Q as Player 2 facing a bet\nShould always play ↓ fold\n\n\n2\n_A↑: A as Player 2 facing a bet\nShould always play ↑ call\n\n\n3\n_A↓: A as Player 2 facing a check\nShould always play ↑ bet\n\n\n4\nQ_↓↑: Q as Player 1 checking and then facing a bet\nShould always play ↓ fold\n\n\n5\nA_↓↑: A as Player 2 checking and then facing a bet\nShould always play ↑ bet\n\n\n\nThe game tree is then reduced to:\n\nThere are 2 others that we knew were slightly different, but we still believed that they belonged in this category:\n\n\n\n\n\n\n\n\n#\nScenario\nAction\n\n\n\n\n6\nK_↓: K as Player 2 facing a check\n\n\n\n7\nK_: K as Player 1 acting first\n\n\n\n\nMy assumption was that 6 and 7 were trickier because they relied on the opponent playing under the conditions of 1-5 above. Things turned around and I became the confused one when a couple of participants pointed out that at K_ it is not always clearly best to play ↓!\nBetting with a K knowing that your opponent will always play perfectly (fold with Q and call with A) does not seem intuitively very profitable, but this exercise is asking for deterministic situations where it is definitely best to play one action.\nGoing forward on this page, we are assuming that players play their mandatory +EV decisions correctly (i.e., numbers 1-5 above)."
  },
  {
    "objectID": "aipcs24/1kuhn_k.html#k",
    "href": "aipcs24/1kuhn_k.html#k",
    "title": "#1: Kuhn Poker | WTK",
    "section": "_K↓",
    "text": "_K↓\n\nWe are looking at TPOT (this part of the tree) when  has a K and is facing a ↓ (check):\n\nIf playing ↑, we have: \\[\n\\mathbb{E} = -2*\\Pr(\\text{P1 A Plays \\downarrow}) + 1*\\Pr(\\text{P1 Q Plays \\downarrow})\n\\]\nPlaying ↓ has: \\[\n\\mathbb{E} = -1*\\Pr(\\text{P1 A Plays \\downarrow}) + 1*\\Pr(\\text{P1 Q Plays \\downarrow})\n\\]\nTherefore in this case ↓ is strictly best and we can include this case on our list of deterministic situations.\nThe updated tree:"
  },
  {
    "objectID": "aipcs24/1kuhn_k.html#k_",
    "href": "aipcs24/1kuhn_k.html#k_",
    "title": "#1: Kuhn Poker | WTK",
    "section": "K_",
    "text": "K_\nWe are now looking at TPOT (this part of the tree) and can end up on one of these 7 nodes:\n\nAt infoset K_, when the K opening action plays ↑, we end up in 2 possible nodes and have known EV:\n\\[\n\\begin{align}\n\\mathbb{E} &= -2*\\Pr(\\text{P2 has A}) + 1*\\Pr(\\text{P2 has Q}) \\\\\n&= -2*0.5 + 1*0.5 \\\\\n&= -0.5\n\\end{align}\n\\]\nNow things get interesting.\nWhen the K opening action plays ↓, we can compute the EV:\n\nAssume that  plays ↑ at K↓↑ with probability \\(k\\).\nAssume that  plays ↑ at _Q↓ with probability \\(q\\).\n\nThere are 3 cases of what can happen now after  plays ↓ at K_ and we can end up in 5 possible nodes:\n\n has A and plays ↑.\n\n\\[\n\\begin{align}\n\\mathbb{E} &= -2*\\Pr(\\text{P1 Calls K}) - 1*\\Pr(\\text{P1 Folds K}) \\\\\n&= -2*k - 1*(1-k) \\\\\n&= -k - 1\n\\end{align}\n\\]\n\n has Q and plays ↓.\n\n\\[\n\\mathbb{E} = 1\n\\]\n\n has Q and plays ↑.\n\n\\[\n\\begin{align}\n\\mathbb{E} &= 2*\\Pr(\\text{P1 Calls K}) - 1*\\Pr(\\text{P1 Folds K}) \\\\\n&= 2*k - 1*(1-k) \\\\\n&= 3*k - 1\n\\end{align}\n\\]\nPutting these together, we have:\n\\[\n\\begin{align}\n\\mathbb{E} &= [-k - 1]*\\Pr(\\text{P2 has A}) + [1]*\\Pr(\\text{P2 has Q and Checks}) + [3*k - 1]*\\Pr(\\text{P2 Has Q and Bets}) \\\\\n&= [-k - 1]*0.5 + [1]*0.5*(1-q) + [3*k - 1]*0.5*(q) \\\\\n&= -0.5*k - 0.5 + 0.5 - 0.5*q + 1.5*k*q - 0.5*q \\\\\n&= -0.5*k - q + 1.5*k*q\n\\end{align}\n\\]\n\nimport {Plot} from '@observablehq/plot'\n\n// Create sliders for k and q, constrained between 0 and 1\nviewof k = Inputs.range([0, 1], {step: 0.01, label: \"k\"})\nviewof q = Inputs.range([0, 1], {step: 0.01, label: \"q\"})\n\n// Calculate z based on the function\nz = -0.5 * k - q + 1.5 * k * q\n\n// Display the current value of z\nmd`The current value of z is: ${z.toFixed(4)}`\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfunction createPlot(k, q) {\n  const points = [];\n  for (let x = 0; x &lt;= 1; x += 0.02) {\n    for (let y = 0; y &lt;= 1; y += 0.02) {\n      points.push({x, y, z: -0.5 * x - y + 1.5 * x * y});\n    }\n  }\n\n  return Plot.plot({\n    width: 600,\n    height: 400,\n    x: {label: \"k\", domain: [0, 1]},\n    y: {label: \"q\", domain: [0, 1]},\n    color: {\n      type: \"linear\",\n      domain: [-1, 1],\n      scheme: \"RdBu\"\n    },\n    marks: [\n      Plot.contour(points, {x: \"x\", y: \"y\", z: \"z\", stroke: \"currentColor\", interval: 0.05}),\n      Plot.image(points, {x: \"x\", y: \"y\", z: \"z\", interpolate: \"nearest\"}),\n      Plot.dot([{x: k, y: q}], {x: \"x\", y: \"y\", stroke: \"red\", fill: \"red\"})\n    ]\n  })\n}\n\ncreatePlot(k, q)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMathematical Conclusions\nWe can set \\(-0.5 = -0.5*k - q + 1.5*k*q\\) to compare the EV of betting to checking.\nWe see that if you have check and then call with at least \\(k \\geq 0.5\\) then you do at least as good as betting. You do strictly better if \\(0.5 &lt; k &lt; 1\\).\n\n\nThis saddle-point visualization is actually really good for me. I feel like I more-intuitively understand that: changing q changes the slope of the line in the k axis, and also the setting that’s flat optimizes minimax when we pick q so that the line is flat in the k axis, P1 still gets to pick among k values that cause P2 to face a different slope If P1 plays a positive p(⬇️|A_), then that gives P2 an incentive to push q down, which is like adding another gradient along the q axis …I forget where I was going with this, but it feels helpful Ross Rheingans-Yoo — 07/16/2024 11:44 AM Potential improvements: above the q slider, write “when k is [current value], z = [formula in q]” above the k slider, write “when q is [current value], z = [formula in k]” on the 3d plot, draw a red dot at the current q, k and a red line along the line that varies k from there, and along the line that varies q from there have an option to switch to a 2d plot with: (subplot 1) z,k where q is represented by color/hue, draw 11 lines for every 0.1 increment of q with opacity 30%, then the line for the current q with opacity 100%; (subplot 2) z,q where k is represented by color I think it might be better to swap the axes of the 3d plot because q happens logically before k, but I’m not sure and the shape might look worse I’m not sure about the q,k axes direction – is it better if 0,0 is the point closest to us? (this might make the shape look bad because we look at it edge-on) can we get a 3d graphing library that lets you click and drag to rotate? I went down my own k-hole to figure out that z=(3q-1)(3k-2)/6+1/3, which I think should be presented somewhere? I think the 3d graph (and the 2d graphs) should do more to make clear where -0.5 is, since that’s the case we’re comparing it to."
  },
  {
    "objectID": "aipcs24/3rps_challenge.html",
    "href": "aipcs24/3rps_challenge.html",
    "title": "#3 Rock Paper Scissors: Challenge",
    "section": "",
    "text": "Usual RPS rules apply with Rock &gt; Scissors &gt; Paper &gt; Rock with \\(+1\\) payouts for a win, \\(-1\\) payouts for a loss, and \\(0\\) for a tie.\nRepeated Rock Paper Scissors was proposed in 2023 to be a “benchmark for multiagent learning” in a DeepMind paper.\nThe paper explains that agents are often measured by (a) average return or (b) robustness against a nemesis agent that tries to minimize the agent’s returns. Yet it’s important for agents to be able to maximize returns and be robust to adversaries.\nWhy is repeated RPS a good benchmark?\n\nIt’s a repeated game with sequential decisions\nPerformance is measured against a population of varied skills\n\n\n\nYou will enter a Rock Paper Scissors bot and the field will be 1/2 student bot submissions and 1/2 our bots that will include:\n\nFixed-percentage bots\nNot-very-sophisticated bots that act based only on the most recent observation\nSome number of more advanced bots\n\nYou will play each other bot once for a 2,000 game match (once as P1 and once as P2 even though these don’t matter in this game).",
    "crumbs": [
      "About",
      "Challenges",
      "#3: Rock Paper Scissors",
      "Challenge"
    ]
  },
  {
    "objectID": "aipcs24/3rps_challenge.html#challenge-a-rock-paper-scissors-rps",
    "href": "aipcs24/3rps_challenge.html#challenge-a-rock-paper-scissors-rps",
    "title": "#3 Rock Paper Scissors: Challenge",
    "section": "",
    "text": "Usual RPS rules apply with Rock &gt; Scissors &gt; Paper &gt; Rock with \\(+1\\) payouts for a win, \\(-1\\) payouts for a loss, and \\(0\\) for a tie.\nRepeated Rock Paper Scissors was proposed in 2023 to be a “benchmark for multiagent learning” in a DeepMind paper.\nThe paper explains that agents are often measured by (a) average return or (b) robustness against a nemesis agent that tries to minimize the agent’s returns. Yet it’s important for agents to be able to maximize returns and be robust to adversaries.\nWhy is repeated RPS a good benchmark?\n\nIt’s a repeated game with sequential decisions\nPerformance is measured against a population of varied skills\n\n\n\nYou will enter a Rock Paper Scissors bot and the field will be 1/2 student bot submissions and 1/2 our bots that will include:\n\nFixed-percentage bots\nNot-very-sophisticated bots that act based only on the most recent observation\nSome number of more advanced bots\n\nYou will play each other bot once for a 2,000 game match (once as P1 and once as P2 even though these don’t matter in this game).",
    "crumbs": [
      "About",
      "Challenges",
      "#3: Rock Paper Scissors",
      "Challenge"
    ]
  },
  {
    "objectID": "aipcs24/3rps_challenge.html#challenge-b-paper-scissors-maybe-rock-psmr",
    "href": "aipcs24/3rps_challenge.html#challenge-b-paper-scissors-maybe-rock-psmr",
    "title": "#3 Rock Paper Scissors: Challenge",
    "section": "Challenge B: Paper, Scissors, Maybe Rock (PSMR)",
    "text": "Challenge B: Paper, Scissors, Maybe Rock (PSMR)\nThe gameplay works the same as RPS with the following additional rule.\nEach matchup begins by the server generating two probabilities:\n\n\\(X\\) is the probability that \\(P1\\) is not allowed to play Rock\n\\(Y\\) is the probability that \\(P2\\) is not allowed to play Scissors\n\nYou will not be told your own or your opponent’s percentage.\n\nThe Competition\nYou will play multiple 1,000 game duplicate matches against each opponent (i.e., 2000 total games per match).\n\nYou will submit a PSMR bot and the field will include at least:\n\na bot that tries to play \\(1/3\\) - \\(1/3\\) - \\(1/3\\) or as close as it can\na bot that plays what would be Nash for it if the opponent were unconstrained\na bot that is our best attempt to do a reasonable thing, limited by the amount of time we actually decide to spend on it\n\nWe aren’t intending to say much more about how the \\(X\\) and \\(Y\\) probabilities will be generated, and no your bot should not be communicating with itself between matchups/duplicate matches or phoning home to you. (It should be storing info for itself game-to-game within a matchup, however.)\nIn the current handout version of challenge-3-psmr in aipc-challenges, you can practice by passing --duplicate &lt;(scripts/psmr_deals.sh PROB_X PROB_Y) to engine.py. The script psmr_deals.sh will generate output that forces a particular sequence of deals, based on the two probabilities. If you want a replicable experiment, you can put a finite number of deals into a file with psmr_deals PROB_X PROB_Y | head -n NUM_LINE &gt; deals.txt and pass --duplicate deals.txt instead.",
    "crumbs": [
      "About",
      "Challenges",
      "#3: Rock Paper Scissors",
      "Challenge"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Poker Camp",
    "section": "",
    "text": "Poker Camp is a program to learn about probability, game theory, AI, and decision making under uncertainty through the lens of poker."
  },
  {
    "objectID": "index.html#sign-up-now",
    "href": "index.html#sign-up-now",
    "title": "Poker Camp",
    "section": "Sign up Now!",
    "text": "Sign up Now!\n\nSep 8: Rock Paper Scissors Hackathon in NYC\n\n11am-4pm in Williamsburg, Brooklyn\n🎟️ SIGN UP!\n\n\n\nSep 9-Oct 14: Optimal Decision Poker Camp in NYC\n\nBehind the Cards and Beyond the Bets: Optimal Decision Making in Gambling and Life\n6 sessions meeting every Monday\n6-8pm in Chinatown\n🎟️ SIGN UP!\n\nNote that the programming and related materials/workshops are for educational purposes and will not use any real money."
  },
  {
    "objectID": "index.html#in-progress-camps",
    "href": "index.html#in-progress-camps",
    "title": "Poker Camp",
    "section": "In Progress Camps",
    "text": "In Progress Camps\nNone currently"
  },
  {
    "objectID": "index.html#upcoming-campsevents",
    "href": "index.html#upcoming-campsevents",
    "title": "Poker Camp",
    "section": "Upcoming Camps/Events",
    "text": "Upcoming Camps/Events\n\nOct TBD: Maybe Optimal Decision Poker Camp in Tel Aviv\n\n1-day workshop\n\nJan 6-31, 2025: Maybe AI Poker Camp Intensive in TBD Location\n\nStructure TBD\n\nFeb 3-Apr 11, 2025: AI Poker Camp Virtual\n\n10 sessions meeting once/week\n\nFeb 3-Apr 11, 2025: Optimal Decision Poker Camp Virtual\n\n10 sessions meeting once/week"
  },
  {
    "objectID": "index.html#completed-camps",
    "href": "index.html#completed-camps",
    "title": "Poker Camp",
    "section": "Completed Camps",
    "text": "Completed Camps\n\nJul 15-Aug 15, 2024: Beta AI Poker Camp in SF\n\n10 sessions meeting on Mondays and Thursday\nThanks to The Commons and 1907 GG for hosting!"
  },
  {
    "objectID": "odpcf24/expectedvalue.html",
    "href": "odpcf24/expectedvalue.html",
    "title": "ODPC #2: Expected Value",
    "section": "",
    "text": "Expected Value",
    "crumbs": [
      "About",
      "Sessions",
      "#2: Expected Value"
    ]
  },
  {
    "objectID": "odpcf24/thinkinginbets.html",
    "href": "odpcf24/thinkinginbets.html",
    "title": "ODPC #1: Thinking in Bets",
    "section": "",
    "text": "Thinking in Bets",
    "crumbs": [
      "About",
      "Sessions",
      "#1: Thinking in Bets"
    ]
  },
  {
    "objectID": "odpcf24/index.html",
    "href": "odpcf24/index.html",
    "title": "Behind the Cards and Beyond the Bets: Optimal Decision Making in Gambling and Life",
    "section": "",
    "text": "Welcome to Behind the Cards and Beyond the Bets: Optimal Decision Making in Gambling and Life"
  },
  {
    "objectID": "odpcf24/changingyourmind.html",
    "href": "odpcf24/changingyourmind.html",
    "title": "ODPC #4: Changing Your Mind",
    "section": "",
    "text": "Changing Your Mind",
    "crumbs": [
      "About",
      "Sessions",
      "#4: Changing Your Mind"
    ]
  }
]