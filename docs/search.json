[
  {
    "objectID": "aicamp/index.html",
    "href": "aicamp/index.html",
    "title": "AI Poker Camp",
    "section": "",
    "text": "Build reinforcement learning AI agents to play games!\nComing soon:\n\nJul 15-Aug 15: Beta in SF\nSep TBD: Beta v2 in NYC\nSep 24-Nov 26: Full course virtual"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Poker Camp",
    "section": "",
    "text": "Poker Camp is a program to learn about probability, game theory, AI, and decision making under uncertainty through the lens of poker.\nOur first camp will take place from Jul 15 to Aug 15 in person in San Francisco.\n\n\n\n\n\n\nSign up now!\n\n\n\nSignup Form\n\nAfter signing up, we‚Äôll be in touch with all details\nThe beta program is free!\n\n\n\nClick here for more info\n\n\n\nNote that the programming and related materials/workshops are for educational purposes and will not use any real money."
  },
  {
    "objectID": "aipcs24/3rps_challenge.html",
    "href": "aipcs24/3rps_challenge.html",
    "title": "#3 Rock Paper Scissors: Challenge",
    "section": "",
    "text": "This week‚Äôs challenge game will be Paper, Scissors, Maybe Rock (PSMR).\nEach player is assigned a probability (\\(X0\\) or \\(X1\\)), which is shown to them but hidden from their opponent. This will be constant for the 1,000 rounds of a match.\nFor each round, each player is dealt a card. With probability \\(X_i\\), player \\(i\\) receives a \\(3\\), otherwise they receive a \\(2\\). Cards are never revealed to a player‚Äôs opponent.\nIf you‚Äôre dealt a \\(3\\), you can play Rock, Paper, or Scissors. If you‚Äôre dealt a \\(2\\), you can only play Paper or Scissors.\nRock beats Scissors beat Paper beat Rock, and the winner of a round wins 1 chip from the loser.\nYou will ultimately play something like 20 matches (of 1k rounds each) against each opponent. We‚Äôll balance matches between pairs of players and across players.\nThe field will include at least:\n\na bot that tries to play \\(1/3\\) - \\(1/3\\) - \\(1/3\\) or as close as it can\na bot that plays what would be Nash for it if the opponent were unconstrained\na bot that is my best attempt to do a reasonable thing, limited by the amount of time we actually decide to spend on it\n\nWe aren‚Äôt intending to say much more about how the \\(X_0\\), \\(X_1\\) probabilities will be generated, and no your bot should not be communicating with itself between matchups or phoning home to you. (It should be storing info for itself round-to-round within a matchup, however.)\nIn the current handout version of challenge-3-psmr in aipc-challenges, you can configure the \\(X0\\), \\(X1\\) by editing duplicates.txt.\nThe game doesn‚Äôt tell these to players yet, though, so at some point we‚Äôd like to push a version that does.\n\n\nAs pre-challenge exercises, you may want to get a CFR solver running and able to solve:\n\nRegular unconstrained RPS\nPSMR with ( \\(X0\\), \\(X1\\) ) = ( \\(1\\), \\(0\\) )\n\nThe connection of these to the bot-writing portion of the challenge is up to you.",
    "crumbs": [
      "Signup: AI Poker Camp Summer 2024",
      "Challenges",
      "#3: Rock Paper Scissors",
      "Challenge"
    ]
  },
  {
    "objectID": "aipcs24/3rps_challenge.html#the-challenge",
    "href": "aipcs24/3rps_challenge.html#the-challenge",
    "title": "#3 Rock Paper Scissors: Challenge",
    "section": "",
    "text": "This week‚Äôs challenge game will be Paper, Scissors, Maybe Rock (PSMR).\nEach player is assigned a probability (\\(X0\\) or \\(X1\\)), which is shown to them but hidden from their opponent. This will be constant for the 1,000 rounds of a match.\nFor each round, each player is dealt a card. With probability \\(X_i\\), player \\(i\\) receives a \\(3\\), otherwise they receive a \\(2\\). Cards are never revealed to a player‚Äôs opponent.\nIf you‚Äôre dealt a \\(3\\), you can play Rock, Paper, or Scissors. If you‚Äôre dealt a \\(2\\), you can only play Paper or Scissors.\nRock beats Scissors beat Paper beat Rock, and the winner of a round wins 1 chip from the loser.\nYou will ultimately play something like 20 matches (of 1k rounds each) against each opponent. We‚Äôll balance matches between pairs of players and across players.\nThe field will include at least:\n\na bot that tries to play \\(1/3\\) - \\(1/3\\) - \\(1/3\\) or as close as it can\na bot that plays what would be Nash for it if the opponent were unconstrained\na bot that is my best attempt to do a reasonable thing, limited by the amount of time we actually decide to spend on it\n\nWe aren‚Äôt intending to say much more about how the \\(X_0\\), \\(X_1\\) probabilities will be generated, and no your bot should not be communicating with itself between matchups or phoning home to you. (It should be storing info for itself round-to-round within a matchup, however.)\nIn the current handout version of challenge-3-psmr in aipc-challenges, you can configure the \\(X0\\), \\(X1\\) by editing duplicates.txt.\nThe game doesn‚Äôt tell these to players yet, though, so at some point we‚Äôd like to push a version that does.\n\n\nAs pre-challenge exercises, you may want to get a CFR solver running and able to solve:\n\nRegular unconstrained RPS\nPSMR with ( \\(X0\\), \\(X1\\) ) = ( \\(1\\), \\(0\\) )\n\nThe connection of these to the bot-writing portion of the challenge is up to you.",
    "crumbs": [
      "Signup: AI Poker Camp Summer 2024",
      "Challenges",
      "#3: Rock Paper Scissors",
      "Challenge"
    ]
  },
  {
    "objectID": "aipcs24/3rps_challenge.html#rock-paper-scissors",
    "href": "aipcs24/3rps_challenge.html#rock-paper-scissors",
    "title": "#3 Rock Paper Scissors: Challenge",
    "section": "Rock Paper Scissors",
    "text": "Rock Paper Scissors\nRock Paper Scissors was proposed in 2023 to be a ‚Äúbenchmark for multiagent learning‚Äù in a paper written primarily by DeepMind.\nThe paper explains that agents are often measured by (a) average return or (b) robustness against a nemesis agent that tries to minimize the agent‚Äôs returns. Yet it‚Äôs important for agents to be able to maximize returns and be robust to adversaries.\nWhy is RPS a good benchmark?\n\nIt‚Äôs a repeated game with sequential decisions\nPerformance is measured against a population of varied skills",
    "crumbs": [
      "Signup: AI Poker Camp Summer 2024",
      "Challenges",
      "#3: Rock Paper Scissors",
      "Challenge"
    ]
  },
  {
    "objectID": "aipcs24/2leduc_leaderboard.html#leduc-poker",
    "href": "aipcs24/2leduc_leaderboard.html#leduc-poker",
    "title": "#2: Leduc Poker | Challenge Leaderboard",
    "section": "Leduc Poker",
    "text": "Leduc Poker",
    "crumbs": [
      "Signup: AI Poker Camp Summer 2024",
      "Challenges",
      "#2: Leduc Poker",
      "Leaderboard"
    ]
  },
  {
    "objectID": "aipcs24/1kuhn_reading.html",
    "href": "aipcs24/1kuhn_reading.html",
    "title": "#1: Kuhn Poker | Reading",
    "section": "",
    "text": "(This game is modified from Lisy et al.¬†2015.)\n\n\n\nBugs and Fudd\n\n\nWe developed this game to show concepts in games with imperfect information that are applicable to our first challenge. Imperfect information games involve hidden information, which are things like opponent cards in poker or enemy location in the Hunting Wabbits game. In perfect information games like chess, all information is available to both players.\nThis game has one chance player (the Author), one maximizing player (Fudd) and one minimizing player (Bugs). Fudd and Bugs are in a long-term, all-out war, and so any energy that Fudd wastes or saves is a gain or loss for Bugs; our game is zero-sum.\nFirst, the Author will choose whether to write an episode where Bugs is at the Opera (50%) or in the Forest (50%). Fudd cannot tell what the Author has chosen.\nNext, Fudd will decide whether to Hunt_Near or Hunt_Far. If he chooses Hunt_Far, he takes -1 value for the extra effort.\nBugs knows whether the Author wrote him into the Opera or Forest, but he does not know Fudd‚Äôs hunting location. If the Author gives him the Opera, Bugs has no choices to make and the game ends after Fudd‚Äôs action. If Forest, Bugs will decide whether to Play_Near or Play_Far. If he plays in the same location as Fudd is hunting, Fudd gets +3 value for a successful hunt (for a total of +2 in the Hunt_Far action due to the -1 value for the extra effort). If they are in different locations (or if Bugs is at the Opera), Fudd will get 0 value for an unsuccessful hunt (-1 for the Hunt_Far misses).\nPutting it all together, the payoff structure of this game is:\n\n\n\nAuthor\nBugs/Fudd\nHunt_Near\nHunt_Far\n\n\n\n\nOpera\n\n0, 0\n+1, -1\n\n\nForest\nPlay_Near\n-3, +3\n+1, -1\n\n\nForest\nPlay_Far\n0, 0\n-2, +2\n\n\n\nThe tree structure is as follows with the payoffs written from the perspective of Fudd:\n\nIf Fudd knows he‚Äôs at the Opera, then he must prefer to Hunt_Near to get a value of \\(0\\) instead of \\(-1\\) for Hunt_Far, but since he doesn‚Äôt know his location, he must take both scenarios into account.\nNote that Bugs‚Äôs optimal actions depend on Fudd‚Äôs Opera strategy even though that outcome cannot be reached once Bugs is playing since Bugs only plays in the Forest! For example if we kept the same game tree except Fudd had a \\(+100\\) Opera Hunt_Far payoff, then he would always Hunt_Far. Bugs would see this and it would affect how Bugs plays in the Forest scenario.\n\n\n\n\n\n\nExercise\n\n\n\nHow would Bugs play in the Forest scenario knowing that Fudd is always playing Hunt_Far?\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nBugs would always Play_Near because then his payoff in the only scenario he can control would be \\(+1\\), whereas Play_Far would get him a payout of \\(-2\\). (The payoffs are all written from the perspective of Fudd, so Bugs‚Äôs are opposite.)",
    "crumbs": [
      "Signup: AI Poker Camp Summer 2024",
      "Challenges",
      "#1: Kuhn Poker",
      "Optional Reading"
    ]
  },
  {
    "objectID": "aipcs24/1kuhn_reading.html#hunting-wabbits-game",
    "href": "aipcs24/1kuhn_reading.html#hunting-wabbits-game",
    "title": "#1: Kuhn Poker | Reading",
    "section": "",
    "text": "(This game is modified from Lisy et al.¬†2015.)\n\n\n\nBugs and Fudd\n\n\nWe developed this game to show concepts in games with imperfect information that are applicable to our first challenge. Imperfect information games involve hidden information, which are things like opponent cards in poker or enemy location in the Hunting Wabbits game. In perfect information games like chess, all information is available to both players.\nThis game has one chance player (the Author), one maximizing player (Fudd) and one minimizing player (Bugs). Fudd and Bugs are in a long-term, all-out war, and so any energy that Fudd wastes or saves is a gain or loss for Bugs; our game is zero-sum.\nFirst, the Author will choose whether to write an episode where Bugs is at the Opera (50%) or in the Forest (50%). Fudd cannot tell what the Author has chosen.\nNext, Fudd will decide whether to Hunt_Near or Hunt_Far. If he chooses Hunt_Far, he takes -1 value for the extra effort.\nBugs knows whether the Author wrote him into the Opera or Forest, but he does not know Fudd‚Äôs hunting location. If the Author gives him the Opera, Bugs has no choices to make and the game ends after Fudd‚Äôs action. If Forest, Bugs will decide whether to Play_Near or Play_Far. If he plays in the same location as Fudd is hunting, Fudd gets +3 value for a successful hunt (for a total of +2 in the Hunt_Far action due to the -1 value for the extra effort). If they are in different locations (or if Bugs is at the Opera), Fudd will get 0 value for an unsuccessful hunt (-1 for the Hunt_Far misses).\nPutting it all together, the payoff structure of this game is:\n\n\n\nAuthor\nBugs/Fudd\nHunt_Near\nHunt_Far\n\n\n\n\nOpera\n\n0, 0\n+1, -1\n\n\nForest\nPlay_Near\n-3, +3\n+1, -1\n\n\nForest\nPlay_Far\n0, 0\n-2, +2\n\n\n\nThe tree structure is as follows with the payoffs written from the perspective of Fudd:\n\nIf Fudd knows he‚Äôs at the Opera, then he must prefer to Hunt_Near to get a value of \\(0\\) instead of \\(-1\\) for Hunt_Far, but since he doesn‚Äôt know his location, he must take both scenarios into account.\nNote that Bugs‚Äôs optimal actions depend on Fudd‚Äôs Opera strategy even though that outcome cannot be reached once Bugs is playing since Bugs only plays in the Forest! For example if we kept the same game tree except Fudd had a \\(+100\\) Opera Hunt_Far payoff, then he would always Hunt_Far. Bugs would see this and it would affect how Bugs plays in the Forest scenario.\n\n\n\n\n\n\nExercise\n\n\n\nHow would Bugs play in the Forest scenario knowing that Fudd is always playing Hunt_Far?\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nBugs would always Play_Near because then his payoff in the only scenario he can control would be \\(+1\\), whereas Play_Far would get him a payout of \\(-2\\). (The payoffs are all written from the perspective of Fudd, so Bugs‚Äôs are opposite.)",
    "crumbs": [
      "Signup: AI Poker Camp Summer 2024",
      "Challenges",
      "#1: Kuhn Poker",
      "Optional Reading"
    ]
  },
  {
    "objectID": "aipcs24/1kuhn_reading.html#concept-information-set",
    "href": "aipcs24/1kuhn_reading.html#concept-information-set",
    "title": "#1: Kuhn Poker | Reading",
    "section": "Concept: Information Set",
    "text": "Concept: Information Set\nIn a perfect information game, we can draw a tree of the possible states the game can be in. Every time a player is called to take an action, they will know what node they are at, and what node they will go to with each legal action.\nIn an imperfect information game, we can still draw that tree, but now a player might be called to take an action without knowing what node they are actually at. Instead, there will be a set of one or more nodes that are indistinguishable to that player based on what they have seen so far, and they will have to take an action knowing only that they are in that set. Such a set of nodes is an information set or an infoset.\nThe infosets contain information about the player and what actions have been seen so far. For example, [Fudd] is an infoset that contains the nodes [Fudd, Forest] and [Fudd, Opera]. (Just [Fudd] because no other actions/information have been revealed to Fudd at this point.)\nA player strategy is a rule that says, for every information set that player will face, what action or (random choice of actions) that player will take. For a game like Hunting Wabbits or Kuhn Poker (the Challenge 1 game), we can list every information set and its probabilities. For a more complicated game, we might write our strategy as a computation that will output probabilities based on inputs and an algorithm.",
    "crumbs": [
      "Signup: AI Poker Camp Summer 2024",
      "Challenges",
      "#1: Kuhn Poker",
      "Optional Reading"
    ]
  },
  {
    "objectID": "aipcs24/1kuhn_reading.html#concept-expected-value",
    "href": "aipcs24/1kuhn_reading.html#concept-expected-value",
    "title": "#1: Kuhn Poker | Reading",
    "section": "Concept: Expected Value",
    "text": "Concept: Expected Value\nOnce we‚Äôve assigned definite values to the ultimate outcomes, the expected value, or EV, of a situation is the value of each outcome weighted by the probability of that outcome.\n\n\n\n\n\n\nExercise\n\n\n\nSuppose that Bugs plays uniform \\(0.5\\) Play_Near and \\(0.5\\) Play_Far.\n\nWhat is the value of each Bugs node and what should Fudd do if he knew Bugs‚Äôs actions? (Recall that the payoff values are from the perspective of Fudd and Bugs uses opposite values.)\nWhat is Fudd‚Äôs expected value in this case?\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nBugs‚Äôs node values are:\n\n\\[\n\\begin{align*}\n\\mathbb{E}(\\text{Left Node}) &= 0.5*-3 + 0.5*0 = -1.5 \\\\\n\\mathbb{E}(\\text{Right Node}) &= 0.5*1 + 0.5*-2 = -0.5\n\\end{align*}\n\\]\n\nThe values for Fudd are inverse, so Fudd prefers the Left Node, which has a value of \\(1.5\\). This means that if Fudd is in the Forest, he prefers to Hunt_Near.\n\n\nWe computed that Fudd prefers Hunt_Near in the Forest scenario and can see on the game tree that he also prefers Hunt_Near in the Opera scenario, so can already know that he will always choose Hunt_Near. Fudd will then have the following expected values:\n\n\\[\n\\begin{equation}\n\\begin{split}\n\\mathbb{E}(\\text{Hunt\\_Near}) &= \\Pr(\\text{Opera})*u(\\text{Hunt\\_Near}) + \\Pr(\\text{Forest})*u(\\text{Hunt\\_Near}) \\\\\n  &= 0.5*0 + 0.5*1.5 \\\\\n  &= 0.75\n\\end{split}\n\\end{equation}\n\\]\n\n\n\nIn imperfect information games, we consider probabilities over two different sources of uncertainty, after assuming a particular P1 strategy and P2 strategy:\n\nUncertainty about which node we are actually in, given that we know that we‚Äôre in one of multiple nodes that we can‚Äôt tell apart. The probabilities of being in node 1, node 2, ‚Ä¶ of an information set can be calculated by the probabilities of strategies upwards in the game tree (and the probabilites of chance events upwards in the game that have already happened). For example, Fudd doesn‚Äôt know if he‚Äôs in the Forest or Opera at the beginning.\nUncertainty about what will happen after we go to a node downwards in the game tree, coming from chance events or strategy probabilities in the players‚Äô following actions. For example, after Fudd selects Hunt_Near, there is uncertainty about the outcome since it depends on Bugs‚Äôs actions.\n\n\n\nWe will focus on zero-sum two-player games, so the value to one player is simply the negative of the value to the other. Therefore, we can represent value in the game as a single number that the maximizing player wishes to make positive and the minimizing player wishes to make negative.\nWe will focus on maximizing (or minimizing) expected value as our goal for all of session 1. One thing that makes it natural to care about expected value is that it‚Äôs usually the best way to predict what your score will be after a very large number of games, whether they are the same game or different from each other.",
    "crumbs": [
      "Signup: AI Poker Camp Summer 2024",
      "Challenges",
      "#1: Kuhn Poker",
      "Optional Reading"
    ]
  },
  {
    "objectID": "aipcs24/1kuhn_reading.html#concept-regret",
    "href": "aipcs24/1kuhn_reading.html#concept-regret",
    "title": "#1: Kuhn Poker | Reading",
    "section": "Concept: Regret",
    "text": "Concept: Regret\nFor a given P1 strategy and P2 strategy, a player has regret when they take an action at an infoset that was not the highest-EV action at that infoset. The amount of regret is the difference between the highest-EV action and the selected action.\n\n\n\n\n\n\nExercise\n\n\n\n\nWhat is the regret for each action?\n\n\n\nAction\nRegret\n\n\n\n\nA\n\n\n\nB\n\n\n\nC\n\n\n\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\n\nAction\nRegret\n\n\n\n\nA\n4\n\n\nB\n2\n\n\nC\n0\n\n\n\n\n\n\nThere are other things that ‚Äúregret‚Äù can mean in English, that are separate from this technical concept:\n\nBased on what chance events later happened, I wish I had taken a different action instead.\nI was wrong about what strategy my opponent was playing, and I wish I had taken a different action instead.\n\nHowever, we will use ‚Äúregret‚Äù as a technical concept to mean how much worse actions that are not-highest-EV perform compared to highest-EV actions given a particular P1 strategy and P2 strategy.",
    "crumbs": [
      "Signup: AI Poker Camp Summer 2024",
      "Challenges",
      "#1: Kuhn Poker",
      "Optional Reading"
    ]
  },
  {
    "objectID": "aipcs24/1kuhn_reading.html#exercises",
    "href": "aipcs24/1kuhn_reading.html#exercises",
    "title": "#1: Kuhn Poker | Reading",
    "section": "Exercises",
    "text": "Exercises\n\n\n\n\n\n\nInformation Set\n\n\n\n\nWhat are Fudd‚Äôs information set(s)?\nWhat are Bugs‚Äôs information set(s)?\n\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nBoth Fudd nodes are a single information set because when Fudd is making the Hunt_Near or Hunt_Far decision, he doesn‚Äôt know whether he‚Äôs in the Opera or the Forest, so his information is the same in both nodes. We can label these simply [Fudd].\nBoth Bugs nodes are also a single information set because although Bugs knows that he‚Äôs in the Forest, he doesn‚Äôt know which action Fudd has taken, so his information is the same in both nodes. We can label these [Bugs, Forest].\n\n\n\n\n\n\n\n\n\n\nExpected Value\n\n\n\nSay that Fudd chooses to Hunt_Near with probability \\(p\\).\n\n\nAt the Bugs infoset where Bugs knows he‚Äôs in the Forest, what is the expected value of choosing to Play_Near?\nWhat is the expected value of choosing to Play_Far?\n\n(Reminder: The payoffs are from the perspective of Fudd and are the opposite for Bugs.)\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\\(\\mathbb{E}(\\text{Play\\_Near}) = -3*p + 1*(1-p) = -4*p + 1\\)\n\\(\\mathbb{E}(\\text{Play\\_Far}) = 0*p + -2*(1-p) = 2*p - 2\\)\n\n\n\n\n\n\n\n\n\n\nExpected Value 2\n\n\n\nSay that Bugs chooses to Play_Near with probability \\(q\\).\n\n\nWhat is Fudd‚Äôs expected value of choosing Hunt_Near at his infoset?\nWhat is Fudd‚Äôs EV of choosing Hunt_Far?\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\\(\\mathbb{E}(\\text{Hunt\\_Near}) = 0.5*0 +0.5*[3*q + 0*(1-q)] = 1.5*q\\)\n\\(\\mathbb{E}(\\text{Hunt\\_Far}) = 0.5*(-1) + 0.5*[-1*q + 2*(1-q)] = -0.5 - 0.5*q + 1 - q = 0.5 - 1.5*q\\)\n\n\n\n\n\n\n\n\n\n\nRegret\n\n\n\nFind a \\(p\\) and a \\(q\\) such that both:\n\nBugs never chooses an action with regret\nFudd never chooses an action with regret\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nIn order to have no regret, the expected value of both actions should be equal\n\n\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nFor Bugs to never choose an action with regret, EV of Play_Near and EV of Play_Far should be equal.\n\n\\(\\mathbb{E}(\\text{Bugs Play\\_Near}) = -4*p + 1\\)\n\\(\\mathbb{E}(\\text{Bugs Play\\_Far}) = 2*p - 2\\)\nSetting equal, we have \\(-4*p + 1 = 2*p - 2 \\Rightarrow 3 = 6*p \\Rightarrow p = \\frac{1}{2} = 0.5\\)\n\\(\\Pr(\\text{Fudd Hunt\\_Near}) = p = 0.5\\)\n\\(\\Pr(\\text{Fudd Hunt\\_Far}) = 1 - p = 0.5\\)\n\nFor Fudd to never choose an action with regret, EV of Hunt_Near and EV of Hunt_Far should be equal.\n\n\\(\\mathbb{E}(\\text{Fudd Hunt\\_Near}) = 1.5*q\\)\n\\(\\mathbb{E}(\\text{Fudd Hunt\\_Far}) = 0.5 - 1.5*q\\)\nSetting equal, we have \\(1.5*q = 0.5 - 1.5*q \\Rightarrow 3*q = 0.5 \\Rightarrow q = \\frac{1}{6} = 0.167\\)\n\\(\\Pr(\\text{Bugs Play\\_Near}) = q = 0.167\\)\n\\(\\Pr(\\text{Bugs Play\\_Far}) = 1 - q = 0.833\\)\nNotice that each player is inducing the other player to have no regret (to be indifferent to both actions) by playing actions at these probabilities, which then result in an equilibrium as shown below:\n\nWe will go deeper into indifference in the next reading.\n\n\n\n\n\n\n\n\n\nGame Value\n\n\n\nWhat is the value of the game (i.e.¬†the expected value as Fudd over the entire game) at the equilibrium strategies found in the previous question?\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nSince the game is zero-sum, we can find Fudd‚Äôs expected value and it is equivalent to the game value:\n\\[\\begin{align}\n\\mathbb{E} &= \\Pr(\\text{Opera}) * \\Pr(\\text{Hunt\\_Near}) * 0 \\\\\n              & + \\Pr(\\text{Opera}) * \\Pr(\\text{Hunt\\_Far}) * -1 \\\\\n              & + \\Pr(\\text{Forest}) * \\Pr(\\text{Hunt\\_Near}) * \\Pr(\\text{Play\\_Near}) * 3 \\\\\n              & + \\Pr(\\text{Forest}) * \\Pr(\\text{Hunt\\_Near}) * \\Pr(\\text{Play\\_Far}) * 0 \\\\\n              & + \\Pr(\\text{Forest}) * \\Pr(\\text{Hunt\\_Far}) * \\Pr(\\text{Play\\_Near}) * -1 \\\\\n              & + \\Pr(\\text{Forest}) * \\Pr(\\text{Hunt\\_Far}) * \\Pr(\\text{Play\\_Far}) * 2 \\\\ \\\\\n    &= 0.5 * 0.5 * 0 \\\\\n    &+ 0.5 * 0.5 * -1 \\\\\n    &+ 0.5 * 0.5 * 0.17 * 3 \\\\\n    &+ 0.5 * 0.5 * 0.83 * 0 \\\\\n    &+ 0.5 * 0.5 * 0.17 * -1 \\\\\n    &+ 0.5 * 0.5 * 0.83 * 2 \\\\ \\\\\n    &= 0 \\\\\n    &+ -0.25 \\\\\n    &+ 0.125 \\\\\n    &+ 0 \\\\\n    &+ -0.042 \\\\\n    &+ 0.42 \\\\ \\\\\n    &= 0.25 \\\\\n\\end{align}\\]\nThis means that if they play repeatedly, we expect Fudd to average a payoff of \\(+0.25\\), while Bugs has a payoff of \\(-0.25\\).",
    "crumbs": [
      "Signup: AI Poker Camp Summer 2024",
      "Challenges",
      "#1: Kuhn Poker",
      "Optional Reading"
    ]
  },
  {
    "objectID": "aipcs24/1kuhn_challenge.html",
    "href": "aipcs24/1kuhn_challenge.html",
    "title": "#1: Kuhn Poker | Challenge (Part 1)",
    "section": "",
    "text": "Enter Part 2 (unlocked) ‚Üí",
    "crumbs": [
      "Signup: AI Poker Camp Summer 2024",
      "Challenges",
      "#1: Kuhn Poker",
      "Challenge"
    ]
  },
  {
    "objectID": "aipcs24/1kuhn_challenge.html#pre-challenge-visualizing-a-game-tree",
    "href": "aipcs24/1kuhn_challenge.html#pre-challenge-visualizing-a-game-tree",
    "title": "#1: Kuhn Poker | Challenge (Part 1)",
    "section": "Pre-Challenge: Visualizing a game tree",
    "text": "Pre-Challenge: Visualizing a game tree\nTo analyze Kuhn Poker further, we‚Äôll represent the game in terms of a visual game tree.\nFirst, deal one card to  and one card to . Each possible deal of the cards forms a separate ‚Äúgame node‚Äù. For example, AK means that  has A and  has K:\n\n\n\n will act first. He knows what his card is, but not what card  has, so he can be in one of 3 information sets, or infosets. An infoset is a set of nodes that are indistinguishable to the player, meaning that they don‚Äôt know which of the states they are in and will act with the same strategy at all of them. They are identified by the player card and the previous actions and are the labels in bold in the game tree.\nWe‚Äôll name ‚Äôs infosets A_, K_, and Q_ based on what card he holds:\n\n\n\nAt each infoset,  can choose ‚ÜëUp or ‚ÜìDown. Note that the probaility with which he chooses ‚ÜëUp (versus ‚ÜìDown) from a given infoset will have to be the same for both nodes, since they are indistinguishable to .\n\n\n\nEach ‚ÜëUp or ‚ÜìDown action will each take us to a distinct game node, based on a unique set of cards and action history:\n\n\n\nNext,  will act. He can observe his card and ‚Äôs first action, but not ‚Äôs card, so those pieces of information characterize his infoset. We‚Äôll give ‚Äôs 6 infosets names like _K‚Üì and _Q‚Üë based on his card and the action:\n\n\n\nAt each of these infosets,  can choose ‚ÜëUp or ‚ÜìDown:\n\n\n\nIf the actions were ‚ÜìDown, ‚ÜëUp (‚Äúcheck‚Äìbet‚Äù), then  will have to act again. Otherwise, the game is now over (with a payoff determined by the cards and the action sequence). Recall the 5 sequences of betting, which end either in one player folding (after an ‚ÜëUp then ‚ÜìDown, a ‚Äúbet‚Äìfold‚Äù) or the higher card winning at showdown, which results from either two ‚ÜìDowns (‚Äúcheck‚Äìcheck‚Äù) or two ‚ÜëUps (‚Äúbet‚Äìcall‚Äù).\nWe‚Äôll write the payoffs from ‚Äôs perspective, and remember that ‚Äôs payoffs will be the inverse:\n\n\n\nIf  still has to act, he‚Äôll be in one of just three infosets, A_‚Üì‚Üë, K_‚Üì‚Üë, or Q_‚Üì‚Üë:\n\n\n\n‚Ä¶and can choose ‚ÜëUp or ‚ÜìDown‚Ä¶\n\n\n\n‚Ä¶but whatever he chooses, the game will end after that move. We now have the entire game tree for Kuhn Poker:\n\n\n\n\n\n\n\n\n\nExercise: Deterministic Places on Tree\n\n\n\n\nEarlier you found situations where a player should take one action 100% of the time. Find these places on the tree.\nPut a light X on parts of the tree that are blocked off (never played) because of these.\n\n\n\n\n\n\n\n\n\nExercise: Following the Tree\n\n\n\n\nPlay a hand of Kuhn Poker with the cards up and follow along with the tree. Point out where you are with your partner.\nPlay a hand of Kuhn Poker with the cards down. Each individual should track where they could be in the tree. Notice where these possibilities overlap.",
    "crumbs": [
      "Signup: AI Poker Camp Summer 2024",
      "Challenges",
      "#1: Kuhn Poker",
      "Challenge"
    ]
  },
  {
    "objectID": "aipcs24/1kuhn_challenge.html#kuhn-poker-strategies",
    "href": "aipcs24/1kuhn_challenge.html#kuhn-poker-strategies",
    "title": "#1: Kuhn Poker | Challenge (Part 1)",
    "section": "Kuhn Poker strategies",
    "text": "Kuhn Poker strategies\nEach player‚Äôs strategy can be completely described in terms of their action probabilities at each infoset (6 of them per player).\n\nExample 1: Determining a local best response\n\n\n\n\n\n\nBeta Note\n\n\n\nAll of the strategy boxes on the whole page are linked together, which is a bad design on our part. This means that if you go to the next example or the final section and change things, the values in this section will no longer make sense. You can get back to a good state by setting all strategy probs to 50%, or opening this page in an incognito window, or by clearing cache including local storage (which will also clear your unlock progress).\n\n\nLet‚Äôs start by considering ‚Äôs decision at the infoset K_‚Üì‚Üë. When  is at K_‚Üì‚Üë, he doesn‚Äôt know whether the true state of the world is KA‚Üì‚Üë or KQ‚Üì‚Üë. This is what the game looks like from ‚Äôs perspective:\n\nAs we said earlier, whatever action (or randomized mix of actions) ‚Äôs strategy says to make, he will be doing so in all of the situations where he arrives at K_‚Üì‚Üë, without the ability to do different things at KA‚Üì‚Üë vs KQ‚Üì‚Üë.\n\n\n\n\n\n\nIf we start by assuming that both players play 50-50 randomly (50% ‚ÜëUp at each infoset), then  would arrive at K_‚Üì‚Üë via KA‚Üì‚Üë equally often as via KQ‚Üì‚Üë. In this case, his expected payoff for playing ‚ÜëUp is a 50%-50% weighted sum of the payoffs KA‚Üì‚Üë‚Üë (-2, calling with K when opponent has A and losing at showdown) and KQ‚Üì‚Üë‚Üë (+2, calling with K when opponent has Q and winning at showdown), for an expected value of 0. The expected value, or EV, of a situation is the value of each outcome weighted by the probability of that outcome.\nSimilarly, his expected payoff for ‚ÜìDown is a 50%-50% weighted sum of the payoffs for KA‚Üì‚Üë‚Üì and KQ‚Üì‚Üë‚Üì, though in this case they‚Äôre both -1 and the EV doesn‚Äôt depend on the composition weights of the infoset (since he folds and loses exactly one chip either way).\nWith these expected values, ‚Äôs at this infoset (holding everything else about both strategies constant) is to play ‚ÜëUp 100% of the time.\n\n\n\n\n\n\nQuestion\n\n\n\nIf we hold everything else about both strategies constant at 50-50, what should  do at K_‚Üì‚Üë? (We‚Äôll call this his local best response.)\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nHolding the rest of both strategies constant,  should always play ‚ÜëUp (‚Äúcall‚Äù).\n\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nHow much does ‚Äôs local best response improve his expected value from the scenario K_‚Üì‚Üë, versus playing 50-50?\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nSwitching from 50% ‚ÜëUp to 100% ‚ÜëUp improves ‚Äôs expected value at K_‚Üì‚Üë by +0.5 (from -0.5 to 0).\n\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nHolding both strategies at 50-50 everywhere else, how much does ‚Äôs local best response improve his expected value of playing the game (versus playing 50-50)?\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nSwitching from 50% ‚ÜëUp to 100% ‚ÜëUp improves ‚Äôs expected value of the whole game by +1/24, or about +0.0417. This combines the previous answer (+0.5) with the probability that any game ends up at K_‚Üì‚Üë (which is 1/12).\n\n\n\nBut what if the probabilities of reaching K_‚Üì‚Üë via KA‚Üì‚Üë versus KQ‚Üì‚Üë aren‚Äôt equal? If the players don‚Äôt play 50-50 randomly, then we‚Äôll have to separately calculate the probability of reaching KA‚Üì‚Üë (including the probability of the initial KA deal) and the probability of reaching KQ‚Üì‚Üë (likewise); the composition of the infoset will be proportional to the the probabilities of reaching these states‚Äîas we‚Äôll see in our next example. (In larger games, we might approximate these reach-state probabilities by sampling games with simulated play instead of calculating them analytically.)\n\n\nExample 2: Changing local best responses\n\n\n\n\n\n\nBeta Note\n\n\n\nThis section also assumes that all other strategy probabilities are set to 50%, including _A‚Üì, though this isn‚Äôt clear in the text. Like in the previous section, you may need to reset the probabilities for the other strategy probs if you changed them in the final section.\n\n\nConsider ‚Äôs actions at _Q‚Üì (‚Äúhaving a Q facing a check‚Äù). If  plays ‚ÜìDown (action sequence: ‚Äúcheck‚Äìcheck‚Äù), he will go to showdown, always have the worse card, and get a payoff of -1.0 (+1.0). If he plays ‚ÜëUp (‚Äúbets‚Äù), then it will be ‚Äôs turn to act.\nIf  were to play randomly at K_‚Üì‚Üë, then ‚Äôs ‚ÜëUp would get a payoff of -0.5 (+0.5), since half the time  will play ‚ÜìDown (‚Äúfold‚Äù, +1) and half the time he will ‚ÜëUp (‚Äúcall‚Äù, showdown, -2).\nNote that ‚Äôs Q is always the worse hand and an ‚ÜëUp action is a bluff, but if  plays ‚ÜìDown (‚Äúfolds‚Äù) often enough, then  can win enough +1s to do better than always taking a 1-chip showdown.\nBut recall that in the previous section, we thought  should always play ‚ÜëUp at K_‚Üì‚Üë. In that case, ‚Äôs payoff for playing ‚ÜëUp at _Q‚Üì becomes -1.25 (+1.25), worse than -1 for playing ‚ÜìDown, and so he should play ‚ÜìDown instead of ‚ÜëUp.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise: Entangled strategies\n\n\n\nReset the probabilities, then set the K_‚Üì‚Üë strategy probability to ‚Äôs local best response. Then set the _Q‚Üì strategy prob to ‚Äôs local best response. What has happened to ‚Äôs local best response?\n\n\n\n\n\n\n\n\nExercise: No-regret mixed strategies\n\n\n\nSet the K_‚Üì‚Üë and _Q‚Üì strategy probabilities to a pair of values such that neither player has regret. A player has regret when they take an action at an infoset that was not the highest-EV action at that infoset. The amount of regret is the difference between the highest-EV action and the selected action. So a player will have no regret if they always take the higher-EV action, or if they mix between actions with equally-highest EV.\n\n\nThis example shows the core difference between solving perfect-information games and imperfect-information games. In Kuhn Poker, when a random  switches to only playing ‚ÜìDown at _Q‚Üì, ‚Äôs payoff EVs at K_‚Üì‚Üë change (because now that infoset is only composed of KA‚Üì‚Üë and no KQ‚Üì‚Üë). With the resulting EVs,  has a payoff of -2 for playing ‚ÜëUp, and should play ‚ÜìDown to get -1 instead. (But now  should‚Ä¶)\nIn a perfect information game, we can find each player‚Äôs best action in each game situation inductively, by passing up the tree from the end-states and determining each situation in terms of known solutions to its successor nodes. And whatever the best thing to do at the downtree node was, it‚Äôll still be the best thing to do, regardless of how we get there.\nBut in an imperfect information game, changing an uptree strategy parameter can change what the best response is at downtree infosets, so we can‚Äôt solve a game in a single pass from the endgames to the beginning. Nearly every game-solving technique for imperfect information games, then, is based on taking a strategy for each player and iteratively improving on both based on local improvements until they resolve on something like a Nash equilibrium.\nThere are two types of uncertainty in an imperfect-information game:\n\nUncertainty about which node we are actually in (depending on strategy and chance probabilities upwards in the game tree).\nUncertainty about what happens after an action (depending on the strategy and chance probabilites downwards in the game tree).",
    "crumbs": [
      "Signup: AI Poker Camp Summer 2024",
      "Challenges",
      "#1: Kuhn Poker",
      "Challenge"
    ]
  },
  {
    "objectID": "aipcs24/1kuhn_challenge.html#challenge",
    "href": "aipcs24/1kuhn_challenge.html#challenge",
    "title": "#1: Kuhn Poker | Challenge (Part 1)",
    "section": "Challenge",
    "text": "Challenge\nThe goal of this challenge is to find optimal strategies for Kuhn Poker in terms of the fixed action probabilities at each player‚Äôs 6 infosets. For this challenge, we recommend that you submit strategies that form a Nash equilibrium (or equivalently, a pair of strategies such that neither player has regret). You will submit strategies for both players (12 probabilities total) and be matched against many opponents (but not yourself); your average score across all matchups will be the basis of your leaderboard rank. For each opponent, you will play an equal number of times as  and as .\n\nPart 1: Manual Solutions\n\n\n\n\n\n\nChallenge part 1: Kuhn Poker (manual solutions)\n\n\n\nUse the boxes below to find a pair of no-regret strategies in the boxes below for  and .\n\n\nWhen you get a pair of strategies such that each infoset is at most 0.1 chips of EV away from regret-free, a link to the next stage will appear at the bottom of the page. You will be able to fine-tune your strategy in the next stage before submitting it to the leaderboard.\n\n\n\n\n\nRun Solver\n\n\nUpdate mode:  100% CFR 10% CFR 1% CFR 0.1% CFR 0.0001% CFR 0.00001% CFR \n\n\nIterations:  1 10 100 1,000 10,000 100,000 \n\n\nSpeed:  Max Fast 10/sec 1/sec \n\n\nTolerance:  0.01 0.03 0.1 0.3 \n\n\n\n\n\n\n\n\nPart 2: Automatic Solver\n\n\n&lt;This section is locked until you complete part 1.&gt;\n\n\n\n\nEnter Part 2 (unlocked) ‚Üí",
    "crumbs": [
      "Signup: AI Poker Camp Summer 2024",
      "Challenges",
      "#1: Kuhn Poker",
      "Challenge"
    ]
  },
  {
    "objectID": "aipcs24/3rps_cfr.html",
    "href": "aipcs24/3rps_cfr.html",
    "title": "#3 Rock Paper Scissors: CFR",
    "section": "",
    "text": "As we think about solving larger games, we start to look at iterative algorithms.\nThe most popular method for iteratively solving poker games is the Counterfactual Regret Minimization (CFR) algorithm. CFR is an iterative algorithm developed in 2007 at the University of Alberta that converges to Nash equilibrium in two player zero-sum games.\nThe handout solver does not exactly use CFR. You can make updates to the solver however you would like, including modifying it to become CFR.\nWhat is a counterfactual?\nActual event: I didn‚Äôt bring an umbrella, and I got wet in the rain\nCounterfactual event: If I had brought an umbrella, I wouldn‚Äôt have gotten wet"
  },
  {
    "objectID": "aipcs24/3rps_cfr.html#counterfactual-regret-minimization",
    "href": "aipcs24/3rps_cfr.html#counterfactual-regret-minimization",
    "title": "#3 Rock Paper Scissors: CFR",
    "section": "",
    "text": "As we think about solving larger games, we start to look at iterative algorithms.\nThe most popular method for iteratively solving poker games is the Counterfactual Regret Minimization (CFR) algorithm. CFR is an iterative algorithm developed in 2007 at the University of Alberta that converges to Nash equilibrium in two player zero-sum games.\nThe handout solver does not exactly use CFR. You can make updates to the solver however you would like, including modifying it to become CFR.\nWhat is a counterfactual?\nActual event: I didn‚Äôt bring an umbrella, and I got wet in the rain\nCounterfactual event: If I had brought an umbrella, I wouldn‚Äôt have gotten wet"
  },
  {
    "objectID": "aipcs24/3rps_cfr.html#cfr-algorithm-parts",
    "href": "aipcs24/3rps_cfr.html#cfr-algorithm-parts",
    "title": "#3 Rock Paper Scissors: CFR",
    "section": "CFR Algorithm Parts",
    "text": "CFR Algorithm Parts\n\nRegret and Strategies\nA strategy at an infoset is a probability distribution over each possible action.\nRegret is a measure of how much each strategy at an infoset is preferred and is used as a way to update strategies.\nFor a given P1 strategy and P2 strategy, a player has regret when they take an action at an infoset that was not the highest-EV action at that infoset.\n\n\n\n\n\n\nRegret Exercise\n\n\n\n\nWhat is the regret for each action?\n\n\n\nAction\nRegret\n\n\n\n\nA\n\n\n\nB\n\n\n\nC\n\n\n\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\n\nAction\nRegret\n\n\n\n\nA\n4\n\n\nB\n2\n\n\nC\n0\n\n\n\n\n\n\n\n\n\n\n\n\nExpected Value Exercise\n\n\n\n\nIf taking a uniform strategy at this node (i.e.¬†\\(\\frac{1}{3}\\) for each action), then what is the expected value of the node?\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\\(\\mathbb{E} = \\frac{1}{3}*1 + \\frac{1}{3}*3+\\frac{1}{3}*5 = 0.33+1+1.67 = 3\\)\n\n\n\n\n\n\n\n\n\nPoker Regret Exercise\n\n\n\n\nIn poker games, the regret for each action is defined as the value for that action minus the expected value of the node. Give the regret values for each action under this definition.\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\n\nAction\nValue\nPoker Regret\n\n\n\n\nA\n1\n-2\n\n\nB\n3\n0\n\n\nC\n5\n2\n\n\n\nAt a node in a poker game, the player prefers actions with higher regrets by this definition.\n\n\n\nEach infoset maintains a strategy and regret tabular counter for each action. These accumulate the sum of all strategies and the sum of all regrets.\nIn a game like Rock Paper Scissors, there is effectively only one infoset, so only one table for strategy over each action (Rock, Paper, Scissors) and one table for regret over each action (Rock, Paper, Scissors).\nRegrets are linked to strategies through a policy called regret matching.\n\nRegret matching\n\nAverage strategy at end\n\n\n\nIterating through the Tree\nThe core feature of the iterative algorithms is self-play by traversing the game tree over all infosets and tracking the strategies and regrets at each.\ncounterfactual values\n\n\nAlternatives to Original Algorithm\n\nCFR+\nLinear CFR\nSampling\nExternal\nChance\nOutcome\nTabular storing strategies and regrets at each infoset\nRegrets based on action values compared to node EV, which is based on counterfactual values\nRegret minimization, usually regret matching, to get new strategies\nAverage strategy converges to Nash equilibrium\nCFR+ variation such that regrets can‚Äôt be &lt;0\nLinear CFR such that regrets are weighted by their recency\nSampling methods\n\nExternal: Sample chance and opponent nodes\nChance: Sample chance only\nOutcome: Sample outcomes"
  },
  {
    "objectID": "aipcs24/1kuhn_challenge2.html",
    "href": "aipcs24/1kuhn_challenge2.html",
    "title": "#1: Kuhn Poker | Challenge (Part 2)",
    "section": "",
    "text": "This section is locked until you complete part 1.\n\n\n\n\n\n\n\n\nChallenge part 2: submission and leaderboard\n\n\n\n\nThe strategy that you used to unlock this stage has been submitted to the leaderboard; you can see it as name [undefined] below. You can resubmit to replace it with another strategy as many times as you like using the ‚ÄúSubmit‚Äù button below. The preceding sentences will be true when the challenge goes live, expect Tuesday or Wednesday.\n\nNext, use the automatic solver tool below to refine your strategies for each player in terms of the fixed action probabilities at each player‚Äôs infosets. For this challenge, we recommend that you submit strategies that form a Nash equilibrium (or equivalently, a pair of strategies such that neither player has regret).\nOnce you have a strategy that you believe improves on your current submission, you can re-submit and wait for the results to be re-run (which may take some minutes).\n\n\n\n\n\n\n\n\n\n\n\nSolver controls\n\n\n\n\n\n\nThe site should save your progress if you navigate away or refresh, though might lose the last few edits, depending.\nIt doesn‚Äôt have any help for sharing solutions between teammates, sorry.\nAt each iteration, the solver will update all 12 nodes (in some arbitrary order), using a rule modified by the update parameters:\n\na magnitude multiplier\nhow to scale the update based on EV (currently supports: no effect or linearly)\nhow to scale the update based on the infoset‚Äôs visit probability (currently supports: no effect or linearly)\nwhether to use a learning rate to decay the magnitude over time (currently supports: no decay, or linear in the sum of updates made to this infoset since reset)\nwhether to update probability or odds\n\nYou can set it to run for a number of iterations. (If you accidentally set it to too many, you can stop the solver by pressing ‚ÄúStop‚Äù or by reloading the page.)\nSpeed is hopefully self-explanatory.\nTolerance controls the difference between action EVs that is too small to update on.\n\n\n\n\n\n\nUpdate:  100% 10% 1% 0.1% 0.01% 0.001%   √ó1 √óEVdiff   √ó1 √óp(visit)   √ó1 /Œ£updates   probability odds \n\n\n\nRun Solver\n\n\nIterations:  1 10 100 1,000 10,000 100,000 \n\n\nSpeed:  Max Fast 10/sec 3/sec 1/sec \n\n\nTolerance:  0.30 chip 0.10 chip 0.03 chip 0.01 chip 0.003 chip \n\n\n\n\n\n\n\n\n\n\n\n\n\nSolver history\n\n\n\n\n\n\nLog:  (this slows the solver down somewhat)\n\n\n\n\nüìã\n\n\n\n\nStrategy\n\n\nA_\n\n\nK_\n\n\nQ_\n\n\n_A‚Üë\n\n\n_A‚Üì\n\n\n_K‚Üë\n\n\n_K‚Üì\n\n\n_Q‚Üë\n\n\n_Q‚Üì\n\n\nA_‚Üì‚Üë\n\n\nK_‚Üì‚Üë\n\n\nQ_‚Üì‚Üë\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEV\n\n\nA_\n\n\nK_\n\n\nQ_\n\n\n_A‚Üë\n\n\n_A‚Üì\n\n\n_K‚Üë\n\n\n_K‚Üì\n\n\n_Q‚Üë\n\n\n_Q‚Üì\n\n\nA_‚Üì‚Üë\n\n\nK_‚Üì‚Üë\n\n\nQ_‚Üì‚Üë\n\n\n\n\n\n\n\n\n\n\n\nLog all:  (this slows the solver down significantly)\n\n\n\n\n\n\n\nOther logs\n\n\n\n\n\n\n\n\n\n\n\n\n\nEV(action=‚ÜëUp)\n\n\nA_\n\n\nK_\n\n\nQ_\n\n\n_A‚Üë\n\n\n_A‚Üì\n\n\n_K‚Üë\n\n\n_K‚Üì\n\n\n_Q‚Üë\n\n\n_Q‚Üì\n\n\nA_‚Üì‚Üë\n\n\nK_‚Üì‚Üë\n\n\nQ_‚Üì‚Üë\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEV(action=‚ÜìDown)\n\n\nA_\n\n\nK_\n\n\nQ_\n\n\n_A‚Üë\n\n\n_A‚Üì\n\n\n_K‚Üë\n\n\n_K‚Üì\n\n\n_Q‚Üë\n\n\n_Q‚Üì\n\n\nA_‚Üì‚Üë\n\n\nK_‚Üì‚Üë\n\n\nQ_‚Üì‚Üë\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nVisit Prob\n\n\nA_\n\n\nK_\n\n\nQ_\n\n\n_A‚Üë\n\n\n_A‚Üì\n\n\n_K‚Üë\n\n\n_K‚Üì\n\n\n_Q‚Üë\n\n\n_Q‚Üì\n\n\nA_‚Üì‚Üë\n\n\nK_‚Üì‚Üë\n\n\nQ_‚Üì‚Üë\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTotal Updates\n\n\nA_\n\n\nK_\n\n\nQ_\n\n\n_A‚Üë\n\n\n_A‚Üì\n\n\n_K‚Üë\n\n\n_K‚Üì\n\n\n_Q‚Üë\n\n\n_Q‚Üì\n\n\nA_‚Üì‚Üë\n\n\nK_‚Üì‚Üë\n\n\nQ_‚Üì‚Üë\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPonder‚Ä¶\n\n\n\nThe solver‚Äôs algorithm is a reinforcement-learning approach that you will use some version of for the rest of the course. Unfortunately, the vanilla version that this page defaults to doesn‚Äôt converge.\nThe solver controls will let you tweak how the algorithm determines the size of the updates, which is critical to having the convergence behavior you want. Try to find a setting of the controls that converges to a good solution.\nFor this task you will almost certainly want to look at the history of updates represented in the ‚ÄúSolver history‚Äù box above.\nWhile Kuhn Poker is small enough to solve by hand or by manual trial-and-error, having an efficient and effective (and converging!) algorithm for learning better strategies is going to be key in later weeks.\n\n\n\n\n\n\n\n\nBeta Note\n\n\n\nThis week‚Äôs challenge (submit a strategy) has drifted apart somewhat from the lesson we tried to build up to (exploring the nuances of reinforcement-update algorithms). Our current thinking is that this would be better if the challenge were actually to submit 100-card Kuhn Poker, which would do a better job of applying the answer to ‚Äúhow does a good solver update?‚Äù\nUnfortunately, we ran out of time to implement the 100-card Kuhn Poker train / test tournament.\n\n\n\n\n\n\n\n\n\n\nBeta Note\n\n\n\nRe-running the tournament currently takes between one and three minutes, and there‚Äôs no indication that it‚Äôs done except the board changing. In some cases, you may have to refresh the page to see changes. Working on improvements.\n\n\n\n\nUpdate Strategy Submission\n\n\n\n\n\n\n\n\n\n\n\n\nMore ‚Üí\n\n\n\n\n\nIf you‚Äôve finished all of the above, we‚Äôd like to hear about it (and any questions you still have‚Äîwhich we expect you do. Then you can do any of:\n\nWait for next week‚Äôs material next week.\nHelp other students with their confusions and stuck points (and let us know how we could have improved!).\nGet a start on the next segment of the course by writing a bot that can learn from your opponent‚Äôs moves and do better than Nash against them. (For this, see the instructors for info on setting up the games environment on your own computer.)"
  },
  {
    "objectID": "aipcs24/1kuhn_challenge2.html#challenge-part-2-automatic-solver",
    "href": "aipcs24/1kuhn_challenge2.html#challenge-part-2-automatic-solver",
    "title": "#1: Kuhn Poker | Challenge (Part 2)",
    "section": "",
    "text": "This section is locked until you complete part 1.\n\n\n\n\n\n\n\n\nChallenge part 2: submission and leaderboard\n\n\n\n\nThe strategy that you used to unlock this stage has been submitted to the leaderboard; you can see it as name [undefined] below. You can resubmit to replace it with another strategy as many times as you like using the ‚ÄúSubmit‚Äù button below. The preceding sentences will be true when the challenge goes live, expect Tuesday or Wednesday.\n\nNext, use the automatic solver tool below to refine your strategies for each player in terms of the fixed action probabilities at each player‚Äôs infosets. For this challenge, we recommend that you submit strategies that form a Nash equilibrium (or equivalently, a pair of strategies such that neither player has regret).\nOnce you have a strategy that you believe improves on your current submission, you can re-submit and wait for the results to be re-run (which may take some minutes).\n\n\n\n\n\n\n\n\n\n\n\nSolver controls\n\n\n\n\n\n\nThe site should save your progress if you navigate away or refresh, though might lose the last few edits, depending.\nIt doesn‚Äôt have any help for sharing solutions between teammates, sorry.\nAt each iteration, the solver will update all 12 nodes (in some arbitrary order), using a rule modified by the update parameters:\n\na magnitude multiplier\nhow to scale the update based on EV (currently supports: no effect or linearly)\nhow to scale the update based on the infoset‚Äôs visit probability (currently supports: no effect or linearly)\nwhether to use a learning rate to decay the magnitude over time (currently supports: no decay, or linear in the sum of updates made to this infoset since reset)\nwhether to update probability or odds\n\nYou can set it to run for a number of iterations. (If you accidentally set it to too many, you can stop the solver by pressing ‚ÄúStop‚Äù or by reloading the page.)\nSpeed is hopefully self-explanatory.\nTolerance controls the difference between action EVs that is too small to update on.\n\n\n\n\n\n\nUpdate:  100% 10% 1% 0.1% 0.01% 0.001%   √ó1 √óEVdiff   √ó1 √óp(visit)   √ó1 /Œ£updates   probability odds \n\n\n\nRun Solver\n\n\nIterations:  1 10 100 1,000 10,000 100,000 \n\n\nSpeed:  Max Fast 10/sec 3/sec 1/sec \n\n\nTolerance:  0.30 chip 0.10 chip 0.03 chip 0.01 chip 0.003 chip \n\n\n\n\n\n\n\n\n\n\n\n\n\nSolver history\n\n\n\n\n\n\nLog:  (this slows the solver down somewhat)\n\n\n\n\nüìã\n\n\n\n\nStrategy\n\n\nA_\n\n\nK_\n\n\nQ_\n\n\n_A‚Üë\n\n\n_A‚Üì\n\n\n_K‚Üë\n\n\n_K‚Üì\n\n\n_Q‚Üë\n\n\n_Q‚Üì\n\n\nA_‚Üì‚Üë\n\n\nK_‚Üì‚Üë\n\n\nQ_‚Üì‚Üë\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEV\n\n\nA_\n\n\nK_\n\n\nQ_\n\n\n_A‚Üë\n\n\n_A‚Üì\n\n\n_K‚Üë\n\n\n_K‚Üì\n\n\n_Q‚Üë\n\n\n_Q‚Üì\n\n\nA_‚Üì‚Üë\n\n\nK_‚Üì‚Üë\n\n\nQ_‚Üì‚Üë\n\n\n\n\n\n\n\n\n\n\n\nLog all:  (this slows the solver down significantly)\n\n\n\n\n\n\n\nOther logs\n\n\n\n\n\n\n\n\n\n\n\n\n\nEV(action=‚ÜëUp)\n\n\nA_\n\n\nK_\n\n\nQ_\n\n\n_A‚Üë\n\n\n_A‚Üì\n\n\n_K‚Üë\n\n\n_K‚Üì\n\n\n_Q‚Üë\n\n\n_Q‚Üì\n\n\nA_‚Üì‚Üë\n\n\nK_‚Üì‚Üë\n\n\nQ_‚Üì‚Üë\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEV(action=‚ÜìDown)\n\n\nA_\n\n\nK_\n\n\nQ_\n\n\n_A‚Üë\n\n\n_A‚Üì\n\n\n_K‚Üë\n\n\n_K‚Üì\n\n\n_Q‚Üë\n\n\n_Q‚Üì\n\n\nA_‚Üì‚Üë\n\n\nK_‚Üì‚Üë\n\n\nQ_‚Üì‚Üë\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nVisit Prob\n\n\nA_\n\n\nK_\n\n\nQ_\n\n\n_A‚Üë\n\n\n_A‚Üì\n\n\n_K‚Üë\n\n\n_K‚Üì\n\n\n_Q‚Üë\n\n\n_Q‚Üì\n\n\nA_‚Üì‚Üë\n\n\nK_‚Üì‚Üë\n\n\nQ_‚Üì‚Üë\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTotal Updates\n\n\nA_\n\n\nK_\n\n\nQ_\n\n\n_A‚Üë\n\n\n_A‚Üì\n\n\n_K‚Üë\n\n\n_K‚Üì\n\n\n_Q‚Üë\n\n\n_Q‚Üì\n\n\nA_‚Üì‚Üë\n\n\nK_‚Üì‚Üë\n\n\nQ_‚Üì‚Üë\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPonder‚Ä¶\n\n\n\nThe solver‚Äôs algorithm is a reinforcement-learning approach that you will use some version of for the rest of the course. Unfortunately, the vanilla version that this page defaults to doesn‚Äôt converge.\nThe solver controls will let you tweak how the algorithm determines the size of the updates, which is critical to having the convergence behavior you want. Try to find a setting of the controls that converges to a good solution.\nFor this task you will almost certainly want to look at the history of updates represented in the ‚ÄúSolver history‚Äù box above.\nWhile Kuhn Poker is small enough to solve by hand or by manual trial-and-error, having an efficient and effective (and converging!) algorithm for learning better strategies is going to be key in later weeks.\n\n\n\n\n\n\n\n\nBeta Note\n\n\n\nThis week‚Äôs challenge (submit a strategy) has drifted apart somewhat from the lesson we tried to build up to (exploring the nuances of reinforcement-update algorithms). Our current thinking is that this would be better if the challenge were actually to submit 100-card Kuhn Poker, which would do a better job of applying the answer to ‚Äúhow does a good solver update?‚Äù\nUnfortunately, we ran out of time to implement the 100-card Kuhn Poker train / test tournament.\n\n\n\n\n\n\n\n\n\n\nBeta Note\n\n\n\nRe-running the tournament currently takes between one and three minutes, and there‚Äôs no indication that it‚Äôs done except the board changing. In some cases, you may have to refresh the page to see changes. Working on improvements.\n\n\n\n\nUpdate Strategy Submission\n\n\n\n\n\n\n\n\n\n\n\n\nMore ‚Üí\n\n\n\n\n\nIf you‚Äôve finished all of the above, we‚Äôd like to hear about it (and any questions you still have‚Äîwhich we expect you do. Then you can do any of:\n\nWait for next week‚Äôs material next week.\nHelp other students with their confusions and stuck points (and let us know how we could have improved!).\nGet a start on the next segment of the course by writing a bot that can learn from your opponent‚Äôs moves and do better than Nash against them. (For this, see the instructors for info on setting up the games environment on your own computer.)"
  },
  {
    "objectID": "aipcs24/2leduc_challenge.html",
    "href": "aipcs24/2leduc_challenge.html",
    "title": "#2: Leduc Poker | Challenge",
    "section": "",
    "text": "This week, you will submit two bots, one to play 100-Card Kuhn Poker, and one to play Leduc Poker.\nSkip ahead to the challenge specification.",
    "crumbs": [
      "Signup: AI Poker Camp Summer 2024",
      "Challenges",
      "#2: Leduc Poker",
      "Challenge"
    ]
  },
  {
    "objectID": "aipcs24/2leduc_challenge.html#kuhn-poker-100-cards",
    "href": "aipcs24/2leduc_challenge.html#kuhn-poker-100-cards",
    "title": "#2: Leduc Poker | Challenge",
    "section": "Kuhn Poker 100 Cards",
    "text": "Kuhn Poker 100 Cards\nKuhn Poker with 100 cards plays the same as Kuhn Poker with 3 cards, but the cards are numbered from 1 to 100 (or 0 to 99).\n\n\n\n\n\n\n100-Card Kuhn Infosets\n\n\n\nKuhn Poker with 3 cards has 6 infosets per player, 12 total.\nHow many infosets are in 100-card Kuhn Poker?\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThese scale linearly and so with 100 cards there are 400 infosets since each card has 4 infosets:\n\nP1 acting first\nP2 facing an Up action\nP2 facing a Down action\nP1 after a Down-Up sequence\n\n\n\n\n\n\n\n\n\n\nGame states for 100-Card Kuhn infosets\n\n\n\nEach infoset in Kuhn Poker has two possible game states that correspond to it.\nHow many game states correspond to each infoset in 100-card Kuhn Poker?\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nEach infoset corresponds to 99 possible game states, one for each card the opponent could have.",
    "crumbs": [
      "Signup: AI Poker Camp Summer 2024",
      "Challenges",
      "#2: Leduc Poker",
      "Challenge"
    ]
  },
  {
    "objectID": "aipcs24/2leduc_challenge.html#leduc-poker",
    "href": "aipcs24/2leduc_challenge.html#leduc-poker",
    "title": "#2: Leduc Poker | Challenge",
    "section": "Leduc Poker",
    "text": "Leduc Poker\nLeduc Poker is a simple toy poker game invented at the University of Alberta.\n\n\n\nAlberta‚Äôs nearby Edmonton Airport is in the city of Leduc\n\n\nHere is the setup:\n\n2 players.\n6 card deck: 2 Queens, 2 Kings, 2 Aces (in ascending order, so Ace is highest).\nEach player antes 1 chip.\nDeal 1 card to each player.\nBetting round 1 (the preflop):\n\nThere is a fixed bet size of 2 chips.\nIf the opponent has made no bet in this round, a player can Check (bet nothing) or Raise (bet one standard bet size).\nIf the opponent has bet, then a player can Fold (bet nothing and lose), Call (add chips to match opponent‚Äôs bet), or Raise (call the bet and add one standard bet size).\nThis round ends when one player Folds (and loses; neither cards are shown), or when one player Calls (and the game continues to the next step below). A special case is when both players Check, which proceeds the same as a Call with no added chips.\n\nDeal a face up community card (shown to both players).\n\nA pair (your card matches the community card) is the best hand, then an unpaired Ace, then an unpaired King, finally an unpaired Queen.\n\nBetting round 2 (the flop):\n\nThere is a fixed bet size of 4 chips.\nPlayers can Check, Fold, Call, and Raise the same as in the first round.\nThis round also ends when one player Folds (and loses), or one player Calls (and proceeds to showdown where the higher hand wins).\n\n\n\n\n\n\n\n\nMaximum bets per round rule in original version\n\n\n\n\n\nThe original version of Leduc Poker has a rule where the maximum bets per betting round is 2 (i.e.¬†a bet and a raise), but we are not using that rule. Instead, each player has a maximum of 50 chips for each game. If a player does not have enough chips to make a full Raise, their Raise is for all their remaining chips.\n\n\n\n\nSample Leduc Hand and Leduc Math\nHere is a Leduc game situation in which:\n\nBoth players ante 1 each.\n\nPot = 2\n\nPreflop: P1 bets 2 and P2 calls.\n\nPot = 6\n\nFlop: Community card K revealed. Player 1 bets 4. Player 2 to act.\n\n\n\n\n\n\n\n\nLeduc Strategy\n\n\n\nWhat should Player 2 do here?\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nRaise! Player 2 has the best possible hand because they have a pair.\n\n\n\n\n\n\n\n\n\nLeduc Infoset\n\n\n\nWhat is Player 2‚Äôs infoset?\nWhat will Player 1‚Äôs infoset be after Player 2 acts?\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nWe could write Player 2‚Äôs infoset as: (P2)[_, K, K][Bet 2, Call 2][Bet 4]\nOr the default solver in pokercamp/aipc-challenges/challenge-2-leduc will write it at: (P2){'community': [2]}[None, 2][Raise, Call, Raise].\n\n\n\n\n\n\n\n\n\nLeduc Ties\n\n\n\nHow often will you and your opponent be dealt the same card?\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nFirst we find the total combinations of cards:\n\\({6\\choose2} = \\frac{6!}{4!*2!} = \\frac{6*5}{2} = 15\\)\nThen we count that there is exactly \\(1\\) way to make Q/Q, \\(1\\) way to make K/K, and \\(1\\) way to make A/A. Therefore the probability of having the same hand as your opponent is \\(\\frac{3}{15} = 0.2\\).\n\n\n\n\n\n\n\n\n\nLeduc Pairs\n\n\n\nSuppose that you are dealt a K. How often will you hit a pair on the flop given that you see it?\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThere are \\(5\\) unknown cards to you and \\(1\\) of them matches yours for a pair, so the \\(\\Pr(\\text{Pair} \\mid \\text{See Flop}) = \\frac{1}{5} = 0.2\\)",
    "crumbs": [
      "Signup: AI Poker Camp Summer 2024",
      "Challenges",
      "#2: Leduc Poker",
      "Challenge"
    ]
  },
  {
    "objectID": "aipcs24/2leduc_challenge.html#hints",
    "href": "aipcs24/2leduc_challenge.html#hints",
    "title": "#2: Leduc Poker | Challenge",
    "section": "Hints",
    "text": "Hints\n\n\n\n\n\n\nCard abstractions in 100-Card Kuhn\n\n\n\n\n\nIt might be more efficient to solve 100-Card Kuhn Poker if you shrink the number of cards to a more manageable size by bucketing a group of nearby numbers together for strategy purposes. For example, you could treat cards 1-10 as the same, 11-20, and so on. Or is there a better way to bucket than uniformly?\n\n\n\n\n\n\n\n\n\nSampling Policy in Leduc\n\n\n\n\n\nThe set of possible action histories in Leduc Poker with 50-chip stacks is relatively large, and you don‚Äôt really care about most of it. Consider whether you can do something more efficient than always expanding every node.\n\n\n\n\n\n\n\n\n\nAveraging intermediate strategy probabilities\n\n\n\n\n\nAs you may have seen with the Kuhn solver site, counterfactual regret minimization tends to cycle around the equilibrium instead of descending into it. If you use some kind of average over recent strategy probs as your final probabilities, you may get much closer to Nash than just using the final values.\n\n\n\n\n\n\n\n\n\nKuhn Game Value Bonus Challenge\n\n\n\nFind an equilibrium strategy for 100 card and 3 card Kuhn Poker and compare the P2 advantage in 100 card Kuhn Poker to 3 card Kuhn Poker.\nExtra bonus: Compare the P2 advantage in 100 card Kuhn Poker to a uniform 10-bucket abstraction, and if possible, a better 10-bucket abstraction.\n\n\n\n\n\n\n\n\nLeduc Game Value Bonus Challenge\n\n\n\nFind an equilibrium strategy for Leduc Poker and compare the P2 advantage to 3-Card Kuhn and 100-Card Kuhn.",
    "crumbs": [
      "Signup: AI Poker Camp Summer 2024",
      "Challenges",
      "#2: Leduc Poker",
      "Challenge"
    ]
  },
  {
    "objectID": "aipcs24/2leduc_class.html",
    "href": "aipcs24/2leduc_class.html",
    "title": "#2: Leduc Poker | Class Materials",
    "section": "",
    "text": "Typically solving a 2-player poker game is defined as finding a Nash equilibrium strategy for both players. This is straightforward to define and has been the target of much poker research, but means finding a fixed strategy that doesn‚Äôt adapt to opponents and might not be the most profitable strategy in a field of a variety of opponents.\nSmall poker games can be solved through linear programming given a matrix of strategies at each information set and a matrix of payoffs (see here for more details).\nAs we prepare to solve larger games, we start to look at iterative algorithms.\nThe core feature of the iterative algorithms is self-play by traversing the game tree over all infosets and tracking the strategies and regrets at each.\n\n\nRegret is a measure of how much each strategy at an infoset is preferred and is used as a way to update strategies.\nFor a given P1 strategy and P2 strategy, a player has regret when they take an action at an infoset that was not the highest-EV action at that infoset.\n\n\n\n\n\n\nRegret Exercise\n\n\n\n\nWhat is the regret for each action?\n\n\n\nAction\nRegret\n\n\n\n\nA\n\n\n\nB\n\n\n\nC\n\n\n\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\n\nAction\nRegret\n\n\n\n\nA\n4\n\n\nB\n2\n\n\nC\n0\n\n\n\n\n\n\n\n\n\n\n\n\nExpected Value Exercise\n\n\n\n\nIf taking a uniform strategy at this node (i.e.¬†\\(\\frac{1}{3}\\) for each action), then what is the expected value of the node?\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\\(\\mathbb{E} = \\frac{1}{3}*1 + \\frac{1}{3}*3+\\frac{1}{3}*5 = 0.33+1+1.67 = 3\\)\n\n\n\n\n\n\n\n\n\nPoker Regret Exercise\n\n\n\n\nIn poker games, the regret for each action is defined as the value for that action minus the expected value of the node. Give the regret values for each action under this definition.\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\n\nAction\nValue\nPoker Regret\n\n\n\n\nA\n1\n-2\n\n\nB\n3\n0\n\n\nC\n5\n2\n\n\n\nAt a node in a poker game, the player prefers actions with higher regrets by this definition.\n\n\n\n\n\n\nThe most popular method for iteratively solving poker games is the Counterfactual Regret Minimization (CFR) algorithm. A good resource on CFR is this 2015 paper from the University of Alberta.\nThe handout solver does not exactly use CFR. You can make updates to the solver however you would like, including modifying it to become CFR.\nWhat is a counterfactual?\nActual event: I didn‚Äôt bring an umbrella, and I got wet in the rain\nCounterfactual event: If I had brought an umbrella, I wouldn‚Äôt have gotten wet\n\n\n\nTabular storing strategies and regrets at each infoset\nRegrets based on action values compared to node EV, which is based on counterfactual values\nRegret minimization, usually regret matching, to get new strategies\nAverage strategy converges to Nash equilibrium\nCFR+ variation such that regrets can‚Äôt be &lt;0\nLinear CFR such that regrets are weighted by their recency\nSampling methods\n\nExternal: Sample chance and opponent nodes\nChance: Sample chance only\nOutcome: Sample outcomes",
    "crumbs": [
      "Signup: AI Poker Camp Summer 2024",
      "Challenges",
      "#2: Leduc Poker",
      "Class Materials"
    ]
  },
  {
    "objectID": "aipcs24/2leduc_class.html#solving-poker-games",
    "href": "aipcs24/2leduc_class.html#solving-poker-games",
    "title": "#2: Leduc Poker | Class Materials",
    "section": "",
    "text": "Typically solving a 2-player poker game is defined as finding a Nash equilibrium strategy for both players. This is straightforward to define and has been the target of much poker research, but means finding a fixed strategy that doesn‚Äôt adapt to opponents and might not be the most profitable strategy in a field of a variety of opponents.\nSmall poker games can be solved through linear programming given a matrix of strategies at each information set and a matrix of payoffs (see here for more details).\nAs we prepare to solve larger games, we start to look at iterative algorithms.\nThe core feature of the iterative algorithms is self-play by traversing the game tree over all infosets and tracking the strategies and regrets at each.\n\n\nRegret is a measure of how much each strategy at an infoset is preferred and is used as a way to update strategies.\nFor a given P1 strategy and P2 strategy, a player has regret when they take an action at an infoset that was not the highest-EV action at that infoset.\n\n\n\n\n\n\nRegret Exercise\n\n\n\n\nWhat is the regret for each action?\n\n\n\nAction\nRegret\n\n\n\n\nA\n\n\n\nB\n\n\n\nC\n\n\n\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\n\nAction\nRegret\n\n\n\n\nA\n4\n\n\nB\n2\n\n\nC\n0\n\n\n\n\n\n\n\n\n\n\n\n\nExpected Value Exercise\n\n\n\n\nIf taking a uniform strategy at this node (i.e.¬†\\(\\frac{1}{3}\\) for each action), then what is the expected value of the node?\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\\(\\mathbb{E} = \\frac{1}{3}*1 + \\frac{1}{3}*3+\\frac{1}{3}*5 = 0.33+1+1.67 = 3\\)\n\n\n\n\n\n\n\n\n\nPoker Regret Exercise\n\n\n\n\nIn poker games, the regret for each action is defined as the value for that action minus the expected value of the node. Give the regret values for each action under this definition.\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\n\nAction\nValue\nPoker Regret\n\n\n\n\nA\n1\n-2\n\n\nB\n3\n0\n\n\nC\n5\n2\n\n\n\nAt a node in a poker game, the player prefers actions with higher regrets by this definition.\n\n\n\n\n\n\nThe most popular method for iteratively solving poker games is the Counterfactual Regret Minimization (CFR) algorithm. A good resource on CFR is this 2015 paper from the University of Alberta.\nThe handout solver does not exactly use CFR. You can make updates to the solver however you would like, including modifying it to become CFR.\nWhat is a counterfactual?\nActual event: I didn‚Äôt bring an umbrella, and I got wet in the rain\nCounterfactual event: If I had brought an umbrella, I wouldn‚Äôt have gotten wet\n\n\n\nTabular storing strategies and regrets at each infoset\nRegrets based on action values compared to node EV, which is based on counterfactual values\nRegret minimization, usually regret matching, to get new strategies\nAverage strategy converges to Nash equilibrium\nCFR+ variation such that regrets can‚Äôt be &lt;0\nLinear CFR such that regrets are weighted by their recency\nSampling methods\n\nExternal: Sample chance and opponent nodes\nChance: Sample chance only\nOutcome: Sample outcomes",
    "crumbs": [
      "Signup: AI Poker Camp Summer 2024",
      "Challenges",
      "#2: Leduc Poker",
      "Class Materials"
    ]
  },
  {
    "objectID": "signup/index.html",
    "href": "signup/index.html",
    "title": "AI Poker Camp (2024 Summer Beta SF)",
    "section": "",
    "text": "Sign up now!\n\n\n\nSignup Form\n\nAfter signing up, we‚Äôll be in touch with all details\nThe beta program is free!"
  },
  {
    "objectID": "signup/index.html#when",
    "href": "signup/index.html#when",
    "title": "AI Poker Camp (2024 Summer Beta SF)",
    "section": "When?",
    "text": "When?\n6pm-8pm Mondays and Thursdays, July 15 through August 15."
  },
  {
    "objectID": "signup/index.html#what",
    "href": "signup/index.html#what",
    "title": "AI Poker Camp (2024 Summer Beta SF)",
    "section": "What?",
    "text": "What?\nA five-week, twice-weekly course on applied game theory through you writing AIs to play games. By the end, you should be able to write an AI to play poker.\nThis is a beta test of a course we‚Äôre planning to run online in the fall, so it‚Äôll be small. We‚Äôll cap signups somewhere between 16 and 24 students."
  },
  {
    "objectID": "signup/index.html#where",
    "href": "signup/index.html#where",
    "title": "AI Poker Camp (2024 Summer Beta SF)",
    "section": "Where?",
    "text": "Where?\nStrictly in-person in San Francisco. Location to be announced.\n\nWait, really? I can‚Äôt make that!\nWe‚Äôre also planning to run an online version of the course starting in late September. You can join our mailing list to learn more when it‚Äôs announced."
  },
  {
    "objectID": "signup/index.html#who",
    "href": "signup/index.html#who",
    "title": "AI Poker Camp (2024 Summer Beta SF)",
    "section": "Who?",
    "text": "Who?\n\nWe recommend students be able to program in Python and perform a Bayesian update (though it‚Äôs fine to lean on an LLM for help on either).\nKnowledge of the game of poker is not necessary. (We are offering a 2 hour Poker Basics workshop on Sun Jul 7.)"
  },
  {
    "objectID": "signup/index.html#whats-the-curriculum",
    "href": "signup/index.html#whats-the-curriculum",
    "title": "AI Poker Camp (2024 Summer Beta SF)",
    "section": "What‚Äôs the curriculum?",
    "text": "What‚Äôs the curriculum?\nThe course is built around six or seven practical challenges ‚Äì think ‚Äúkaggle competition for game-playing programs‚Äù. These will cover:\n\nTutorial: One-card / Kuhn Poker\n\nTopic: Algorithms for solving incomplete-information games.\n\nLarger one-card poker formats and other simple games\n\nTopic: Scaling up algorithms to larger game trees.\n\nRock-Paper-Scissors against imperfect opponents\n\nTopic: Techniques for modeling empirical opponent behavior.\n\nHidden-information version of Probabilistic Tic-Tac-Toe\n\nTopic: Modeling hidden information from opponent actions.\n\nTexas Holdem with simplified betting\n\nTopic: Putting it together!\n\n\nWe‚Äôre intending for the Summer Beta to have about the intensity of one (1) college course in applied CS. You should expect to make at least 9 out of 10 class sessions."
  },
  {
    "objectID": "signup/index.html#whos-teaching",
    "href": "signup/index.html#whos-teaching",
    "title": "AI Poker Camp (2024 Summer Beta SF)",
    "section": "Who‚Äôs teaching?",
    "text": "Who‚Äôs teaching?\nRoss Rheingans-Yoo wrote the advanced trading simulations for the Jane Street trading internship program from 2018 to 2021.\nMax Chiswick is a former poker pro who has played more than 10 million hands of online poker, and created AI Poker Tutorial.\nRicki Heicklen is a curriculum advisor (but will not be teaching in the SF Beta)."
  },
  {
    "objectID": "signup/index.html#how",
    "href": "signup/index.html#how",
    "title": "AI Poker Camp (2024 Summer Beta SF)",
    "section": "How?",
    "text": "How?\nSignup Form\n\nAfter signing up, we‚Äôll be in touch with all details\nThe beta program is free!"
  },
  {
    "objectID": "signup/index.html#something-else",
    "href": "signup/index.html#something-else",
    "title": "AI Poker Camp (2024 Summer Beta SF)",
    "section": "Something else?",
    "text": "Something else?\nGet in touch!"
  },
  {
    "objectID": "aipcs24/2leduc_challenge copy.html",
    "href": "aipcs24/2leduc_challenge copy.html",
    "title": "#2: Leduc Poker | Challenge",
    "section": "",
    "text": "Leduc Poker is a simple toy poker game invented at the University of Alberta.\nHere is the setup:\n\n6 card deck: 2 Queens, 2 Kings, 2 Aces (in ascending order, so Ace is highest)\nLeduc Poker is played with 2 players. We‚Äôll again use  and .\nEach player antes 1 chip\nDeal 1 card to each player\nBetting round 1:\n\nThere is a fixed bet size of 2 chips\nThere is a maximum of 2 bets per round (i.e.¬†a bet and a raise)\n\nDeal a face up community card\n\nPlayers make the best 2 card hand combining their card and the community card, meaning a pair is the best possible hand\n\nBetting round 2:\n\nThere is a fixed bet size of 4 chips\nThere is a maximum of 2 bets per round (i.e.¬†a bet and a raise)\n\nNotes:\n\nPlayer 1 acts first, rotate who is Player 1 each hand\nPlayers can win/lose a maximum of 13 chips per hand\n\nInfosets and payoffs\n\n - ‚ÜëUp (putting a chip into the pot)\n\n‚ÜìDown (not putting a chip into the pot)\n\n\n\n\n\n\n\n‚ÜëUp and ‚ÜìDown in traditional poker terms\n\n\n\n\n‚ÜëUp actions indicate a Bet or Call.\n‚ÜìDown actions indicate a Check or Fold.\n\n\n\n\n\n\n\n\n\nExample game\n\n\n\n\nPlayers ante and cards are dealt.\n sees a A and plays ‚ÜëUp (1 more chip into pot).\n sees a K and plays ‚ÜëUp (1 more chip into pot).\nBoth cards are revealed in a 2-chip showdown.  has an A and  has a K.\n has the better hand and wins +2 chips (+1 from the ante, +1 from ‚Äôs ‚ÜëUp).\n\n\n\nThe betting (and the game) can go in three ways:\n\nOne player plays ‚ÜëUp, then the other player plays ‚ÜìDown. The player who played ‚ÜìDown folds. The winner wins the loser‚Äôs ante (and gets their own chip back). The players‚Äô cards are not revealed. Note that this happens if the action is :\n\n\n‚ÜëUp, ‚ÜìDown, or\n‚ÜìDown, ‚ÜëUp, ‚ÜìDown.\n\n\nBoth players play ‚ÜìDown. They go to a showdown and the winner wins the one chip that the loser anted (and their own back).\nA player plays ‚ÜëUp, then the other player plays ‚ÜëUp. They go to a showdown and the winner wins the two chips the loser has put in the pot (and gets their own chips back). Note that the game will proceed to a 2-chip showdown if the action is:\n\n\n‚ÜëUp, ‚ÜëUp or - ‚ÜìDown, ‚ÜëUp , ‚ÜëUp.\n\nHere is a list of all possible betting sequences:\n\n\n\n\n\n\n\n\n\n\n\n\nWinner\n\n\n\n\n‚ÜëUp\n‚ÜìDown\n\n (+1)\n\n\n‚ÜëUp\n‚ÜëUp\n\nHigher Card (+2)\n\n\n‚ÜìDown\n‚ÜìDown\n\nHigher Card (+1)\n\n\n‚ÜìDown\n‚ÜëUp\n‚ÜìDown\n (+1)\n\n\n‚ÜìDown\n‚ÜëUp\n‚ÜëUp\nHigher Card (+2)\n\n\n\n\n\n\n\n\n\nPartner Exercise: Get started with Kuhn Poker\n\n\n\n\nGet cards, chips, paper, pen\nPlay 3 hands of Kuhn Poker and get used to how the game runs\nUse the pen and paper to start writing down all deterministic situations in the game (situations where there is clearly a correct move that you should take 100% of the time)\nPlay more hands as you think helpful\nOnce you have what you think is a full list of the deterministic states, you can stop and review the optional reading"
  },
  {
    "objectID": "aipcs24/2leduc_challenge copy.html#leduc-poker-rules",
    "href": "aipcs24/2leduc_challenge copy.html#leduc-poker-rules",
    "title": "#2: Leduc Poker | Challenge",
    "section": "",
    "text": "Leduc Poker is a simple toy poker game invented at the University of Alberta.\nHere is the setup:\n\n6 card deck: 2 Queens, 2 Kings, 2 Aces (in ascending order, so Ace is highest)\nLeduc Poker is played with 2 players. We‚Äôll again use  and .\nEach player antes 1 chip\nDeal 1 card to each player\nBetting round 1:\n\nThere is a fixed bet size of 2 chips\nThere is a maximum of 2 bets per round (i.e.¬†a bet and a raise)\n\nDeal a face up community card\n\nPlayers make the best 2 card hand combining their card and the community card, meaning a pair is the best possible hand\n\nBetting round 2:\n\nThere is a fixed bet size of 4 chips\nThere is a maximum of 2 bets per round (i.e.¬†a bet and a raise)\n\nNotes:\n\nPlayer 1 acts first, rotate who is Player 1 each hand\nPlayers can win/lose a maximum of 13 chips per hand\n\nInfosets and payoffs\n\n - ‚ÜëUp (putting a chip into the pot)\n\n‚ÜìDown (not putting a chip into the pot)\n\n\n\n\n\n\n\n‚ÜëUp and ‚ÜìDown in traditional poker terms\n\n\n\n\n‚ÜëUp actions indicate a Bet or Call.\n‚ÜìDown actions indicate a Check or Fold.\n\n\n\n\n\n\n\n\n\nExample game\n\n\n\n\nPlayers ante and cards are dealt.\n sees a A and plays ‚ÜëUp (1 more chip into pot).\n sees a K and plays ‚ÜëUp (1 more chip into pot).\nBoth cards are revealed in a 2-chip showdown.  has an A and  has a K.\n has the better hand and wins +2 chips (+1 from the ante, +1 from ‚Äôs ‚ÜëUp).\n\n\n\nThe betting (and the game) can go in three ways:\n\nOne player plays ‚ÜëUp, then the other player plays ‚ÜìDown. The player who played ‚ÜìDown folds. The winner wins the loser‚Äôs ante (and gets their own chip back). The players‚Äô cards are not revealed. Note that this happens if the action is :\n\n\n‚ÜëUp, ‚ÜìDown, or\n‚ÜìDown, ‚ÜëUp, ‚ÜìDown.\n\n\nBoth players play ‚ÜìDown. They go to a showdown and the winner wins the one chip that the loser anted (and their own back).\nA player plays ‚ÜëUp, then the other player plays ‚ÜëUp. They go to a showdown and the winner wins the two chips the loser has put in the pot (and gets their own chips back). Note that the game will proceed to a 2-chip showdown if the action is:\n\n\n‚ÜëUp, ‚ÜëUp or - ‚ÜìDown, ‚ÜëUp , ‚ÜëUp.\n\nHere is a list of all possible betting sequences:\n\n\n\n\n\n\n\n\n\n\n\n\nWinner\n\n\n\n\n‚ÜëUp\n‚ÜìDown\n\n (+1)\n\n\n‚ÜëUp\n‚ÜëUp\n\nHigher Card (+2)\n\n\n‚ÜìDown\n‚ÜìDown\n\nHigher Card (+1)\n\n\n‚ÜìDown\n‚ÜëUp\n‚ÜìDown\n (+1)\n\n\n‚ÜìDown\n‚ÜëUp\n‚ÜëUp\nHigher Card (+2)\n\n\n\n\n\n\n\n\n\nPartner Exercise: Get started with Kuhn Poker\n\n\n\n\nGet cards, chips, paper, pen\nPlay 3 hands of Kuhn Poker and get used to how the game runs\nUse the pen and paper to start writing down all deterministic situations in the game (situations where there is clearly a correct move that you should take 100% of the time)\nPlay more hands as you think helpful\nOnce you have what you think is a full list of the deterministic states, you can stop and review the optional reading"
  },
  {
    "objectID": "aipcs24/2leduc_challenge copy.html#types-of-games",
    "href": "aipcs24/2leduc_challenge copy.html#types-of-games",
    "title": "#2: Leduc Poker | Challenge",
    "section": "Types of Games",
    "text": "Types of Games\n\n\n\n\n\n\nExercise\n\n\n\nFill in the table below with games you know about. Thanks to Eliezer for getting us started.\n\n\n\n\n\n\n\n\n\n\nGame/Opponent\nFixed/Probabilistic Opponent\nAdversarial Opponent\n\n\n\n\nImperfect Info, No Player Agency/Decisions\n\n\n\n\nPerfect Info, Player Actions Always Perfect Info\n\n\n\n\nImperfect Info, Imperfect from Randomness\n\n\n\n\nImperfect Info, Imperfect from Randomness AND Game States\n\n\n\n\n\n\n\n\n\n\n\nPre-Filled Table\n\n\n\n\n\n\n\n\n\n\n\n\n\nGame/Opponent\nFixed/Probabilistic Opponent\nAdversarial Opponent\n\n\n\n\nImperfect Info, No Player Agency/Decisions\nCandy Land, War, Dreidel, Bingo, Chutes and Ladders, Slot machine\n\n\n\nPerfect Info, Player Actions Always Perfect Info\nPuzzles\nTictactoe, Checkers, Chess, Arimaa, Go\n\n\nImperfect Info, Imperfect from Randomness\nBlackjack\nBackgammon\n\n\nImperfect Info, Imperfect from Randomness AND Game States\nPartial Info Multi-armed Bandit\nPoker, Rock Paper Scissors, Liar‚Äôs Dice, Figgie\n\n\n\n\n\n\nWhat about solitaire? With Blackjack? What about a lottery? Mahjong? Tennis value of states in RL\nPure strategies in perfect info games vs.¬†mixed in imperfect info\nDeterministic Nature: Because all players can see the entire game state and know all possible moves, strategies can be deterministic. Players can calculate and choose the optimal move based on this complete information. Pure Strategies: A pure strategy is a complete plan of action for every possible situation in a game. In perfect information games, players can follow a pure strategy because they know exactly what will happen as a result of each possible move.\nUncertainty and Hidden Information: Because players cannot see the entire game state, they must account for uncertainty and the hidden information of their opponents. This makes the game more about probabilities and expectations rather than certainties. Mixed Strategies: A mixed strategy involves randomizing over possible moves to prevent opponents from exploiting predictable patterns. By using mixed strategies, players can become less predictable and make it more difficult for opponents to formulate a counter-strategy.\nStrategic Randomization: In games like poker, where bluffing and deception play significant roles, mixed strategies are essential. For example, a player might choose to bluff (make a bet with a weak hand) with a certain probability to keep opponents guessing and to avoid being exploited by always playing in a predictable manner.\n(Ross note: The difference between ‚ÄúChance‚Äù and ‚ÄúImperfect Info‚Äù is that in Chance, the unknown [thing] doesn‚Äôt affect anything about the world until it becomes known, and then it‚Äôs not unknown any more. In Imperfect Info, the information has some effect on the world at time T1, then you need to make a decision at time T2, then the information will matter at some later point T3.)\n\n\n\n\n\n\nExercise\n\n\n\nWhat makes poker and other games in the bottom right of the table interesting?"
  },
  {
    "objectID": "aipcs24/2leduc_challenge copy.html#simulator",
    "href": "aipcs24/2leduc_challenge copy.html#simulator",
    "title": "#2: Leduc Poker | Challenge",
    "section": "Simulator",
    "text": "Simulator\nRandom, reward avg, etc.\nDescribe the exploration vs.¬†exploitation dilemma. Introduce basic strategies: epsilon-greedy, UCB (Upper Confidence Bound), and Thompson Sampling.\nRecord the results and display them in real-time (either through the program or manually on a board). Strategy Discussion:\nAfter a few rounds, pause and discuss the strategies teams are using. Introduce the different algorithms and how they would approach the problem.\nAlgorithm Implementation:\nAllow teams to adopt one of the introduced algorithms for the next rounds. Compare the performance of different algorithms in terms of accumulated rewards.\n\n\n\n\n\nArm\n\n\nAverage Reward\n\n\nPulls\n\n\nActions\n\n\n\n\n\n\n\n\n\nReset"
  },
  {
    "objectID": "aipcs24/2leduc_challenge copy.html#solving-poker-games",
    "href": "aipcs24/2leduc_challenge copy.html#solving-poker-games",
    "title": "#2: Leduc Poker | Challenge",
    "section": "Solving Poker Games",
    "text": "Solving Poker Games\n\nKuhn Normal Form\nKuhn Game Tree"
  },
  {
    "objectID": "aipcs24/2leduc_challenge copy.html#cfr",
    "href": "aipcs24/2leduc_challenge copy.html#cfr",
    "title": "#2: Leduc Poker | Challenge",
    "section": "CFR",
    "text": "CFR\nSlides, what to include?"
  },
  {
    "objectID": "aipcs24/solvers.html",
    "href": "aipcs24/solvers.html",
    "title": "Implementing a Basic Solver",
    "section": "",
    "text": "We‚Äôve provided a basic game-solver with the challenges code. You can find it at aipcs-challenges/solvers/default/ and run it with python solver.py --iter N.\nIn its base form, it will perform a simple version of the Counterfactual Regret Minimization Algorithm, computing expected values of each action and updating action probabilities towards the actions with higher EV (much like the Kuhn automatic-solver page).\nYou‚Äôre also welcome to write whatever else you want; we just thought this might be helpful for getting started.",
    "crumbs": [
      "Signup: AI Poker Camp Summer 2024",
      "Configuration",
      "Building and Running Solvers"
    ]
  },
  {
    "objectID": "aipcs24/solvers.html#solver-framework",
    "href": "aipcs24/solvers.html#solver-framework",
    "title": "Implementing a Basic Solver",
    "section": "",
    "text": "We‚Äôve provided a basic game-solver with the challenges code. You can find it at aipcs-challenges/solvers/default/ and run it with python solver.py --iter N.\nIn its base form, it will perform a simple version of the Counterfactual Regret Minimization Algorithm, computing expected values of each action and updating action probabilities towards the actions with higher EV (much like the Kuhn automatic-solver page).\nYou‚Äôre also welcome to write whatever else you want; we just thought this might be helpful for getting started.",
    "crumbs": [
      "Signup: AI Poker Camp Summer 2024",
      "Configuration",
      "Building and Running Solvers"
    ]
  },
  {
    "objectID": "aipcs24/solvers.html#the-algorithm",
    "href": "aipcs24/solvers.html#the-algorithm",
    "title": "Implementing a Basic Solver",
    "section": "The algorithm",
    "text": "The algorithm\n\nBegin at the first turn of the game, with Chance actions determined randomly.\nAt each infoset, we will either use the sampling policy sample or expand_all (determined by get_sampling_policy()) to get an estimate of the expected value of this node.\n\nIf sample, we pick an action randomly based on get_training_strategy_probabilities() for the current infoset, and use the value of the state that takes us to as the value of this state.\nIf expand_all, then get the values of each possible successor state, and use the weighted average by get_training_strategy_probabilities() to get the expected value of this state.\n\nEach time we do expand_all (including during a recursive drill-down step), update the strategy probabiliites for this infoset based on which actions did better (in this state) than this state‚Äôs overall expected value (weighted over all actions).\n\nIn particular, we keep a running sum of the amount each action beat the EV by (floored at zero), and use the ratio of the sums as our training_strategy_probabilities.\n\nYou probably want to do a different transformation of training_strategy_probabilities to get your final strategy, but for now we just return the same thing.\n\nSome easy things to change are: - get_sampling_policy() to return \"sample\" at some infosets and \"expand_all\" at others. - determine_infoset() to coalesce different observable states into the same infoset. - get_training_strategy_probabilities() to have different behavior for default and updated strategy probs.",
    "crumbs": [
      "Signup: AI Poker Camp Summer 2024",
      "Configuration",
      "Building and Running Solvers"
    ]
  },
  {
    "objectID": "aipcs24/solvers.html#making-changes",
    "href": "aipcs24/solvers.html#making-changes",
    "title": "Implementing a Basic Solver",
    "section": "Making changes",
    "text": "Making changes\nIn general, you can change the behavior of the solver significantly by editing the functions defined in solver.py:\n\nhandle_new_iteration() - called with the iteration number when a new iteration is about to begin.\nhandle_iteration_over() - called with the iteration number when an iteration is over.\nget_root() - called witht the iteration number to get a RoundState object to begin the new iteration at. (Can be used to define how states should be explored.)\ndetermine_infoset() - called to get the canonical name of the infoset for a given visible game state. (Can be used to coalesce infosets.)\nsample_actions() - relevant if action types have variable parameters (like bet sizes), called to go from a list of legal action types to a set of action-instances to investigate.\nget_sampling_policy() - currently supports \"sample\" and \"expand_all\", called with the iteration number to determine whether to use a random sample to approximate this state‚Äôs EV, or to call all possible actions and compute a weighted sum. (Can vary policy by iteration by using the iteration number.)\nhandle_new_samples() - called with the sampling policy, and the resulting samples, from visiting a state node. (Can be used to update probabilities based on EVs.)\nget_training_strategy_probabilities() - used by both sampling policies, called to get the current strategy‚Äôs probabilities of taking each action at a given infoset.\nget_final_strategy_probabilities() - relevant if you want to use a different transformation of the data to get final probabilities than the intermediate probabilities used in training steps.",
    "crumbs": [
      "Signup: AI Poker Camp Summer 2024",
      "Configuration",
      "Building and Running Solvers"
    ]
  },
  {
    "objectID": "aipcs24/solvers.html#more",
    "href": "aipcs24/solvers.html#more",
    "title": "Implementing a Basic Solver",
    "section": "More",
    "text": "More\nIf you have positive or negative feedback about the solver, feel free to share it in the #aipcs24-technical-feedback channel of the discord.",
    "crumbs": [
      "Signup: AI Poker Camp Summer 2024",
      "Configuration",
      "Building and Running Solvers"
    ]
  },
  {
    "objectID": "aipcs24/2leduc_reading.html",
    "href": "aipcs24/2leduc_reading.html",
    "title": "#2: Leduc Poker | Class Materials",
    "section": "",
    "text": "Typically solving a 2-player poker game is defined as finding a Nash equilibrium strategy for both players. This is straightforward to define and has been the target of much poker research, but means finding a fixed strategy that doesn‚Äôt adapt to opponents and might not be the most profitable strategy in a field of a variety of opponents.\nSmall poker games can be solved through linear programming given a matrix of strategies at each information set and a matrix of payoffs (see here for more details).\nAs we prepare to solve larger games, we start to look at iterative algorithms.\nThe core feature of the iterative algorithms is self-play by traversing the game tree over all infosets and tracking the strategies and regrets at each.\n\n\nRegret is a measure of how much each strategy at an infoset is preferred and is used as a way to update strategies.\nFor a given P1 strategy and P2 strategy, a player has regret when they take an action at an infoset that was not the highest-EV action at that infoset.\n\n\n\n\n\n\nRegret Exercise\n\n\n\n\nWhat is the regret for each action?\n\n\n\nAction\nRegret\n\n\n\n\nA\n\n\n\nB\n\n\n\nC\n\n\n\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\n\nAction\nRegret\n\n\n\n\nA\n4\n\n\nB\n2\n\n\nC\n0\n\n\n\n\n\n\n\n\n\n\n\n\nExpected Value Exercise\n\n\n\n\nIf taking a uniform strategy at this node (i.e.¬†\\(\\frac{1}{3}\\) for each action), then what is the expected value of the node?\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\\(\\mathbb{E} = \\frac{1}{3}*1 + \\frac{1}{3}*3+\\frac{1}{3}*5 = 0.33+1+1.67 = 3\\)\n\n\n\n\n\n\n\n\n\nPoker Regret Exercise\n\n\n\n\nIn poker games, the regret for each action is defined as the value for that action minus the expected value of the node. Give the regret values for each action under this definition.\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\n\nAction\nValue\nPoker Regret\n\n\n\n\nA\n1\n-2\n\n\nB\n3\n0\n\n\nC\n5\n2\n\n\n\nAt a node in a poker game, the player prefers actions with higher regrets by this definition.\n\n\n\n\n\n\nThe most popular method for iteratively solving poker games is the Counterfactual Regret Minimization (CFR) algorithm.\nThe handout solver does not exactly use CFR. You can make updates to the solver however you would like, including modifying it to become CFR.\nWhat is a counterfactual?\nActual event: I didn‚Äôt bring an umbrella, and I got wet in the rain\nCounterfactual event: If I had brought an umbrella, I wouldn‚Äôt have gotten wet\n\n\n\ngeneral algorithm\n\nstore: strategy, regret\nregret matching\naverage strategy at end\nCFR+, Linear CFR\nsampling (external, sampling, chance, etc. )"
  },
  {
    "objectID": "aipcs24/2leduc_reading.html#solving-poker-games",
    "href": "aipcs24/2leduc_reading.html#solving-poker-games",
    "title": "#2: Leduc Poker | Class Materials",
    "section": "",
    "text": "Typically solving a 2-player poker game is defined as finding a Nash equilibrium strategy for both players. This is straightforward to define and has been the target of much poker research, but means finding a fixed strategy that doesn‚Äôt adapt to opponents and might not be the most profitable strategy in a field of a variety of opponents.\nSmall poker games can be solved through linear programming given a matrix of strategies at each information set and a matrix of payoffs (see here for more details).\nAs we prepare to solve larger games, we start to look at iterative algorithms.\nThe core feature of the iterative algorithms is self-play by traversing the game tree over all infosets and tracking the strategies and regrets at each.\n\n\nRegret is a measure of how much each strategy at an infoset is preferred and is used as a way to update strategies.\nFor a given P1 strategy and P2 strategy, a player has regret when they take an action at an infoset that was not the highest-EV action at that infoset.\n\n\n\n\n\n\nRegret Exercise\n\n\n\n\nWhat is the regret for each action?\n\n\n\nAction\nRegret\n\n\n\n\nA\n\n\n\nB\n\n\n\nC\n\n\n\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\n\nAction\nRegret\n\n\n\n\nA\n4\n\n\nB\n2\n\n\nC\n0\n\n\n\n\n\n\n\n\n\n\n\n\nExpected Value Exercise\n\n\n\n\nIf taking a uniform strategy at this node (i.e.¬†\\(\\frac{1}{3}\\) for each action), then what is the expected value of the node?\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\\(\\mathbb{E} = \\frac{1}{3}*1 + \\frac{1}{3}*3+\\frac{1}{3}*5 = 0.33+1+1.67 = 3\\)\n\n\n\n\n\n\n\n\n\nPoker Regret Exercise\n\n\n\n\nIn poker games, the regret for each action is defined as the value for that action minus the expected value of the node. Give the regret values for each action under this definition.\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\n\nAction\nValue\nPoker Regret\n\n\n\n\nA\n1\n-2\n\n\nB\n3\n0\n\n\nC\n5\n2\n\n\n\nAt a node in a poker game, the player prefers actions with higher regrets by this definition.\n\n\n\n\n\n\nThe most popular method for iteratively solving poker games is the Counterfactual Regret Minimization (CFR) algorithm.\nThe handout solver does not exactly use CFR. You can make updates to the solver however you would like, including modifying it to become CFR.\nWhat is a counterfactual?\nActual event: I didn‚Äôt bring an umbrella, and I got wet in the rain\nCounterfactual event: If I had brought an umbrella, I wouldn‚Äôt have gotten wet\n\n\n\ngeneral algorithm\n\nstore: strategy, regret\nregret matching\naverage strategy at end\nCFR+, Linear CFR\nsampling (external, sampling, chance, etc. )"
  },
  {
    "objectID": "aipcs24/3rps_reading.html",
    "href": "aipcs24/3rps_reading.html",
    "title": "#3 Rock Paper Scissors: Reading 1",
    "section": "",
    "text": "Rock defeats scissors, scissors defeats paper, and paper defeats rock. You get 1 point for winning, -1 point for losing, and 0 points for ties.\nThe goal of this challenge is to focus on tracking opponent distributions and how to respond to them.",
    "crumbs": [
      "Signup: AI Poker Camp Summer 2024",
      "Challenges",
      "#3: Rock Paper Scissors",
      "Class Materials"
    ]
  },
  {
    "objectID": "aipcs24/3rps_reading.html#counterfactual-regret-minimization",
    "href": "aipcs24/3rps_reading.html#counterfactual-regret-minimization",
    "title": "#3 Rock Paper Scissors: Reading 1",
    "section": "Counterfactual Regret Minimization",
    "text": "Counterfactual Regret Minimization\nAs we think about solving larger games, we start to look at iterative algorithms.\nThe most popular method for iteratively solving poker games is the Counterfactual Regret Minimization (CFR) algorithm. CFR is an iterative algorithm developed in 2007 at the University of Alberta that converges to Nash equilibrium in two player zero-sum games.\n(Note: The handout solver does not exactly use CFR. You can make updates to the solver however you would like, including modifying it to become CFR.)\nWhat is a counterfactual? Here‚Äôs an example:\nActual event: I didn‚Äôt bring an umbrella, and I got wet in the rain\nCounterfactual event: If I had brought an umbrella, I wouldn‚Äôt have gotten wet\n\nRegret and Strategies\nA strategy at an infoset is a probability distribution over each possible action.\nRegret is a measure of how much each strategy at an infoset is preferred and is used as a way to update strategies.\nFor a given P1 strategy and P2 strategy, a player has regret when they take an action at an infoset that was not the highest-EV action at that infoset.\n\n\n\n\n\n\nRegret Exercise\n\n\n\n\nWhat is the regret for each action?\n\n\n\nAction\nRegret\n\n\n\n\nA\n\n\n\nB\n\n\n\nC\n\n\n\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\n\nAction\nRegret\n\n\n\n\nA\n4\n\n\nB\n2\n\n\nC\n0\n\n\n\n\n\n\n\n\n\n\n\n\nExpected Value Exercise\n\n\n\n\nIf taking a uniform strategy at this node (i.e.¬†\\(\\frac{1}{3}\\) for each action), then what is the expected value of the node?\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\\(\\mathbb{E} = \\frac{1}{3}*1 + \\frac{1}{3}*3+\\frac{1}{3}*5 = 0.33+1+1.67 = 3\\)\n\n\n\n\n\n\n\n\n\nPoker Regret Exercise\n\n\n\n\nIn poker games, the regret for each action is defined as the value for that action minus the expected value of the node. Give the regret values for each action under this definition.\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\n\nAction\nValue\nPoker Regret\n\n\n\n\nA\n1\n-2\n\n\nB\n3\n0\n\n\nC\n5\n2\n\n\n\nAt a node in a poker game, the player prefers actions with higher regrets by this definition.\n\n\n\nEach infoset maintains a strategy and regret tabular counter for each action. These accumulate the sum of all strategies and the sum of all regrets.\nIn a game like Rock Paper Scissors, there is effectively only one infoset, so only one table for strategy over each action (Rock, Paper, Scissors) and one table for regret over each action (Rock, Paper, Scissors).\nRegrets are linked to strategies through a policy called regret matching.\n\n\nRegret Matching\n\n\n\n\n\n\nRPS Regret Details\n\n\n\n\n\nIn general, we define regret as:\n\\(\\text{Regret} = u(\\text{Alternative Strategy}) ‚àí u(\\text{Current Strategy})\\)\nWe prefer alternative actions with high regret and wish to minimize our overall regret.\nWe play Rock and opponent plays Paper \\(\\implies \\text{u(rock,paper)} = -1\\)\n\\(\\text{Regret(scissors)} = \\text{u(scissors,paper)} - \\text{u(rock,paper)} = 1-(-1) = 2\\)\n\\(\\text{Regret(paper)} = \\text{u(paper,paper)} - \\text{u(rock,paper)} = 0-(-1) = 1\\)\n\\(\\text{Regret(rock)} = \\text{u(rock,paper)} - \\text{u(rock,paper)} = -1-(-1) = 0\\)\nWe play Scissors and opponent plays Paper \\(\\implies \\text{u(scissors,paper)} = 1\\)\n\\(\\text{Regret(scissors)} = \\text{u(scissors,paper)} - \\text{u(scissors,paper)} = 1-1 = 0\\)\n\\(\\text{Regret(paper)} = \\text{u(paper,paper)} - \\text{u(scissors,paper)} = 0-1 = -1\\)\n\\(\\text{Regret(rock)} = \\text{u(rock,paper)} - \\text{u(scissors,paper)} = -1-1 = -2\\)\nWe play Paper and opponent plays Paper \\(\\implies \\text{u(paper,paper)} = 0\\)\n\\(\\text{Regret(scissors)} = \\text{u(scissors,paper)} - \\text{u(paper,paper)} = 1-0 = 1\\)\n\\(\\text{Regret(paper)} = \\text{u(paper,paper)} - \\text{u(paper,paper)} = 0-0 = 0\\)$\n\\(\\text{Regret(rock)} = \\text{u(rock,paper)} - \\text{u(paper,paper)} = -1-0 = -1\\)\nTo generalize:\n\nThe action played always gets a regret of 0 since the ‚Äúalternative‚Äù is really just that same action\nWhen we play a tying action, the alternative losing action gets a regret of -1 and the alternative winning action gets a regret of +1\nWhen we play a winning action, the alternative tying action gets a regret of -1 and the alternative losing action gets a regret of -2\nWhen we play a losing action, the alternative winning action gets a regret of +2 and the alternative tying action gets a regret of +1\n\nAfter each play, we accumulate regrets for each of the 3 actions.\n\n\n\nWe decide our strategy probability distribution using regret matching, which means playing a strategy that normalizes over the positive accumulated regrets, i.e.¬†playing in proportion to the positive regrets.\nExample from Marc Lanctot‚Äôs CFR Tutorial:\n\nGame 1: Choose Rock and opponent chooses Paper\n\nLose 1\nRock: Regret 0\nPaper: Regret 1\nScissors: Regret 2\n\nNext Action: Proportional \\[\n\\begin{pmatrix}\n\\text{Rock} & 0/3 = 0 \\\\\n\\text{Paper} & 1/3 = 0.333 \\\\\n\\text{Scissors} & 2/3 = 0.667\n\\end{pmatrix}\n\\]\nGame 2: Choose Scissors (With probability \\(2/3\\)) and opponent chooses Rock\n\nLose 1\nRock: Regret 1\nPaper: Regret 2\nScissors: Regret 0\n\nCumulative regrets:\n\nRock: 1\nPaper: 3\nScissors: 2\n\nNext Action: Proportional \\[\n\\begin{pmatrix}\n\\text{Rock} & 1/6 = 0167 \\\\\n\\text{Paper} & 3/6 = 0.500 \\\\\n\\text{Scissors} & 2/6 = 0.333\n\\end{pmatrix}\n\\]\n\nRegret matching definitions:\n\n\\(a\\) is actions\n\\(\\sigma\\) is strategy\n\\(t\\) is time\n\\(i\\) is player\n\\(R\\) is cumulative regret\n\n\\[\n\\sigma_i^t(a) = \\begin{cases}\n\\frac{\\max(R_i^t(a), 0)}{\\sum_{a' \\in A} \\max(R_i^t(a'), 0)} & \\text{if } \\sum_{a' \\in A} \\max(R_i^t(a'), 0) &gt; 0 \\\\\n\\frac{1}{|A|} & \\text{otherwise}\n\\end{cases}\n\\]\nThis is showing that we take the cumulative regret for an action divided by the cumulative regrets for all actions (normalizing) and then play that strategy for this action on the next iteration.\nIf all cumulative regrets are \\(\\leq 0\\) then we use the uniform distribution.\nIf cumulative regrets are positive, but are are \\(&lt;0\\) for a specific action, then we use \\(0\\) for that action.\nIn code:\n    def get_strategy(self):\n  #First find the normalizing sum\n        normalizing_sum = 0\n        for a in range(NUM_ACTIONS):\n            if self.regret_sum[a] &gt; 0:\n                self.strategy[a] = self.regret_sum[a]\n            else:\n                self.strategy[a] = 0\n            normalizing_sum += self.strategy[a]\n\n    #Then normalize each action\n        for a in range(NUM_ACTIONS):\n            if normalizing_sum &gt; 0:\n                self.strategy[a] /= normalizing_sum\n            else:\n                self.strategy[a] = 1.0/NUM_ACTIONS\n            self.strategy_sum[a] += self.strategy[a]\n\n        return self.strategy\nAfter using regret matching and after many iterations, we can minimize expected regret by using the average strategy at the end, which is the strategy that converges to equilibrium.\nIf two players were training against each other using regret matching, they would converge to the Nash Equilibrium of \\(1/3\\) for each action using the average strategy in Rock Paper Scissors.\n\n\nRPS Regret Matching Experiment\nHere we show that regret matching converges only using the average strategy over 10,000 iterations:\n\nThe bottom shows both players converging to \\(1/3\\), while the top shows Player 1‚Äôs volatile current strategies that are cycling around.\nSuppose that your opponent Player 2 is playing 40% Rock, 30% Paper, and 30% Scissors. Here is a regret matching 10,000 game experiment. It shows that it takes around 1,600 games before Player 1 plays only Paper (this will vary).\n\nWe see that if there is a fixed player, regret matching converges to the best strategy.\nBut what if your opponent is not using a fixed strategy? We‚Äôll talk about that soon.\n\n\nIterating through the Tree\nThe core feature of the iterative algorithms is self-play by traversing the game tree over all infosets and tracking the strategies and regrets at each.\nFrom above, we know how to find the strategy and regret in the simple Rock Paper Scissors environment.\nIn poker:\n\nStrategies are determined the same as above, through regret matching from the previous regret values at the specific information set for each action\nCFR definitions:\n\n\\(a\\) is actions\n\\(I\\) is infoset\n\\(\\sigma\\) is strategy\n\\(t\\) is time\n\\(i\\) is player\n\\(R\\) is cumulative regret\n\\(z\\) is a terminal node\n\\(u\\) is utility (payoffs)\n\\(p\\) is the current player who plays at this node\n\\(-p\\) is the the opponent player and chance\n\\(v\\) is counterfactual value\n\nCounterfactual values are effectively the value of an information set. They are weighted by the probability of opponent and chance playing to this node (in other words, the probability of playing to this node if this player tried to do so).\n\nCounterfactual value: \\(v^\\sigma (I) = \\sum_{z\\in Z_I} \\pi^{\\sigma}_{-p}(z[I])\\pi^{\\sigma}(z[I] \\rightarrow z)u_p(z)\\)\n\\(\\sum_{z\\in Z_I}\\) is summing over all terminal histories reachable from this node\n\\(\\pi^{\\sigma}_{-p}(z[I])\\) is the probability of opponents and chance reaching this node\n\\(\\pi^{\\sigma}(z[I] \\rightarrow z)\\) is the probability of playing from this node to terminal history \\(z\\), i.e.¬†the weight component of the expected value\n\\(u_p(z)\\) is the utility at terminal history \\(z\\), i.e.¬†the value component of the expected value\n\nInstantaneous regrets are based on action values compared to infoset EV. Each action EV then adds to its regret counter:\n\n\\(r^t(I,a) = v^{\\sigma^t}(I,a) - v^{\\sigma^t}(I)\\)\n\nCumulative (counterfactual) regrets are the sum of the individual regrets:\n\n\\(R^T(I,a) = \\sum_{t=1}^T r^t(I,a)\\)\n\n\n\n\nAlternatives/Updates to Original Algorithm\n\nCFR+ variation such that regrets can‚Äôt be \\(\\leq 0\\)\nLinear CFR such that regrets are weighted by their recency\nSampling\n\nExternal: Sample chance and opponent nodes\nChance: Sample chance only\nOutcome: Sample outcomes",
    "crumbs": [
      "Signup: AI Poker Camp Summer 2024",
      "Challenges",
      "#3: Rock Paper Scissors",
      "Class Materials"
    ]
  },
  {
    "objectID": "aipcs24/3rps_reading.html#data-talk-paper-scissors",
    "href": "aipcs24/3rps_reading.html#data-talk-paper-scissors",
    "title": "#3 Rock Paper Scissors: Reading 1",
    "section": "Data: Talk Paper Scissors",
    "text": "Data: Talk Paper Scissors\neieio games made a Rock Paper Scissors over voice game in which players call a phone number and get matched up with another player for a 3 game RPS match.\nThey published their 40,000 round data on X:\n Overall: R 37.2%, P 35.4%, S 27.4%\n Round 1: R 39.7%, P 37.6%, S 22.7%\n Round 2: R 34.0%, 33.4%, 32.6%\n Round 3: R 37.2%, 34.7%, 28.1%\n\n\n\n\n\n\nExpected Value Against TPS Player\n\n\n\nWhat is the best strategy per round against the average TPS player? What is your expected value per round and overall?\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThe best strategy is to always play Paper.\n\\(\\mathbb{E}(\\text{Round 1}) = 0.397*1 + 0.376*0 + 0.227*-1 = 0.17\\)\n\\(\\mathbb{E}(\\text{Round 2}) = 0.34*1 + 0.334*0 + 0.326*-1 = 0.014\\)\n\\(\\mathbb{E}(\\text{Round 3}) = 0.372*1 + 0.347*0 + 0.281*-1 = 0.091\\)\n\\(\\mathbb{E}(\\text{Round 4}) = 0.17 + 0.014 + 0.091 = 0.275\\)",
    "crumbs": [
      "Signup: AI Poker Camp Summer 2024",
      "Challenges",
      "#3: Rock Paper Scissors",
      "Class Materials"
    ]
  },
  {
    "objectID": "aipcs24/3rps_reading.html#more-exercises",
    "href": "aipcs24/3rps_reading.html#more-exercises",
    "title": "#3 Rock Paper Scissors: Reading 1",
    "section": "More Exercises",
    "text": "More Exercises\n\n\n\n\n\n\nMaximize Against non-Nash Fixed Opponent\n\n\n\nHow would you maximize in RPS knowing the opponent plays a fixed non-Nash strategy that you don‚Äôt know?\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nOne option is to play the equilibrium strategy until you get a significant sample on your opponent and then to exploit their strategy going forward.\n\n\n\n\n\n\n\n\n\nStrategy Against No-Rock Opponent\n\n\n\nWhat is the optimal play if your opponent can‚Äôt play Rock?\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\n\nPlayer 1/2\nPaper\nScissors\n\n\n\n\nRock\n(-1, 1)\n(1, -1)\n\n\nPaper\n(0, 0)\n(-1, 1)\n\n\nScissors\n(1, -1)\n(0, 0)\n\n\n\nWe can see that Player 1 playing Paper is dominated by Scissors, so Player 1 should never play Paper.\n\n\n\nPlayer 1/2\nPaper\nScissors\n\n\n\n\nRock\n(-1, 1)\n(1, -1)\n\n\nScissors\n(1, -1)\n(0, 0)\n\n\n\nIn the reduced game, we see that if Player 2 plays Paper with probability \\(p\\) and Scissors with probability \\(s\\), then:\n\\(\\mathbb{E}(\\text{P1 R}) = -1*p + 1*s = -p + s\\) \\(\\mathbb{E}(\\text{P1 S}) = 1*p + 0*s = p\\)\nSetting these equal, \\(-p + s = p \\Rightarrow s = 2p\\).\nWe also know that \\(s + p = 1\\).\nTherefore \\(s = 1 - p\\) and \\(1 - p = 2p \\Rightarrow 1 = 3p \\Rightarrow p = 1/3\\).\nTherefore, \\(s = 1 - 1/3 = 2/3\\).\nFor Player 2, we have \\(s = 2/3\\) and \\(p = 1/3\\).\nFor Player 1, we can solve similarly:\n\\(\\mathbb{E}(\\text{P2 P}) = 1*r - 1*s = r - s\\) \\(\\mathbb{E}(\\text{P2 S}) = -1*r + 0*s = -r\\)\n\\(r - s = -r \\Rightarrow 2r = s\\)\nWe also know that \\(r + s = 1\\).\nTherefore \\(s = 1 - r\\) and \\(1 - r = 2r \\Rightarrow 1 = 3r \\Rightarrow r = 1/3\\).\nTherefore, \\(s = 1 - 1/3 = 2/3\\).\nFor Player 2, we have \\(s = 2/3\\) and \\(p = 1/3\\).\nInserting these probabilities, we have:\n\n\n\nPlayer 1/2\nPaper (1/3)\nScissors (2/3)\n\n\n\n\nRock (1/3)\n(-1, 1) (1/9)\n(1, -1) (2/9)\n\n\nScissors (2/3)\n(1, -1) (2/9)\n(0, 0) (4/9)\n\n\n\nTherefore Player 1 has payoffs of: \\(1/9 * -1 + 2/9 * 1 + 2/9 * 1 + 4/9 * 0 = 3/9 = 1/3\\). Therefore the player that can still play Rock has an advantage of \\(1/3\\) at equilibrium.\n\n\n\n\n\n\n\n\n\nMaximize Against Adapting Rock Opponent\n\n\n\nWhat if the opponent is adapting to you, but 10% of the time they are forced to play Rock?\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nSoon\n\n\n\n\n\n\n\n\n\nSkewed Rock Payoff\n\n\n\nWhat is the equilibrium strategy if the payoff for Rock over Scissors is 2 (others stay the same)?\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nSoon",
    "crumbs": [
      "Signup: AI Poker Camp Summer 2024",
      "Challenges",
      "#3: Rock Paper Scissors",
      "Class Materials"
    ]
  },
  {
    "objectID": "aipcs24/1kuhn_challenge_review.html",
    "href": "aipcs24/1kuhn_challenge_review.html",
    "title": "#1: Kuhn Poker | Challenge Review",
    "section": "",
    "text": "Explain differences here between the regret that wasn‚Äôt converging on the site and the one that is\nAverage strategy thing Regret min thing from cfr.pdf Failure mode spinning around the right solution, want to spin inwards/converge"
  },
  {
    "objectID": "aipcs24/1kuhn_challenge_review.html#review-of-challenge-1",
    "href": "aipcs24/1kuhn_challenge_review.html#review-of-challenge-1",
    "title": "#1: Kuhn Poker | Challenge Review",
    "section": "Review of Challenge 1",
    "text": "Review of Challenge 1\nGoal: Equilibrium agent, later opponent modeling\nWhat does best bot vs you look like? What is your exploitability? How do we evaluate agents?\n\nTournament Results\nYep, just a slight variation here where the dealer burns 26/52 cards and then you play against the house; make it something other than just off-the-shelf and make your MC solver have to do some real work.\n\n\nHow the Interactive Works\nOdds vs.¬†probs Regret Full CFR details next session Show equations and graphs of how things correct when a single probability is thrown off\n\n\nOptimal Strategies\nSurprise! There are multiple Nash equilibria in Kuhn!\nValue of the game\nPosition thing\nPrinciple of 3 general types of hands and how it applies to regular poker\n\n\nPure vs.¬†Mixed Strategies\nGradient Descent: Involves iteratively adjusting parameters to minimize a cost function. Each step moves the parameters in the direction of the negative gradient of the cost function, gradually converging to a local or global minimum. Aims to converge to the optimal parameters that minimize the cost function. With an appropriate learning rate and sufficient iterations, it can find the minimum. Utilizes feedback from the gradient of the cost function at each iteration to update the parameters. Focuses on a static objective function (cost function) and aims to find its minimum.\nRegret Minimization: In online learning and decision-making contexts, it involves iteratively updating strategies to minimize regret, which is the difference between the actual cumulative loss and the best possible cumulative loss in hindsight. Each step adjusts the strategy based on past performance to improve future decisions. Aims to minimize regret over time, which means the strategy becomes nearly as good as the best fixed strategy in hindsight. With enough iterations, the average regret per iteration tends to zero. Utilizes feedback from past performance (losses) to update the strategy, aiming to reduce future regret. Focuses on a dynamic objective (minimizing regret over time) in potentially changing environments, where the best action may vary over time.\nRegret Matching: Involves iteratively updating the probability distribution over actions based on past regrets. Actions with higher regrets (indicating they would have performed better in the past) are chosen more frequently in the future. Uses past regrets to update the probability distribution over actions. The probability of selecting each action increases proportionally to the regret of not having taken that action.\nPruning and compare to CFR"
  },
  {
    "objectID": "aipcs24/sessions.html",
    "href": "aipcs24/sessions.html",
    "title": "Sessions and Challenges",
    "section": "",
    "text": "This is the schedule for AI Poker Camp Summer 2024. We plan to spend Mondays reviewing previous challenges and thinking about new challenges/adjacent problems and Thursdays working on the challenges, which will be due every Sunday.\n\n\nThe calendar below includes all dates, locations, and times. You can add the whole thing to your own calendar with the button in the bottom right.",
    "crumbs": [
      "Signup: AI Poker Camp Summer 2024",
      "About the Course",
      "Sessions and Challenges"
    ]
  },
  {
    "objectID": "aipcs24/order.html",
    "href": "aipcs24/order.html",
    "title": "Order in the AIPC",
    "section": "",
    "text": "Intro/Kuhn Poker\nRock Paper Scissors/Kuhn Poker vs.¬†Fixed Opponent\nRock Paper Scissors/Kuhn Poker Equilibrium\nRock Paper Scissors Tournament (vs.¬†equilibrium/adaptive/fixed opponents)\n100-Card Kuhn Poker Tournament (vs.¬†equilibrium/adaptive/fixed opponents)\nLeduc Poker (or slightly more complicated)\nTexas Tac Toe\nAllin/Fold 10BB NLHE\nRock Poker Scissors\nRock Poker Scissors"
  },
  {
    "objectID": "aipcs24/order.html#ideas-for-order",
    "href": "aipcs24/order.html#ideas-for-order",
    "title": "Order in the AIPC",
    "section": "",
    "text": "Intro/Kuhn Poker\nRock Paper Scissors/Kuhn Poker vs.¬†Fixed Opponent\nRock Paper Scissors/Kuhn Poker Equilibrium\nRock Paper Scissors Tournament (vs.¬†equilibrium/adaptive/fixed opponents)\n100-Card Kuhn Poker Tournament (vs.¬†equilibrium/adaptive/fixed opponents)\nLeduc Poker (or slightly more complicated)\nTexas Tac Toe\nAllin/Fold 10BB NLHE\nRock Poker Scissors\nRock Poker Scissors"
  },
  {
    "objectID": "aipcs24/bots.html",
    "href": "aipcs24/bots.html",
    "title": "Building and Running Bots",
    "section": "",
    "text": "Our game engine is run in Python 3.\n\nCheck that you have at least Python 3.12 installed with:\n\npython3 --version\n\nThe game engine uses the eval7 package, which is a Python Texas Hold‚Äôem hand evaluation library:\n\npip3 install eval7",
    "crumbs": [
      "Signup: AI Poker Camp Summer 2024",
      "Configuration",
      "Building and Running Bots"
    ]
  },
  {
    "objectID": "aipcs24/bots.html#python-setup",
    "href": "aipcs24/bots.html#python-setup",
    "title": "Building and Running Bots",
    "section": "",
    "text": "Our game engine is run in Python 3.\n\nCheck that you have at least Python 3.12 installed with:\n\npython3 --version\n\nThe game engine uses the eval7 package, which is a Python Texas Hold‚Äôem hand evaluation library:\n\npip3 install eval7",
    "crumbs": [
      "Signup: AI Poker Camp Summer 2024",
      "Configuration",
      "Building and Running Bots"
    ]
  },
  {
    "objectID": "aipcs24/bots.html#poker-camp-game-engine",
    "href": "aipcs24/bots.html#poker-camp-game-engine",
    "title": "Building and Running Bots",
    "section": "Poker Camp Game Engine",
    "text": "Poker Camp Game Engine\n7/18 Note: The game engine is under development and you might see changes, especially over the first couple of weeks of the AIPCS24.\n\nEngine\nThe engine is in engine.py. You can use python3 engine.py to test two agents playing against each other.\nTo run a 100 hand match with two bots that are named p1 and p2 and run the logic from players/random/ folder and output results to the p1p2test folder, do this:\npython3 engine.py -p1 'p1' players/random/ -p2 'p2' players/random/ -o p1p2test -n 100\nThe generic usage is:\npython3 engine.py -p1 {p1_name} {p1_file_path} -p2 {p2_name} {p2_file_path} -o {output_dir} -n {n_hands}\"\nThe output files are:\n\nscores.p1.p2.txt contains the raw scores (i.e.¬†profits) of each player\nThe p1.p2 folder contains:\n\n\ngamelog.txt: A log of all hands played\nOther log files for each player\n\n\n\nConfig\nThe config.py file contains various parameters to control the game engine. You should not need to modify this in normal use.",
    "crumbs": [
      "Signup: AI Poker Camp Summer 2024",
      "Configuration",
      "Building and Running Bots"
    ]
  },
  {
    "objectID": "aipcs24/bots.html#build-a-bot",
    "href": "aipcs24/bots.html#build-a-bot",
    "title": "Building and Running Bots",
    "section": "Build a Bot",
    "text": "Build a Bot\nThe player.py file is where you write your poker bot.\nNote that for Kuhn Poker, the cards are assigned as follows:\n\n\n\nCard\nEngine\n\n\n\n\nQ\n0\n\n\nK\n1\n\n\nA\n2\n\n\n\nThere are three preconfigured bots that you can see to get a sense of how they work:\n\nrandom: Every action is random. In Kuhn this means 50% ‚Üë actions and 50% ‚Üì actions.\nlinear: For Kuhn, every Q action is ‚Üì, every K action is 50% ‚Üë and 50% ‚Üì, and every A action is 100% ‚Üë.\nfrom-weights: This is how the Kuhn Challenge works. Each infoset is assigned a specific weight and the bot always plays according to those strategy probabilities.\n\n\n\n\n\n\n\nOther files include that you should not need to modify\n\n\n\n\n\n\nactions.py: The actions a player can take\nbot.py: Defines the interface of the player.py functions\nrunner.py: The infrastructure for interacting with the engine\nstates.py: Encapsulates game and round state information for the player\n\n\n\n\n\nUsing player.py to Build a Bot\nplayer.py contains 3 functions:\n\nhandle_new_round(): Gets called when a new round (i.e.¬†hand) starts\nhandle_round_over(): Gets called when a new round (i.e.¬†hand) ends\nget_action(): The main function to implement, which is called any time the engine needs an action from your bot.\n\nYou should write these functions so that get_action() returns the actions that you want in the situations it faces.\n\nThe get_action() function\nThe arguments coming in to get_action() are:\n\ngame_state: the GameState object, which is the state of the entire match of hands. This was 100 in the above example. The game state gives:\nbankroll: Profits over the match\ngame_clock: Your time remaining to use during the match\nround_num: The round of betting, always 1 in Kuhn Poker\n\nHere‚Äôs an example GameState:\ngame state GameState(bankroll=0, game_clock=29.991, round_num=1)\n\nround_state: the RoundState object, which contains all information about the current hand.\n\nThis includes :\n\nturn: The number of actions that have taken place this game. (turn % 2 gives the player who will act next.)\nstreet: Current betting round (in Kuhn Poker, this will always be 0).\npips: How many chips each player has contributed to the pot on the current hand.\nstacks: How many chips each player has left (not contributed to the pot).\nhands: List of known hands to you, with None for unknown hands.\ndeck: This won‚Äôt be known to you, so it will probably always be None.\naction_history: History of actions, a list of UpAction() or DownAction(). (The type of this will definitely change as we work on it.)\nprevious_state: The previous state of the hand, as a RoundState.\n\nHere‚Äôs an example RoundState:\nRoundState(\n    turn=1,\n    street=0,\n    pips=[1, 1],\n    stacks=[1, 1],\n    hands=[None, 1],\n    deck=None,\n    action_history=[DownAction()],\n    previous_state=\n        RoundState(\n            turn=0,\n            street=0,\n            pips=[1, 1],\n            stacks=[1, 1],\n            hands=[None, 1],\n            deck=None,\n            action_history=[],\n            previous_state=None\n        )\n    )\n\nactive: your player‚Äôs index\n\nThe return is:\n\nYour action\n\n\n\n\nDebugging your broken bot\nBecause of the way engine.py captures the output of the bots it runs, you probably don‚Äôt get the printed output of your broken bot failing. If you comment out the line of stdout=subprocess.PIPE, stderr=subprocess.STDOUT, in this function call, you can (probably?) disable this behavior:\nproc = subprocess.Popen(\n    self.commands['run'] + [str(port)],\n    stdout=subprocess.PIPE, stderr=subprocess.STDOUT,\n    cwd=self.path,\n)",
    "crumbs": [
      "Signup: AI Poker Camp Summer 2024",
      "Configuration",
      "Building and Running Bots"
    ]
  },
  {
    "objectID": "aipcs24/bots.html#sample-bots-for-challenge-1-kuhn-poker",
    "href": "aipcs24/bots.html#sample-bots-for-challenge-1-kuhn-poker",
    "title": "Building and Running Bots",
    "section": "Sample Bots (for Challenge 1 / Kuhn Poker)",
    "text": "Sample Bots (for Challenge 1 / Kuhn Poker)\n\nRandom\nThe most simple random agent doesn‚Äôt care about the GameState or RoundState and implements the simple action:\nreturn random.choice([UpAction(), DownAction()])\n\n\nLinear Agent\nThe linear agent also doesn‚Äôt use GameState or RoundState, but does change its actions depending on its own hand. It uses this code to match my_hand to the appropriate linear case.\nmatch my_hand:\n    case 0:\n        return DownAction()\n    case 1:\n        return random.choice([UpAction(), DownAction()])\n    case 2:\n        return UpAction()\n\n\nWeights (Probabilities) Agent\nThe from-weights agent does need to use the round_state to first see whose turn it is to act (round_state.turn) and then to match the hand to the appropriate infoset using match my_hand. From there, the strategy can be defined according to the strategy probabilities (weights) for that infoset.\nmatch round_state.turn:\n    case 0:\n        match my_hand:\n            case 0: # Q_\n                up_prob = self.strategy[\"Q_\"]\n            case 1: # K_\n                up_prob = self.strategy[\"K_\"]\n            case 2: # A_\n                up_prob = self.strategy[\"A_\"]\n    case 1:\n        match my_hand, round_state.action_history[0]:\n            case 0, DownAction(): #_Q‚Üì\n                up_prob = self.strategy[\"_QD\"]\n...\n\n\n‚Ä¶and beyond?\nThe from-weights agent gives you the tools to implement any fixed strategy that you want. If you want to do better than Nash, though, you‚Äôll have to do something that remembers what your opponent has played in previous rounds, and use it to do something differently in the future‚Ä¶\n\n\n\n\n\n\nBeta Note\n\n\n\nWe expect to add a handle_observed_action() function in the bot/runner framework, to make certain kinds of tracking easier. For now, you can do this by adding logging logic to the get_action() and/or handle_round_over().",
    "crumbs": [
      "Signup: AI Poker Camp Summer 2024",
      "Configuration",
      "Building and Running Bots"
    ]
  },
  {
    "objectID": "aipcs24/index.html",
    "href": "aipcs24/index.html",
    "title": "AI Poker Camp Summer 2024",
    "section": "",
    "text": "Welcome to AI Poker Camp Summer 2024 Beta in San Francisco!\n\nThanks for signing up. This is the first time we‚Äôre running AI Poker Camp and it‚Äôs very much in beta. The idea to run this course has existed for a few years, but this curriculum is very new and came about from a discussion at Manifest.\nWe‚Äôre glad to have you along for the ride and welcome any feedback about all aspects of the course including things like whether the pace is too fast or too slow, too easy or too hard, etc.\nWe plan to spend Mondays reviewing previous challenges and thinking about new challenges/adjacent problems and Thursdays working on the challenges, which will be due every Sunday.\nOur goal is to develop your intuitions around decision making and problem solving by engaging with challenges in fun and non-idealized scenarios."
  },
  {
    "objectID": "aipcs24/1kuhn_extrareadings.html",
    "href": "aipcs24/1kuhn_extrareadings.html",
    "title": "#1: Kuhn Poker | Extra Readings",
    "section": "",
    "text": "Exercise\n\n\n\nFill in the table below with games you know about. Thanks to Eliezer for getting us started.\n\n\n\n\n\n\n\n\n\n\nGame/Opponent\nFixed/Probabilistic Opponent\nAdversarial Opponent\n\n\n\n\nImperfect Info, No Player Agency/Decisions\n\n\n\n\nPerfect Info, Player Actions Always Perfect Info\n\n\n\n\nImperfect Info, Imperfect from Randomness\n\n\n\n\nImperfect Info, Imperfect from Randomness AND Game States\n\n\n\n\n\n\n\n\n\n\n\nPre-Filled Table\n\n\n\n\n\n\n\n\n\n\n\n\n\nGame/Opponent\nFixed/Probabilistic Opponent\nAdversarial Opponent\n\n\n\n\nImperfect Info, No Player Agency/Decisions\nCandy Land, War, Dreidel, Bingo, Chutes and Ladders, Slot machine\n\n\n\nPerfect Info, Player Actions Always Perfect Info\nPuzzles\nTictactoe, Checkers, Chess, Arimaa, Go\n\n\nImperfect Info, Imperfect from Randomness\nBlackjack\nBackgammon\n\n\nImperfect Info, Imperfect from Randomness AND Game States\nPartial Info Multi-armed Bandit\nPoker, Rock Paper Scissors, Liar‚Äôs Dice, Figgie"
  },
  {
    "objectID": "aipcs24/1kuhn_extrareadings.html#types-of-games",
    "href": "aipcs24/1kuhn_extrareadings.html#types-of-games",
    "title": "#1: Kuhn Poker | Extra Readings",
    "section": "",
    "text": "Exercise\n\n\n\nFill in the table below with games you know about. Thanks to Eliezer for getting us started.\n\n\n\n\n\n\n\n\n\n\nGame/Opponent\nFixed/Probabilistic Opponent\nAdversarial Opponent\n\n\n\n\nImperfect Info, No Player Agency/Decisions\n\n\n\n\nPerfect Info, Player Actions Always Perfect Info\n\n\n\n\nImperfect Info, Imperfect from Randomness\n\n\n\n\nImperfect Info, Imperfect from Randomness AND Game States\n\n\n\n\n\n\n\n\n\n\n\nPre-Filled Table\n\n\n\n\n\n\n\n\n\n\n\n\n\nGame/Opponent\nFixed/Probabilistic Opponent\nAdversarial Opponent\n\n\n\n\nImperfect Info, No Player Agency/Decisions\nCandy Land, War, Dreidel, Bingo, Chutes and Ladders, Slot machine\n\n\n\nPerfect Info, Player Actions Always Perfect Info\nPuzzles\nTictactoe, Checkers, Chess, Arimaa, Go\n\n\nImperfect Info, Imperfect from Randomness\nBlackjack\nBackgammon\n\n\nImperfect Info, Imperfect from Randomness AND Game States\nPartial Info Multi-armed Bandit\nPoker, Rock Paper Scissors, Liar‚Äôs Dice, Figgie"
  },
  {
    "objectID": "aipcs24/1kuhn_extrareadings.html#concept-nash-equilibrium",
    "href": "aipcs24/1kuhn_extrareadings.html#concept-nash-equilibrium",
    "title": "#1: Kuhn Poker | Extra Readings",
    "section": "Concept: Nash Equilibrium",
    "text": "Concept: Nash Equilibrium\nA Nash equililibrium is a set of strategies for both players such that neither player ever plays an action with regret. Under Nash equilibrium, no player can gain by unilaterally deviating from their strategy. The other paradigm for game strategies is opponent exploitation, which we will address in future sections.\nRecall that regret only makes sense in the context of a particular strategy and assumed opponent‚Äôs strategy. When you submit a strategy to Challenge 1, you submit a strategy for being P1, and a strategy for being P2, but you won‚Äôt ever play against yourself ‚Äì so why is it helpful to find a pair that plays against itself without regret?"
  },
  {
    "objectID": "aipcs24/1kuhn_extrareadings.html#indifference-and-penalty-kicks",
    "href": "aipcs24/1kuhn_extrareadings.html#indifference-and-penalty-kicks",
    "title": "#1: Kuhn Poker | Extra Readings",
    "section": "Indifference and Penalty Kicks",
    "text": "Indifference and Penalty Kicks\nConsider the Soccer Penalty Kick game where a Kicker is trying to score a goal and the Goalie is trying to block it.\n\n\n\nKicker/Goalie\nLean Left\nLean Right\n\n\n\n\nKick Left\n0, 0\n+2, -2\n\n\nKick Right\n+1, -1\n0, 0\n\n\n\nThe game setup is zero-sum. If Kicker and Goalie both go in one direction, then it‚Äôs assumed that the goal will miss and both get \\(0\\) payoffs. If the Kicker plays Kick Right when the Goalie plays Lean Left, then the Kicker is favored and gets a payoff of \\(+1\\). If the Kicker plays Kick Left when the Goalie plays Lean Right, then the kicker is even more favored, because it‚Äôs easier to kick left than right, and gets \\(+2\\).\n\n\n\n\n\n\nNash Equilibrium Exercise\n\n\n\nWhich of these, if any, is a Nash equilibrium? You can check by seeing if either player would benefit by changing their action.\n\n\n\nKicker\nGoalie\nEquilibrium or Change?\n\n\n\n\nLeft\nLeft\n\n\n\nLeft\nRight\n\n\n\nRight\nLeft\n\n\n\nRight\nRight\n\n\n\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThere are no pure Nash equilibrium solutions because when the actions match, the Kicker will always want to change, and when they don‚Äôt match, the Goalie will always want to change.\n\n\n\nKicker\nGoalie\nEquilibrium or Change?\n\n\n\n\nLeft\nLeft\nKicker changes to right\n\n\nLeft\nRight\nGoalie changes to left\n\n\nRight\nLeft\nGoalie changes to right\n\n\nRight\nRight\nKicker changes to left\n\n\n\n\n\n\n\n\n\n\n\n\nExpected Value Exercise\n\n\n\nAssume that they both play Left 50% and Right 50% ‚Äì what is the expected value of the game?\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\n\nKicker/Goalie\nLean Left (0.5)\nLean Right (0.5)\n\n\n\n\nKick Left (0.5)\n0, 0\n+2, -2\n\n\nKick Right (0.5)\n+1, -1\n0, 0\n\n\n\nWe apply these probabilities to each of the 4 outcomes:\n\n\n\nKicker/Goalie\nLean Left (0.5)\nLean Right (0.5)\n\n\n\n\nKick Left (0.5)\n0, 0 (0.25)\n+2, -2 (0.25)\n\n\nKick Right (0.5)\n+1, -1 (0.25)\n0, 0 (0.25)\n\n\n\nNow for the Kicker, we have \\(\\mathbb{E} = 0.25*0 + 0.25*2 + 0.25*1 + 0.25*0 = 0.75\\).\nSince it‚Äôs zero-sum, we have \\(\\mathbb{E} = -0.75\\) for the Goalie.\nNote that, for example, the Kicker playing 50% Left and 50% Right could be interpreted as a single player having these probabilities or a field of players averaging to these probabilities. So out of 100 players, this could mean:\n\n100 players playing 50% Left and 50% Right\n50 players playing 100% Left and 50 players playing 100% Right\n50 players playing 75% Left/25% Right and 50 players playing 25% Left/75% right\n\n\n\n\nWhen the Goalie plays left with probability \\(p\\) and right with probability \\(1-p\\), we can find the expected value of the Kicker actions.\n\n\n\nKicker/Goalie\nLean Left (p)\nLean Right (1-p)\n\n\n\n\nKick Left\n0, 0\n+2, -2\n\n\nKick Right\n+1, -1\n0, 0\n\n\n\n\\(\\mathbb{E}(\\text{Kick Left}) = 0*p + 2*(1-p) = 2 - 2*p\\)\n\\(\\mathbb{E}(\\text{Kick Right}) = 1*p + 0*(1-p) = 1*p\\)\nThe Kicker is going to play the best response to the Goalie‚Äôs strategy. The Goalie wants to make the Kicker indifferent to Kick Left and Kick Right because if the Kicker was not going to be indifferent, then he would prefer one of the actions, meaning that action would be superior to the other. Therefore the Kicker will play a mixed strategy in response that will result in a Nash equilibrium where neither player benefits from unilaterally changing strategies. (Note that indifferent does not mean 50% each, but means the expected value is the same for each.)\n\nBy setting the values equal, we get \\(2 - 2*p = 1*p \\Rightarrow p = \\frac{2}{3}\\) as shown in the plot. This means that \\(1-p = 1 - \\frac{2}{3} = \\frac{1}{3}\\). Therefore the Goalie should play Lean Left \\(\\frac{2}{3}\\) and Lean Right \\(\\frac{1}{3}\\). The value for the Kicker is \\(\\frac{2}{3}\\), or \\((0.67)\\), for both actions, regardless of the Kicker‚Äôs mixing strategy.\nNote that the Kicker is worse off now (\\(0.67\\) now compared to \\(0.75\\)) than when both players played 50% each action. Why?\nIf the Kicker plays Left with probability \\(q\\) and Right with probability \\(1-q\\), then the Goalie‚Äôs values are:\n\\(\\mathbb{E}(\\text{Lean Left}) = 0*q - 1*(1-q) = -1 + q\\)\n\\(\\mathbb{E}(\\text{Lean Right}) = -2*q + 0 = -2*q\\)\nSetting equal,\n\\[\n\\begin{equation}\n\\begin{split}\n-1 + q &= -2*q \\\\\n-1 &= -3*q  \\\\\n\\frac{1}{3} &= q\n\\end{split}\n\\end{equation}\n\\]\nTherefore the Kicker should play Left \\(\\frac{1}{3}\\) and Right \\(\\frac{2}{3}\\), giving a value of \\(-\\frac{2}{3}\\) to the Goalie.\nWe can see this from the game table:\n\n\n\n\n\n\n\n\nKicker/Goalie\nLean Left (\\(\\frac{2}{3}\\))\nLean Right (\\(\\frac{1}{3}\\))\n\n\n\n\nKick Left (\\(\\frac{1}{3}\\))\n0, 0 (\\(\\frac{2}{9}\\))\n+2, -2 (\\(\\frac{1}{9}\\))\n\n\nKick Right (\\(\\frac{2}{3}\\))\n+1, -1 (\\(\\frac{4}{9}\\))\n0, 0 (\\(\\frac{2}{9}\\))\n\n\n\nTherefore the expected payoffs in this game are \\(\\frac{2}{9}*0 + \\frac{1}{9}*2 + \\frac{4}{9}*1 + \\frac{2}{9}*0 = \\frac{6}{9} = 0.67\\) for the Kicker and \\(-0.67\\) for the Goalie.\nIn an equilibrium, no player should be able to unilaterally improve by changing their strategy. What if the Kicker switches to always Kick Left?\n\n\n\n\n\n\n\n\nKicker/Goalie\nLean Left (\\(\\frac{2}{3}\\))\nLean Right (\\(\\frac{1}{3}\\))\n\n\n\n\nKick Left (\\(1\\))\n0, 0 (\\(\\frac{2}{3}\\))\n+2, -2 (\\(\\frac{1}{3}\\))\n\n\nKick Right (\\(0\\))\n+1, -1 (\\(0\\))\n0, 0 (\\(0\\))\n\n\n\nNow the Kicker‚Äôs payoff is still \\(\\frac{1}{3}*2 = 0.67\\).\nWhen a player makes their opponent indifferent, this means that any action the opponent takes (within the set of equilibrium actions) will result in the same payoff!\nSo if you know your opponent is playing the equilibrium strategy, then you can actually do whatever you want with no penalty with the mixing actions. Sort of.\nThe risk is that the opponent can now deviate from equilibrium and take advantage of your new strategy. For example, if the Goalie caught on and moved to always Lean Left, then expected value is reduced to \\(0\\) for both players.\nTo summarize, you can only be penalized for not playing the equilibrium mixing strategy if your opponent plays a non-equilibrium strategy that exploits your strategy.\n\n\n\n\n\n\nIndifference\n\n\n\nWhy do players make their opponent indifferent?"
  },
  {
    "objectID": "aipcs24/1kuhn_extrareadings.html#indifference-in-poker",
    "href": "aipcs24/1kuhn_extrareadings.html#indifference-in-poker",
    "title": "#1: Kuhn Poker | Extra Readings",
    "section": "Indifference in Poker",
    "text": "Indifference in Poker\nBack to poker. We can apply this indifference principle in computing equilibrium strategies in poker. When you make your opponent indifferent, then you don‚Äôt give them any best play.\nImportant note: If you play an equilibrium strategy, opponents will only get penalized for playing hands outside of the set of hands in the mixed strategy equilibrium (also known as the support, or the set of pure strategies that are played with non-zero probability under the mixed strategy). If opponents are not playing equilibrium, though, then they open themselves up to exploitation.\nLet‚Äôs look at one particular situation in Kuhn Poker and work it out by hand. Suppose that you are Player 2 with card Q after a Check from Player 1.\n\n\n\n\n\n\n\nExpected Value Exercise\n\n\n\nWhat indifference is Player 2 trying to induce? Compute it.\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nMaking P1 indifferent between calling and folding with a K\nWe can work out Player 2‚Äôs betting strategy by calculating the indifference. Let \\(b\\) be the probability that P2 bets with a Q after P1 checks.\n\\[\\begin{equation}\n\\begin{split}\n\\mathbb{E}(\\text{P1 Check K then Fold to Bet}) &= 0 \\\\\n\\\\\n\n\\mathbb{E}(\\text{P1 Check K then Call Bet}) &= -1*\\Pr(\\text{P2 has A and Bets}) + 3*\\Pr(\\text{P2 has Q and Bets}) \\\\\n  &= -1*\\frac{1}{2} + 3*\\frac{1}{2}*b \\\\\n  &= -0.5 + 1.5*b\n\\end{split}\n\\end{equation}\\]\nSetting these equal:\n\\(0 = -0.5 + 1.5*b\\)\n\\(b = \\frac{1}{3}\\)\nTherefore in equilibrium, P2 should bet \\(\\frac{1}{3}\\) with Q after P1 checks.\n\n\n\n\n\n\n\n\n\nEquilibrium Mixed Strategy Change Exercise\n\n\n\nIf P2 bet \\(\\frac{2}{3}\\) instead of \\(\\frac{1}{3}\\) with Q after P1 checks and P1 is playing an equilibrium strategy, how would P2‚Äôs expected value change?\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nIt wouldn‚Äôt! As long as P1 doesn‚Äôt modify their equilibrium strategy, then P2 can mix his strategy (at mixing infosets) however he wants and have the same EV.\n\n\n\n\n\n\n\n\n\nBluff:Value Ratio Exercise\n\n\n\nGiven that P2 has bet after P1 checks and is playing the equilibrium strategy, what is the probability that they are bluffing?\n(Note: Including cases where you have an A, so Q bets are bluffs and A bets are value bets.)\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nP2 has Q and A each \\(\\frac{1}{2}\\) of the time.\nP2 is betting Q \\(\\frac{1}{3}\\) of the time (bluffing).\nP2 is betting A always (value betting).\nTherefore for every 3 times you have Q you will bet once and for every 3 times you have A you will bet 3 times. Out of the 4 bets, 1 of them is a bluff.\n\\(\\Pr(\\text{P2 Bluff after P1 Check}) = \\frac{1}{4}\\)"
  },
  {
    "objectID": "aipcs24/1kuhn_extrareadings.html#extremely-optional-exercises",
    "href": "aipcs24/1kuhn_extrareadings.html#extremely-optional-exercises",
    "title": "#1: Kuhn Poker | Extra Readings",
    "section": "Extremely Optional Exercises",
    "text": "Extremely Optional Exercises\n\nDeeper into Kuhn Math\n\nSolve the above indifference exercise (Player 2 with card Q after Player 1 checks) if the bet size was 2 instead of 1. Can you come up with a general equation for how often to bluff given a pot size and bet size? What about how often to call?\nAnalytically solve for an additional Kuhn infoset\nSelect a set of infosets in Kuhn Poker and formula a system of equations for how they affect each other\nAt equilibrium in Kuhn Poker, Player 1 should bet A 3 times the amount that they bet Q. Against a Nash opponent, a mixing mistake will not result in any EV loss at those infosets. But are there any effects down the game tree?\nIf we as P1 Check with K, with a large sample size, we can estimate how often our opponent will bluff with a Q. Over a sample of 1000 hands that we Check with K, we expect that 500 times they would have Q and 500 times they would have A. We expect that they would always bet with A and bet some percentage \\(q\\) of their Q hands. Write an equation for how often we should call in terms of \\(q\\). In what other cases could we write equations for how to optimally play against our opponent?\n\n\n\nCoding a Kuhn Solver\n\nCode a Kuhn solver\nCode a Kuhn solver for the 100-card version"
  },
  {
    "objectID": "aipcs24/2othergames_reading.html",
    "href": "aipcs24/2othergames_reading.html",
    "title": "#2: Other Games | Reading",
    "section": "",
    "text": "This course is primarily about poker and poker-adjacent games, but let‚Äôs take a step back and look at different classes of games and where poker fits.\nThe goal of this reading is to introduce various concepts that will be useful when building agents for future challenges."
  },
  {
    "objectID": "aipcs24/2othergames_reading.html#types-of-games",
    "href": "aipcs24/2othergames_reading.html#types-of-games",
    "title": "#2: Other Games | Reading",
    "section": "",
    "text": "This course is primarily about poker and poker-adjacent games, but let‚Äôs take a step back and look at different classes of games and where poker fits.\nThe goal of this reading is to introduce various concepts that will be useful when building agents for future challenges."
  },
  {
    "objectID": "aipcs24/2othergames_reading.html#reinforcement-learning-rl",
    "href": "aipcs24/2othergames_reading.html#reinforcement-learning-rl",
    "title": "#2: Other Games | Reading",
    "section": "Reinforcement Learning (RL)",
    "text": "Reinforcement Learning (RL)\nMany ideas in this section on RL come from Sutton and Barto‚Äôs Reinforcement Learning book.\nThe main idea of reinforcement learning is that an agent learns by taking actions in its environment. The environment then gives feedback in the form of rewards and a new state.\n\n\n\nDiagram from Sutton Barton Reinforcement Learning\n\n\nWe have for each time \\(t\\): - State: \\(S_t\\) - Action: \\(A_t\\) - Reward: \\(R_t\\)\nThe goal of an agent is generally to maximize the expected value of the rewards received.\nReward values can be designed in a way to optimally train the agent. In chess for example, we could give a reward of \\(+1\\) for winning a game, \\(-1\\) for losing, and \\(0\\) for all other states including draws. Defining intermediate rewards for actions like taking an opponent piece risks the agent prioritizing that goal over winning the game.\nReward hacking is when agents find a way to obtain rewards in a way that isn‚Äôt aligned with the intended goal. (The agents will take the reward structure very literally and this is all they have to go on!)\nA Markov Decision Process (MDP) is a simplified model of the reinforcement learning problem in that the probability of future states and rewards depends only on the previous state and actions.\nThis can be written as:\n\\[\np(s', r \\mid s, a) = \\Pr\\{S_t=s', R_t=r \\mid S_{t-1}=s, A_{t-1}=a\\}\n\\]\nValue functions estimate the value of being in a certain state in terms of expected future rewards (which can be discounted).\n\nIf you‚Äôre about to flip a coin and want it to be heads (\\(+1\\)) and not tails (\\(-1\\)), then the value of that state is \\(0\\).\nIf you‚Äôre in a chess game where winning is \\(+1\\) and losing is \\(-1\\) and everything else is \\(0\\) and you‚Äôre in a state where it‚Äôs your turn and you can checkmate, then that state has a value of \\(+1\\).\n\nA policy is a mapping from states to probabilties of selecting each action and is written as \\(\\pi(a\\mid s)\\).\nSimple MDP example, like buy carrots not anything else\nPolicy, value function, optimal policy, optimal value function"
  },
  {
    "objectID": "aipcs24/2othergames_reading.html#war",
    "href": "aipcs24/2othergames_reading.html#war",
    "title": "#2: Other Games | Reading",
    "section": "War",
    "text": "War\nIf you thought that Kuhn Poker was a simple card game, meet War.\nThere are two players, each gets half the deck, 26 cards. Each player turns over their top card and faces it against the opponent‚Äôs top card. The better card wins, with Aces high. This repeats until one player has all the cards.\nWhen the cards match, the players go to ‚ÄúWar‚Äù. When this happens, put the next card face down, and then the card after that face up, and then these up-cards face off against each other. The winner takes all six cards. If there‚Äôs another tie, then repeat and the winner takes 10 cards, etc.\n\n\n\n\n\n\nOptional Coding Exercise\n\n\n\nCode and run 10,000 simulations of War and determine how many turns the average game takes and how many War situations occur on average in each simulation.\n\n\nYou can see a Dreidel game simulator written by Ben Blatt in Slate from 2014 at this link."
  },
  {
    "objectID": "aipcs24/2othergames_reading.html#tictactoe",
    "href": "aipcs24/2othergames_reading.html#tictactoe",
    "title": "#2: Other Games | Reading",
    "section": "Tictactoe",
    "text": "Tictactoe\n\nEvolutionary algorithms for poker and tic-tac-toe\nExercise: Look at this game tree with payouts at the bottom written in terms of Player 1. Start from the bottom of the tree and figure out the actions of each player at each node. Then figure out the value of the game for Player 1 and for Player 2. This procedure is called backpropagation.\nQuestion: Why can‚Äôt we use this procedure in Kuhn Poker and imperfect info games? Tree imperfect info vs perfect info issues, show trees Compare this to the Wabbits game and problem from paper\n\nMinimax\nhttps://www.neverstopbuilding.com/blog/minimax\n\n\nValue Function\nMinimax assumes opponent playing best plays too temporal-difference\n\n\nRL: Planning and Learning\n\n\nRL: Monte Carlo Tree Search\nhttps://starai.cs.ucla.edu/papers/VdBBNAIC09.pdf What about just using imperfect info version of MCTS?"
  },
  {
    "objectID": "aipcs24/2othergames_reading.html#multi-armed-bandits",
    "href": "aipcs24/2othergames_reading.html#multi-armed-bandits",
    "title": "#2: Other Games | Reading",
    "section": "Multi-Armed Bandits",
    "text": "Multi-Armed Bandits\n\nBandits are a set of problems with repeated decisions and a fixed number of actions possible coming from a single state.\nThe agent updates its strategy based on what it learns from the feedback from the environment. You could think of this in real-world settings like picking which dish to eat at a restaurant.\nConsider a 10-armed bandit setup like in the image below:\n\n\n\nSutton Barto Bandits\n\n\nEach time the player pulls an arm, they get some reward, which could be positive or negative.\nA basic setting initializes each of 10 arms with \\(q*(\\text{arm}) = \\mathcal{N}(0,1)\\) so each is initialized with a center point around a Gaussian distribution. Each pull of an arm returns a reward of \\(R = \\mathcal{N}(q*(\\text{arm}_i), 1)\\).\nIn other words, each arm is initialized with a value centered around \\(0\\) but with some variance, so each will be a bit different. Then from that point, the actual pull of an arm is centered around that new point with some variance as seen in this figure above.\nThe agent can sample actions and estimate their values based on experience and then use some algorithm for deciding which action to use next to maximize rewards. The estimated value of action \\(a\\) at timestep \\(t\\) is defined as \\(Q_t(a)\\).\nThe agent is then faced with two competing goals:\n\nGet as accurate an estimate \\(Q_t(a)\\) as possible\nSelect actions with the highest rewards as much as possible\n\nExploring refers to figuring out the values of the arms, or in the case of a restaurant, figuring out how good each dish is.\nExploiting refers to using current knowledge to choose the highest value estimated action.\n\n\n\n\n\n\nExercise\n\n\n\nA naive approach is that a player could sample each action once and then always use the action that gave the best reward going forward.\nWhat is a problem with this strategy? Can you suggest a better one?\n\n\n\\(Q_t(a)\\) can simply be estimated by averaging the rewards received each time a specific arm has been tried. The so-called greedy action rule is to then take the largest \\(Q_t(a)\\) action, \\(A_t = \\argmax_{a} Q_t(a)\\)\n\nRegret\nHow would you define regret in this bandit setting? How is minimizing regret related to maximizing reward? explore exploit\n\n\nRL: Action-Value Methods\nHidden imperfect info, understand in distribution over possible states of the world Depending on state will want to make decisions differently Bayesian explore exploit optimizer What is my opponent range exercise in Bayesian update Explore vs.¬†exploit Bayesian updating https://aipokertutorial.com/game-theory-foundation/#regret https://www.reddit.com/r/statistics/comments/1949met/how_are_multi_armed_bandits_related_to_bayesian/ https://tor-lattimore.com/downloads/book/book.pdf https://lcalem.github.io/blog/2018/09/22/sutton-chap02-bandits#26-optimistic-initial-values\nBut you could use the MCTS that does work by changing the game setup"
  },
  {
    "objectID": "aipcs24/2othergames_reading.html#blackjack",
    "href": "aipcs24/2othergames_reading.html#blackjack",
    "title": "#2: Other Games | Reading",
    "section": "Blackjack",
    "text": "Blackjack\nSetup Solving\n\nRL: Dynamic Programming\nMy blog post\n\n\nRL: Monte Carlo Methods"
  },
  {
    "objectID": "aipcs24/2othergames_reading.html#gridworld",
    "href": "aipcs24/2othergames_reading.html#gridworld",
    "title": "#2: Other Games | Reading",
    "section": "Gridworld",
    "text": "Gridworld\npg 60\n\nRL: Q-Learning"
  },
  {
    "objectID": "aipcs24/2othergames_reading.html#poker-games",
    "href": "aipcs24/2othergames_reading.html#poker-games",
    "title": "#2: Other Games | Reading",
    "section": "Poker Games",
    "text": "Poker Games\nWhy are we mostly interested in poker games? We think that\nimperfect adversarial widely played well researched, but only nash equilibrium"
  },
  {
    "objectID": "aipcs24/1kuhn_k.html",
    "href": "aipcs24/1kuhn_k.html",
    "title": "#1: Kuhn Poker | WTK",
    "section": "",
    "text": "During the first AI Poker Camp session on Monday 7/15/24, we asked participants to list the deterministic situations in the Kuhn Poker game. In other words, points of the game where it is strictly best to play a certain action.\nWe started with the full game tree:\n\nWe came up with a list of 5 of these:\n\n\n\n\n\n\n\n\n#\nScenario\nAction\n\n\n\n\n1\n_Q‚Üë: Q as Player 2 facing a bet\nShould always play ‚Üì fold\n\n\n2\n_A‚Üë: A as Player 2 facing a bet\nShould always play ‚Üë call\n\n\n3\n_A‚Üì: A as Player 2 facing a check\nShould always play ‚Üë bet\n\n\n4\nQ_‚Üì‚Üë: Q as Player 1 checking and then facing a bet\nShould always play ‚Üì fold\n\n\n5\nA_‚Üì‚Üë: A as Player 2 checking and then facing a bet\nShould always play ‚Üë bet\n\n\n\nThe game tree is then reduced to:\n\nThere are 2 others that we knew were slightly different, but we still believed that they belonged in this category:\n\n\n\n\n\n\n\n\n#\nScenario\nAction\n\n\n\n\n6\nK_‚Üì: K as Player 2 facing a check\n\n\n\n7\nK_: K as Player 1 acting first\n\n\n\n\nMy assumption was that 6 and 7 were trickier because they relied on the opponent playing under the conditions of 1-5 above. Things turned around and I became the confused one when a couple of participants pointed out that at K_ it is not always clearly best to play ‚Üì!\nBetting with a K knowing that your opponent will always play perfectly (fold with Q and call with A) does not seem intuitively very profitable, but this exercise is asking for deterministic situations where it is definitely best to play one action.\nGoing forward on this page, we are assuming that players play their mandatory +EV decisions correctly (i.e., numbers 1-5 above)."
  },
  {
    "objectID": "aipcs24/1kuhn_k.html#what-the-k",
    "href": "aipcs24/1kuhn_k.html#what-the-k",
    "title": "#1: Kuhn Poker | WTK",
    "section": "",
    "text": "During the first AI Poker Camp session on Monday 7/15/24, we asked participants to list the deterministic situations in the Kuhn Poker game. In other words, points of the game where it is strictly best to play a certain action.\nWe started with the full game tree:\n\nWe came up with a list of 5 of these:\n\n\n\n\n\n\n\n\n#\nScenario\nAction\n\n\n\n\n1\n_Q‚Üë: Q as Player 2 facing a bet\nShould always play ‚Üì fold\n\n\n2\n_A‚Üë: A as Player 2 facing a bet\nShould always play ‚Üë call\n\n\n3\n_A‚Üì: A as Player 2 facing a check\nShould always play ‚Üë bet\n\n\n4\nQ_‚Üì‚Üë: Q as Player 1 checking and then facing a bet\nShould always play ‚Üì fold\n\n\n5\nA_‚Üì‚Üë: A as Player 2 checking and then facing a bet\nShould always play ‚Üë bet\n\n\n\nThe game tree is then reduced to:\n\nThere are 2 others that we knew were slightly different, but we still believed that they belonged in this category:\n\n\n\n\n\n\n\n\n#\nScenario\nAction\n\n\n\n\n6\nK_‚Üì: K as Player 2 facing a check\n\n\n\n7\nK_: K as Player 1 acting first\n\n\n\n\nMy assumption was that 6 and 7 were trickier because they relied on the opponent playing under the conditions of 1-5 above. Things turned around and I became the confused one when a couple of participants pointed out that at K_ it is not always clearly best to play ‚Üì!\nBetting with a K knowing that your opponent will always play perfectly (fold with Q and call with A) does not seem intuitively very profitable, but this exercise is asking for deterministic situations where it is definitely best to play one action.\nGoing forward on this page, we are assuming that players play their mandatory +EV decisions correctly (i.e., numbers 1-5 above)."
  },
  {
    "objectID": "aipcs24/1kuhn_k.html#k",
    "href": "aipcs24/1kuhn_k.html#k",
    "title": "#1: Kuhn Poker | WTK",
    "section": "_K‚Üì",
    "text": "_K‚Üì\n\nWe are looking at TPOT (this part of the tree) when  has a K and is facing a ‚Üì (check):\n\nIf playing ‚Üë, we have: \\[\n\\mathbb{E} = -2*\\Pr(\\text{P1 A Plays \\downarrow}) + 1*\\Pr(\\text{P1 Q Plays \\downarrow})\n\\]\nPlaying ‚Üì has: \\[\n\\mathbb{E} = -1*\\Pr(\\text{P1 A Plays \\downarrow}) + 1*\\Pr(\\text{P1 Q Plays \\downarrow})\n\\]\nTherefore in this case ‚Üì is strictly best and we can include this case on our list of deterministic situations.\nThe updated tree:"
  },
  {
    "objectID": "aipcs24/1kuhn_k.html#k_",
    "href": "aipcs24/1kuhn_k.html#k_",
    "title": "#1: Kuhn Poker | WTK",
    "section": "K_",
    "text": "K_\nWe are now looking at TPOT (this part of the tree) and can end up on one of these 7 nodes:\n\nAt infoset K_, when the K opening action plays ‚Üë, we end up in 2 possible nodes and have known EV:\n\\[\n\\begin{align}\n\\mathbb{E} &= -2*\\Pr(\\text{P2 has A}) + 1*\\Pr(\\text{P2 has Q}) \\\\\n&= -2*0.5 + 1*0.5 \\\\\n&= -0.5\n\\end{align}\n\\]\nNow things get interesting.\nWhen the K opening action plays ‚Üì, we can compute the EV:\n\nAssume that  plays ‚Üë at K‚Üì‚Üë with probability \\(k\\).\nAssume that  plays ‚Üë at _Q‚Üì with probability \\(q\\).\n\nThere are 3 cases of what can happen now after  plays ‚Üì at K_ and we can end up in 5 possible nodes:\n\n has A and plays ‚Üë.\n\n\\[\n\\begin{align}\n\\mathbb{E} &= -2*\\Pr(\\text{P1 Calls K}) - 1*\\Pr(\\text{P1 Folds K}) \\\\\n&= -2*k - 1*(1-k) \\\\\n&= -k - 1\n\\end{align}\n\\]\n\n has Q and plays ‚Üì.\n\n\\[\n\\mathbb{E} = 1\n\\]\n\n has Q and plays ‚Üë.\n\n\\[\n\\begin{align}\n\\mathbb{E} &= 2*\\Pr(\\text{P1 Calls K}) - 1*\\Pr(\\text{P1 Folds K}) \\\\\n&= 2*k - 1*(1-k) \\\\\n&= 3*k - 1\n\\end{align}\n\\]\nPutting these together, we have:\n\\[\n\\begin{align}\n\\mathbb{E} &= [-k - 1]*\\Pr(\\text{P2 has A}) + [1]*\\Pr(\\text{P2 has Q and Checks}) + [3*k - 1]*\\Pr(\\text{P2 Has Q and Bets}) \\\\\n&= [-k - 1]*0.5 + [1]*0.5*(1-q) + [3*k - 1]*0.5*(q) \\\\\n&= -0.5*k - 0.5 + 0.5 - 0.5*q + 1.5*k*q - 0.5*q \\\\\n&= -0.5*k - q + 1.5*k*q\n\\end{align}\n\\]\n\nimport {Plot} from '@observablehq/plot'\n\n// Create sliders for k and q, constrained between 0 and 1\nviewof k = Inputs.range([0, 1], {step: 0.01, label: \"k\"})\nviewof q = Inputs.range([0, 1], {step: 0.01, label: \"q\"})\n\n// Calculate z based on the function\nz = -0.5 * k - q + 1.5 * k * q\n\n// Display the current value of z\nmd`The current value of z is: ${z.toFixed(4)}`\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfunction createPlot(k, q) {\n  const points = [];\n  for (let x = 0; x &lt;= 1; x += 0.02) {\n    for (let y = 0; y &lt;= 1; y += 0.02) {\n      points.push({x, y, z: -0.5 * x - y + 1.5 * x * y});\n    }\n  }\n\n  return Plot.plot({\n    width: 600,\n    height: 400,\n    x: {label: \"k\", domain: [0, 1]},\n    y: {label: \"q\", domain: [0, 1]},\n    color: {\n      type: \"linear\",\n      domain: [-1, 1],\n      scheme: \"RdBu\"\n    },\n    marks: [\n      Plot.contour(points, {x: \"x\", y: \"y\", z: \"z\", stroke: \"currentColor\", interval: 0.05}),\n      Plot.image(points, {x: \"x\", y: \"y\", z: \"z\", interpolate: \"nearest\"}),\n      Plot.dot([{x: k, y: q}], {x: \"x\", y: \"y\", stroke: \"red\", fill: \"red\"})\n    ]\n  })\n}\n\ncreatePlot(k, q)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMathematical Conclusions\nWe can set \\(-0.5 = -0.5*k - q + 1.5*k*q\\) to compare the EV of betting to checking.\nWe see that if you have check and then call with at least \\(k \\geq 0.5\\) then you do at least as good as betting. You do strictly better if \\(0.5 &lt; k &lt; 1\\)."
  },
  {
    "objectID": "aipcs24/1kuhn_rules.html",
    "href": "aipcs24/1kuhn_rules.html",
    "title": "#1: Kuhn Poker | Rules",
    "section": "",
    "text": "Kuhn Poker is the simplest nontrivial version of poker. Here is the setup:\n\n3 card deck: Queen, King, Ace (in ascending order, so Ace is highest)\n\n\n\n\n\n\n\nAlternative (but equivalent) decks\n\n\n\n\n\nAny deck of 3 ranked cards works. For example:\n\nThe python game engine that we‚Äôll use in later challenges uses {0, 1, 2}.\nWikipedia‚Äôs description uses {Jack, Queen, King}.\n\n\n\n\n \n\nKuhn Poker is played with 2 players. For this exercise, they‚Äôll be  and .\nEach player starts with 2 chips and both players ante 1 chip.\nDeal 1 card to each player (discard the third).\n\n\n\nThere is one street of betting in which players can take these actions:\n\n‚ÜëUp (putting a chip into the pot)\n‚ÜìDown (not putting a chip into the pot)\n\n\n\n\n\n\n\n\n‚ÜëUp and ‚ÜìDown in traditional poker terms\n\n\n\n\n‚ÜëUp actions indicate a Bet or Call.\n‚ÜìDown actions indicate a Check or Fold.\n\n\n\n\n\n\n\n\n\nExample game\n\n\n\n\nPlayers ante and cards are dealt.\n sees a A and plays ‚ÜëUp (1 more chip into pot).\n sees a K and plays ‚ÜëUp (1 more chip into pot).\nBoth cards are revealed in a 2-chip showdown.  has an A and  has a K.\n has the better hand and wins +2 chips (+1 from the ante, +1 from ‚Äôs ‚ÜëUp).\n\n\n\nThe betting (and the game) can go in three ways:\n\nOne player plays ‚ÜëUp, then the other player plays ‚ÜìDown. The player who played ‚ÜìDown folds. The winner wins the loser‚Äôs ante (and gets their own chip back). The players‚Äô cards are not revealed. Note that this happens if the action is :\n\n\n‚ÜëUp, ‚ÜìDown, or\n‚ÜìDown, ‚ÜëUp, ‚ÜìDown.\n\n\nBoth players play ‚ÜìDown. They go to a showdown and the winner wins the one chip that the loser anted (and their own back).\nA player plays ‚ÜëUp, then the other player plays ‚ÜëUp. They go to a showdown and the winner wins the two chips the loser has put in the pot (and gets their own chips back). Note that the game will proceed to a 2-chip showdown if the action is:\n\n\n‚ÜëUp, ‚ÜëUp or - ‚ÜìDown, ‚ÜëUp , ‚ÜëUp.\n\nHere is a list of all possible betting sequences:\n\n\n\n\n\n\n\n\n\n\n\n\nWinner\n\n\n\n\n‚ÜëUp\n‚ÜìDown\n\n (+1)\n\n\n‚ÜëUp\n‚ÜëUp\n\nHigher Card (+2)\n\n\n‚ÜìDown\n‚ÜìDown\n\nHigher Card (+1)\n\n\n‚ÜìDown\n‚ÜëUp\n‚ÜìDown\n (+1)\n\n\n‚ÜìDown\n‚ÜëUp\n‚ÜëUp\nHigher Card (+2)\n\n\n\n\n\n\n\n\n\nPartner Exercise: Get started with Kuhn Poker\n\n\n\n\nGet cards, chips, paper, pen\nPlay 3 hands of Kuhn Poker and get used to how the game runs\nUse the pen and paper to start writing down all deterministic situations in the game (situations where there is clearly a correct move that you should take 100% of the time)\nPlay more hands as you think helpful\nOnce you have what you think is a full list of the deterministic states, you can stop and review the optional reading",
    "crumbs": [
      "Signup: AI Poker Camp Summer 2024",
      "Challenges",
      "#1: Kuhn Poker",
      "Rules"
    ]
  },
  {
    "objectID": "aipcs24/1kuhn_rules.html#kuhn-poker-rules",
    "href": "aipcs24/1kuhn_rules.html#kuhn-poker-rules",
    "title": "#1: Kuhn Poker | Rules",
    "section": "",
    "text": "Kuhn Poker is the simplest nontrivial version of poker. Here is the setup:\n\n3 card deck: Queen, King, Ace (in ascending order, so Ace is highest)\n\n\n\n\n\n\n\nAlternative (but equivalent) decks\n\n\n\n\n\nAny deck of 3 ranked cards works. For example:\n\nThe python game engine that we‚Äôll use in later challenges uses {0, 1, 2}.\nWikipedia‚Äôs description uses {Jack, Queen, King}.\n\n\n\n\n \n\nKuhn Poker is played with 2 players. For this exercise, they‚Äôll be  and .\nEach player starts with 2 chips and both players ante 1 chip.\nDeal 1 card to each player (discard the third).\n\n\n\nThere is one street of betting in which players can take these actions:\n\n‚ÜëUp (putting a chip into the pot)\n‚ÜìDown (not putting a chip into the pot)\n\n\n\n\n\n\n\n\n‚ÜëUp and ‚ÜìDown in traditional poker terms\n\n\n\n\n‚ÜëUp actions indicate a Bet or Call.\n‚ÜìDown actions indicate a Check or Fold.\n\n\n\n\n\n\n\n\n\nExample game\n\n\n\n\nPlayers ante and cards are dealt.\n sees a A and plays ‚ÜëUp (1 more chip into pot).\n sees a K and plays ‚ÜëUp (1 more chip into pot).\nBoth cards are revealed in a 2-chip showdown.  has an A and  has a K.\n has the better hand and wins +2 chips (+1 from the ante, +1 from ‚Äôs ‚ÜëUp).\n\n\n\nThe betting (and the game) can go in three ways:\n\nOne player plays ‚ÜëUp, then the other player plays ‚ÜìDown. The player who played ‚ÜìDown folds. The winner wins the loser‚Äôs ante (and gets their own chip back). The players‚Äô cards are not revealed. Note that this happens if the action is :\n\n\n‚ÜëUp, ‚ÜìDown, or\n‚ÜìDown, ‚ÜëUp, ‚ÜìDown.\n\n\nBoth players play ‚ÜìDown. They go to a showdown and the winner wins the one chip that the loser anted (and their own back).\nA player plays ‚ÜëUp, then the other player plays ‚ÜëUp. They go to a showdown and the winner wins the two chips the loser has put in the pot (and gets their own chips back). Note that the game will proceed to a 2-chip showdown if the action is:\n\n\n‚ÜëUp, ‚ÜëUp or - ‚ÜìDown, ‚ÜëUp , ‚ÜëUp.\n\nHere is a list of all possible betting sequences:\n\n\n\n\n\n\n\n\n\n\n\n\nWinner\n\n\n\n\n‚ÜëUp\n‚ÜìDown\n\n (+1)\n\n\n‚ÜëUp\n‚ÜëUp\n\nHigher Card (+2)\n\n\n‚ÜìDown\n‚ÜìDown\n\nHigher Card (+1)\n\n\n‚ÜìDown\n‚ÜëUp\n‚ÜìDown\n (+1)\n\n\n‚ÜìDown\n‚ÜëUp\n‚ÜëUp\nHigher Card (+2)\n\n\n\n\n\n\n\n\n\nPartner Exercise: Get started with Kuhn Poker\n\n\n\n\nGet cards, chips, paper, pen\nPlay 3 hands of Kuhn Poker and get used to how the game runs\nUse the pen and paper to start writing down all deterministic situations in the game (situations where there is clearly a correct move that you should take 100% of the time)\nPlay more hands as you think helpful\nOnce you have what you think is a full list of the deterministic states, you can stop and review the optional reading",
    "crumbs": [
      "Signup: AI Poker Camp Summer 2024",
      "Challenges",
      "#1: Kuhn Poker",
      "Rules"
    ]
  },
  {
    "objectID": "camp/index.html",
    "href": "camp/index.html",
    "title": "Poker and Applied Rationality Camp",
    "section": "",
    "text": "Coming soon:\n\nAug/Sep TBD: Beta in NYC or virtual\nSep 23-Nov 25: Full course virtual"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Poker Camp is a project by Ross Rheingans-Yoo and Max Chiswick."
  },
  {
    "objectID": "about.html#mission",
    "href": "about.html#mission",
    "title": "About",
    "section": "Mission",
    "text": "Mission\nTeaching practical decision-making and rationality through poker and other games, blending real-world skills with theoretical insights"
  },
  {
    "objectID": "about.html#max-bio",
    "href": "about.html#max-bio",
    "title": "About",
    "section": "Max bio",
    "text": "Max bio\n\nFormer online poker pro on PokerStars, playing over 10m hands including 990k in 1 month (record for most ever played in a month above microstakes)\nAI poker work:\n\nMSc with thesis on AI poker at Technion Israel Institute of Technology with Nahum Shimkin\nAI Poker Tutorial website (needs updating!)\nTwo poker research papers with Sam Ganzfried\n\nSubstack"
  },
  {
    "objectID": "about.html#ross-bio",
    "href": "about.html#ross-bio",
    "title": "About",
    "section": "Ross bio",
    "text": "Ross bio\n\nFormer Jane Street trader\nWrote the advanced trading simulations for the Jane Street trading internship program from 2018 to 2021\nHomepage\nTwitter"
  },
  {
    "objectID": "about.html#contribute",
    "href": "about.html#contribute",
    "title": "About",
    "section": "Contribute",
    "text": "Contribute\n\nWe are in the process of becoming a 501(c)(3) nonprofit. If you are interested in contributing monetarily, please see our .org site: https://pokercamp.org.\nIf you are interested in contributing to materials or working with us, be in touch"
  }
]