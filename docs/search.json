[
  {
    "objectID": "aicamp/index.html",
    "href": "aicamp/index.html",
    "title": "AI Poker Camp",
    "section": "",
    "text": "Coming soon:\n\nJul 8-Aug 8: Beta in SF\nAug 19-Sep 19: Beta v2 in NYC\nSep 24-Nov 26: Full course virtual"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Poker Camp",
    "section": "",
    "text": "Poker Camp is a program to learn about probability, game theory, AI, and decision making under uncertainty through the lens of poker.\nOur first camp will take place from Jul 15 to Aug 15 in person in San Francisco.\n\n\n\n\n\n\nSign up now!\n\n\n\nSignup Form\n\nAfter signing up, we’ll be in touch with all details\nThe beta program is free!\n\n\n\nClick here for more info\n\n\n\nNote that the programming and related materials/workshops are for educational purposes and will not use any real money."
  },
  {
    "objectID": "aipcs24/index.html",
    "href": "aipcs24/index.html",
    "title": "AI Poker Camp Summer 2024",
    "section": "",
    "text": "WELCOME ETC"
  },
  {
    "objectID": "signup/index.html",
    "href": "signup/index.html",
    "title": "AI Poker Camp (2024 Summer Beta SF)",
    "section": "",
    "text": "Sign up now!\n\n\n\nSignup Form\n\nAfter signing up, we’ll be in touch with all details\nThe beta program is free!"
  },
  {
    "objectID": "signup/index.html#when",
    "href": "signup/index.html#when",
    "title": "AI Poker Camp (2024 Summer Beta SF)",
    "section": "When?",
    "text": "When?\n6pm-8pm Mondays and Thursdays, July 15 through August 15."
  },
  {
    "objectID": "signup/index.html#what",
    "href": "signup/index.html#what",
    "title": "AI Poker Camp (2024 Summer Beta SF)",
    "section": "What?",
    "text": "What?\nA five-week, twice-weekly course on applied game theory through you writing AIs to play games. By the end, you should be able to write an AI to play poker.\nThis is a beta test of a course we’re planning to run online in the fall, so it’ll be small. We’ll cap signups somewhere between 16 and 24 students."
  },
  {
    "objectID": "signup/index.html#where",
    "href": "signup/index.html#where",
    "title": "AI Poker Camp (2024 Summer Beta SF)",
    "section": "Where?",
    "text": "Where?\nStrictly in-person in San Francisco. Location to be announced.\n\nWait, really? I can’t make that!\nWe’re also planning to run an online version of the course starting in late September. You can join our mailing list to learn more when it’s announced."
  },
  {
    "objectID": "signup/index.html#who",
    "href": "signup/index.html#who",
    "title": "AI Poker Camp (2024 Summer Beta SF)",
    "section": "Who?",
    "text": "Who?\n\nWe recommend students be able to program in Python and perform a Bayesian update (though it’s fine to lean on an LLM for help on either).\nKnowledge of the game of poker is not necessary. (We are offering a 2 hour Poker Basics workshop on Sun Jul 7 for a $20 supplement.)"
  },
  {
    "objectID": "signup/index.html#whats-the-curriculum",
    "href": "signup/index.html#whats-the-curriculum",
    "title": "AI Poker Camp (2024 Summer Beta SF)",
    "section": "What’s the curriculum?",
    "text": "What’s the curriculum?\nThe course is built around six or seven practical challenges – think “kaggle competition for game-playing programs”. These will cover:\n\nTutorial: One-card / Kuhn Poker\n\nTopic: Algorithms for solving incomplete-information games.\n\nLarger one-card poker formats and other simple games\n\nTopic: Scaling up algorithms to larger game trees.\n\nRock-Paper-Scissors against imperfect opponents\n\nTopic: Techniques for modeling empirical opponent behavior.\n\nHidden-information version of Probabilistic Tic-Tac-Toe\n\nTopic: Modeling hidden information from opponent actions.\n\nTexas Holdem with simplified betting\n\nTopic: Putting it together!\n\n\nWe’re intending for the Summer Beta to have about the intensity of one (1) college course in applied CS. You should expect to make at least 9 out of 10 class sessions."
  },
  {
    "objectID": "signup/index.html#whos-teaching",
    "href": "signup/index.html#whos-teaching",
    "title": "AI Poker Camp (2024 Summer Beta SF)",
    "section": "Who’s teaching?",
    "text": "Who’s teaching?\nRoss Rheingans-Yoo wrote the advanced trading simulations for the Jane Street trading internship program from 2018 to 2021.\nMax Chiswick is a former poker pro who has played more than 10 million hands of online poker, and created AI Poker Tutorial.\nRicki Heicklen is a curriculum advisor (but will not be teaching in the SF Beta)."
  },
  {
    "objectID": "signup/index.html#how",
    "href": "signup/index.html#how",
    "title": "AI Poker Camp (2024 Summer Beta SF)",
    "section": "How?",
    "text": "How?\nSignup Form\n\nAfter signing up, we’ll be in touch with all details\nThe beta program is free!"
  },
  {
    "objectID": "signup/index.html#something-else",
    "href": "signup/index.html#something-else",
    "title": "AI Poker Camp (2024 Summer Beta SF)",
    "section": "Something else?",
    "text": "Something else?\nGet in touch!"
  },
  {
    "objectID": "aipcs24/challenge1kuhn.html",
    "href": "aipcs24/challenge1kuhn.html",
    "title": "Challenge 1: Kuhn Poker CFR Tutorial",
    "section": "",
    "text": "(This game is modified from Lisy et al. 2015).\n\n\n\nBugs and Fudd\n\n\nThis game has one chance player (the Author), one maximizing player (Fudd) and one minimizing player (Bugs). Fudd and Bugs are in a long-term, all-out war, and so any energy that Fudd wastes or saves is a gain or loss for Bugs; our game is zero-sum.\nFirst, the Author will choose whether to write an episode where Bugs is at the Opera (50%) or in the Forest (50%). Fudd cannot tell what the Author has chosen.\nNext, Fudd will decide whether to Hunt_Near or Hunt_Far. If he chooses Hunt_Far, he takes -1 value for the extra effort.\nBugs knows whether the Author wrote him into the Opera or Forest, but he does not know Fudd’s hunting location. If the Author gives him the Opera, Bugs has no choices to make and the game ends. If Forest, Bugs will decide whether to Play_Near or Play_Far. If he plays in the same location as Fudd is hunting, Fudd gets +3 value for a successful hunt. If they are in different locations (or if Bugs is at the Opera), Fudd will get 0 value for an unsuccessful hunt.\nPutting it all together, the payoff structure of this game is:\n\n\n\nAuthor\nBugs/Fudd\nHunt_Near\nHunt_Far\n\n\n\n\nOpera\n\n0, 0\n+1, -1\n\n\nForest\nPlay_Near\n-3, +3\n0, 0\n\n\nForest\nPlay_Far\n+1, -1\n-2, +2\n\n\n\nIf it were a complete-information game, the tree structure would be:\n\nIf Fudd knows he’s at the Opera, then he prefers to Hunt Near, but since he doesn’t know his location, he must take both scenarios into account. Note that Bugs’s optimal actions depend on Fudd’s Opera action even though that outcome cannot be reached once Bugs is playing!",
    "crumbs": [
      "Signup: AI Poker Camp Summer 2024",
      "Challenges",
      "1: Kuhn Poker"
    ]
  },
  {
    "objectID": "aipcs24/challenge1kuhn.html#hunting-wabbits",
    "href": "aipcs24/challenge1kuhn.html#hunting-wabbits",
    "title": "Challenge 1: Kuhn Poker CFR Tutorial",
    "section": "",
    "text": "(This game is modified from Lisy et al. 2015).\n\n\n\nBugs and Fudd\n\n\nThis game has one chance player (the Author), one maximizing player (Fudd) and one minimizing player (Bugs). Fudd and Bugs are in a long-term, all-out war, and so any energy that Fudd wastes or saves is a gain or loss for Bugs; our game is zero-sum.\nFirst, the Author will choose whether to write an episode where Bugs is at the Opera (50%) or in the Forest (50%). Fudd cannot tell what the Author has chosen.\nNext, Fudd will decide whether to Hunt_Near or Hunt_Far. If he chooses Hunt_Far, he takes -1 value for the extra effort.\nBugs knows whether the Author wrote him into the Opera or Forest, but he does not know Fudd’s hunting location. If the Author gives him the Opera, Bugs has no choices to make and the game ends. If Forest, Bugs will decide whether to Play_Near or Play_Far. If he plays in the same location as Fudd is hunting, Fudd gets +3 value for a successful hunt. If they are in different locations (or if Bugs is at the Opera), Fudd will get 0 value for an unsuccessful hunt.\nPutting it all together, the payoff structure of this game is:\n\n\n\nAuthor\nBugs/Fudd\nHunt_Near\nHunt_Far\n\n\n\n\nOpera\n\n0, 0\n+1, -1\n\n\nForest\nPlay_Near\n-3, +3\n0, 0\n\n\nForest\nPlay_Far\n+1, -1\n-2, +2\n\n\n\nIf it were a complete-information game, the tree structure would be:\n\nIf Fudd knows he’s at the Opera, then he prefers to Hunt Near, but since he doesn’t know his location, he must take both scenarios into account. Note that Bugs’s optimal actions depend on Fudd’s Opera action even though that outcome cannot be reached once Bugs is playing!",
    "crumbs": [
      "Signup: AI Poker Camp Summer 2024",
      "Challenges",
      "1: Kuhn Poker"
    ]
  },
  {
    "objectID": "aipcs24/challenge1kuhn.html#concept-information-set",
    "href": "aipcs24/challenge1kuhn.html#concept-information-set",
    "title": "Challenge 1: Kuhn Poker CFR Tutorial",
    "section": "Concept: Information Set",
    "text": "Concept: Information Set\nIn a complete-information game, we can draw a tree of the possible states the game can be in. Every time a player is called to take an action, they will know what node they are at, and what node they will go to with each legal action.\nIn an incomplete-information game, we can still draw that tree, but now a player might be called to take an action without knowing what node they are actually at. Instead, there will be a set of one or more nodes that are indistinguishable to that player based on what they have seen so far, and they will have to take an action knowing only that they are in that set. Such a set of nodes is an information set or an infoset.\nA player strategy is a rule that says, for every information set that player will face, what action or (random choice of actions) that player will take. For a game like Hunting Wabbits or Kuhn Poker, we can list every information set and its probabilities. For a more complicated game, we might write our strategy as a computation that will output probabilities based on inputs and an algorithm.",
    "crumbs": [
      "Signup: AI Poker Camp Summer 2024",
      "Challenges",
      "1: Kuhn Poker"
    ]
  },
  {
    "objectID": "aipcs24/challenge1kuhn.html#concept-expected-value",
    "href": "aipcs24/challenge1kuhn.html#concept-expected-value",
    "title": "Challenge 1: Kuhn Poker CFR Tutorial",
    "section": "Concept: Expected Value",
    "text": "Concept: Expected Value\nOnce we’ve assigned definite values to the ultimate outcomes, the expected value of a situation is the value of the average outcome, weighted by the probability that you get to that outcome, respectively.\n\n\n\n\n\n\nExercise\n\n\n\nSuppose that Bugs plays uniform \\(0.5\\) Play_Near and \\(0.5\\) Play_Far.\n\nWhat is the value of each Bugs node and what should Fudd do if he knew Bugs’s actions?\nWhat is Fudd’s expected value in this case?\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nBugs’s node values are:\n\n\\(\\mathbb{E}(\\text{Left Node}) = 0.5*-3 + 0.5*0 = -1.5\\)\n\\(\\mathbb{E}(\\text{Right Node}) = 0.5*1 + 0.5*-2 = -0.5\\)\nThe values for Fudd are inverse, so Fudd prefers the Left Node with a value of \\(1.5\\). This means that if Fudd is in the Forest, he prefers to Hunt_Near. Lucky for him, he also prefers to Hunt_Near in the Opera.\n\nTherefore Fudd will always play Hunt_Near and will have the following expected value:\n\n\\[\n\\begin{equation}\n\\begin{split}\n\\mathbb{E}(\\text{Hunt\\_Near}) &= \\Pr(\\text{Opera})*u(\\text{Hunt\\_Near}) + \\Pr(\\text{Forest})*u(\\text{Hunt\\_Near}) \\\\ \\\\\n  &= 0.5*0 + 0.5*1.5 \\\\\n  &= 0.75\n\\end{split}\n\\end{equation}\n\\]\n\n\n\nIn imperfect-information games, we consider probabilities over two different sources of uncertainty, after assuming a particular P1 strategy and P2 strategy:\n\nUncertainty about which node we are actually in, given that we know that we’re in one of multiple nodes that we can’t tell apart.\n\n\nThe probabilities of being in node 1, node 2, … of an information set can be calculated by the probabilities of decisions upwards in the game tree (and the probabilites of chance events that have already happened).\n\n\nUncertainty about what will happen after we go to a node downwards in the game tree, coming from chance events or strategy probabilities in the players’ following actions.\n\nBy taking the expected value over the second type of uncertainty, we calculate the expected values of being at each node we might be going to (e.g., EV at the node [ Forest, Hunt_Near ]). Then by taking the expected value over the first type of uncertainty, we calculate the expected value of taking a given action at that information set (e.g., EV of playing Play_Near at infoset [ Forest, _ ]).\n\nWe will focus on zero-sum two-player games, so the value to one player is simply the negative of the value to the other. Therefore, we can represent value in the game as a single number that the maximizing player wishes to make positive and the minimizing player wishes to make negative.\nWe will focus on maximizing (or minimizing) expected value as our goal for all of Course 1. One thing that makes it natural to care about expected value is that it’s usually the best way to predict what your score will be after a very large number of games, whether they are the same game or different from each other.",
    "crumbs": [
      "Signup: AI Poker Camp Summer 2024",
      "Challenges",
      "1: Kuhn Poker"
    ]
  },
  {
    "objectID": "aipcs24/challenge1kuhn.html#concept-regret",
    "href": "aipcs24/challenge1kuhn.html#concept-regret",
    "title": "Challenge 1: Kuhn Poker CFR Tutorial",
    "section": "Concept: Regret",
    "text": "Concept: Regret\nFor a given P1 strategy and P2 strategy, a player has regret when they take an action at an infoset that was not the highest-EV action at that infoset. The amount of regret is the difference between the highest-EV action and the selected action.\n\n\n\n\n\n\nExercise\n\n\n\n\nWhat is the regret for each action?\n\n\n\nAction\nRegret\n\n\n\n\nA\n\n\n\nB\n\n\n\nC\n\n\n\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\n\nAction\nRegret\n\n\n\n\nA\n4\n\n\nB\n2\n\n\nC\n0\n\n\n\n\n\n\nThere are other things that “regret” can mean in English, that are separate from this technical concept:\n\nBased on what chance events later happened, I wish I had taken a different action instead.\nI was wrong about what strategy my opponent was playing, and I wish I had taken a different action instead.\n\nHowever, we will use “regret” as a technical concept to mean only actions that are not-highest-EV given a particular P1 strategy and P2 strategy.\nSome students report that they misunderstood what we mean by “regret” the first time that they read this. Go back and check the two things that regret isn’t.",
    "crumbs": [
      "Signup: AI Poker Camp Summer 2024",
      "Challenges",
      "1: Kuhn Poker"
    ]
  },
  {
    "objectID": "aipcs24/challenge1kuhn.html#exercises",
    "href": "aipcs24/challenge1kuhn.html#exercises",
    "title": "Challenge 1: Kuhn Poker CFR Tutorial",
    "section": "Exercises",
    "text": "Exercises\n\n\n\n\n\n\nInformation Set\n\n\n\n\nWhat are Fudd’s information set(s)?\nWhat are Bugs’s information set(s)?\n\n\n\n\n\n\n\n\n\n\nExpected Value\n\n\n\nSay that Fudd chooses to Hunt_Near (at the appropriate information set) with probability \\(p\\).\n\n\nAt the Bugs infoset where Bugs knows he’s in the Forest, what is the expected value of choosing to Play_Near?\nWhat is the expected value of choosing to Play_Far?\n\n(Reminder: Bugs wants to avoid a “positive” score.)\n\n\n\n\n\n\n\n\nExpected Value 2\n\n\n\nSay that Bugs chooses to Play_Near with probability \\(q\\).\n\n\nWhat is Fudd’s expected value of choosing Hunt_Near at his infoset?\nWhat is Fudd’s EV of choosing Hunt_Far at his infoset?\n\n\n\n\n\n\n\n\n\nRegret\n\n\n\nFind a \\(p\\) and a \\(q\\) such that both:\n\nBugs never chooses an action with regret\nFudd never chooses an action with regret\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nIn order to have no regret, the expected value of both actions should be equal",
    "crumbs": [
      "Signup: AI Poker Camp Summer 2024",
      "Challenges",
      "1: Kuhn Poker"
    ]
  },
  {
    "objectID": "aipcs24/challenge1kuhn.html#kuhn-poker",
    "href": "aipcs24/challenge1kuhn.html#kuhn-poker",
    "title": "Challenge 1: Kuhn Poker CFR Tutorial",
    "section": "Kuhn Poker",
    "text": "Kuhn Poker\nKuhn Poker is the most simple version of poker.\n\n\n3 card deck: Queen, King, Ace (Ace is highest)\n2 players\nEach player starts with 2 chips and antes 1\nDeal 1 card to each player (discard the 3rd)\nPlayers can take Up or Down actions\n\nUp actions indicate a Bet or Call (putting a chip into the pot)\nDown actions indicate a Check or Fold (not putting a chip into the pot)\n\nRotate who acts first each hand\nThere is one betting round that can take the following sequences:\n\n\n\n\nResult\nPlayer 1\nPlayer 2\nPlayer 1\nWinner\n\n\n\n\nSequence 1\nDown\nUp\nDown\nPlayer 2 (+1)\n\n\nSequence 2\nDown\nUp\nUp\nHigh Card (+2)\n\n\nSequence 3\nDown\nDown\n\nHigh Card (+1)\n\n\nSequence 4\nUp\nDown\n\nPlayer 1 (+1)\n\n\nSequence 5\nUp\nUp\n\nHigh Card (+2)",
    "crumbs": [
      "Signup: AI Poker Camp Summer 2024",
      "Challenges",
      "1: Kuhn Poker"
    ]
  },
  {
    "objectID": "aipcs24/challenge1kuhn.html#challenge-1",
    "href": "aipcs24/challenge1kuhn.html#challenge-1",
    "title": "Challenge 1: Kuhn Poker CFR Tutorial",
    "section": "Challenge 1",
    "text": "Challenge 1\nPlay with Kuhn Poker strategies on our interactive site and submit once you find the strategy that you think is best.\nAfter a short delay, the leaderboard should update; your score will be compared to other players and selected bots. You will be listed as “tied” with another entry if your total score against all opponents is within one standard error of theirs (a statistician might say, you have a p&gt;31%).\nAdvice: We believe you will get the most out of the course if you approach each challenge with the goal of making your bot achieve the highest score possible in each matchup. Because there is no memory between rounds, this is equivalent to maximizing your expected score in each round.\n\nHow Kuhn Interactive works\nTO DO\n\n\nWhy can’t we just do tree search?\nBecause of incomplete information.\nIf we had complete information, then the regret-minimizing strategy downstream of a node would depend only on stuff downtree of that node, and we could – in theory – compute the strategy recursively from the bottom up.\nIf you attempt to follow this strategy in Kuhn poker, it will fail. You can try this for yourself – if it was valid to “solve from the bottom up”, then the following steps should get to a regret-minimal strategy:\n\nReset\nUpdate the strategy weights for the X_↓↑ nodes until they converge.\nUpdate the strategy weights for the _X↓ and _X↑ nodes until they converge.\nUpdate the strategy weights for the X_ nodes until they converge.\n\nFollow these steps. You do not have a regret-minimal strategy. Why not? What went wrong?\n\n\nDo we really need mixed strategies?\nYou can test this for yourself. If you set the learning rate to 1.0, updating a cell will move it all the way to playing the locally better action 100% of the time.\nFollow these steps:\n\nReset\nSet the learning rate to 1.0.\nIn any order you like, pick cells and update them.\nRepeat 3 while there are still suboptimal moves in your strategy.\n\nA bad thing is happening to you. Why?\n\n\nActually solving Kuhn Poker\nPick a reasonable learning rate, update individual cells until they converge.\nNext, enter your strategy into the daily leaderboard.\n\n\nHow results are generated\nThis will generate and submit a bot that plays with your chosen probabilities. (It has no memory of other rounds, and plays each round based on the probabilities.) Your submission will play 100,000 times against all other submissions (including the 10 bots that we added to start the challenge). It will play as P1 half the time and P2 half the time. Each 1v1 will be played in duplicate.",
    "crumbs": [
      "Signup: AI Poker Camp Summer 2024",
      "Challenges",
      "1: Kuhn Poker"
    ]
  },
  {
    "objectID": "camp/index.html",
    "href": "camp/index.html",
    "title": "Poker and Applied Rationality Camp",
    "section": "",
    "text": "Coming soon:\n\nAug 21-Sep 20: Beta in NYC or virtual\nSep 23-Nov 25: Full course virtual"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Poker Camp is a project by Ross Rheingans-Yoo and Max Chiswick."
  },
  {
    "objectID": "about.html#mission",
    "href": "about.html#mission",
    "title": "About",
    "section": "Mission",
    "text": "Mission\nTeaching practical decision-making and rationality through poker and other games, blending real-world skills with theoretical insights"
  },
  {
    "objectID": "about.html#max-bio",
    "href": "about.html#max-bio",
    "title": "About",
    "section": "Max bio",
    "text": "Max bio\n\nFormer online poker pro on PokerStars, playing over 10m hands including 990k in 1 month (record for most ever played in a month above microstakes)\nAI poker research:\n\nMSc with thesis on AI poker at Technion Israel Institute of Technology with Nahum Shimkin\nTwo poker research papers with Sam Ganzfried\n\nLinkedIn"
  },
  {
    "objectID": "about.html#ross-bio",
    "href": "about.html#ross-bio",
    "title": "About",
    "section": "Ross bio",
    "text": "Ross bio\n\nFormer Jane Street trader\nWrote the advanced trading simulations for the Jane Street trading internship program from 2018 to 2021\nHomepage\nTwitter"
  },
  {
    "objectID": "about.html#contribute",
    "href": "about.html#contribute",
    "title": "About",
    "section": "Contribute",
    "text": "Contribute\n\nWe are in the process of becoming a 501(c)(3) nonprofit. If you are interested in contributing monetarily, please see our .org site: https://pokercamp.org.\nIf you are interested in contributing to materials or working with us, be in touch"
  }
]